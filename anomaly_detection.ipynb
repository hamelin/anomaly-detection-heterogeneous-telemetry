{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection over heterogeneous data streams\n",
    "\n",
    "The recipe, inspired from [this paper](https://arxiv.org/abs/1812.02848) by Palladino and Thissen:\n",
    "\n",
    "1. Split time line into intervals; split the data streams into subsets corresponding to these intervals.\n",
    "1. For each interval:\n",
    "    1. Compose a graph out of the events belonging to the interval.\n",
    "    1. Extract a vector topological features for each vertex.\n",
    "    1. Reduce the dimension of these feature vectors.\n",
    "    1. Compute the anomaly indicator according to the reduced feature vectors for the interval.\n",
    "    \n",
    "These various steps will be detailed as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on --max_line_length 120 --ignore W293,E302\n",
    "%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notebooks_as_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupytest import Suite, Report, Magic, fail, summarize_results, assert_, eq\n",
    "suite = Suite('test')\n",
    "if __name__ == '__main__':\n",
    "    suite |= Report()\n",
    "    suite |= Magic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from growing import growing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time line partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each data stream is taken in as a Dask dataframe, itself indexed by a certain key. A heterogeneous data stream is a set of named data streams, all indexed in the same key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as ddf\n",
    "from typing import Mapping, TypeVar\n",
    "\n",
    "DomainIndex = TypeVar(\"DomainIndex\")\n",
    "StreamHeterogeneous = Mapping[str, ddf.DataFrame]  # Data frame is indexed by DomainIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a subset of index values (articulated as an ordered, strictly monotonic increasing iterable sequence), a *partition* is thus a heterogeneous stream, mapped by these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Mapping, Tuple\n",
    "\n",
    "Partition = Tuple[DomainIndex, StreamHeterogeneous]\n",
    "Partitioning = Mapping[DomainIndex, StreamHeterogeneous]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning sequence\n",
    "\n",
    "The partitioning sequence is processed into a sequence of slices, with which to interrogate the index of the heterogeneous stream. Namely, a sequence `[i0, i1, i2... iN]` yields mutually exclusive slices `i0:(i1-1)`, `i1:(i2-1)`... `iN-1:((iN)-1)`. Let's embody partition sequences into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Generic, Sequence, Iterator, Tuple\n",
    "\n",
    "\n",
    "class SequencePartition(ABC, Generic[DomainIndex]):\n",
    "\n",
    "    @abstractmethod\n",
    "    def __iter__(self) -> Iterator[DomainIndex]:\n",
    "        raise NotImplementedError()\n",
    "        yield 0\n",
    "        \n",
    "    @abstractmethod\n",
    "    def pred(self, n: DomainIndex) -> DomainIndex:\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def slices(self) -> Iterator[Tuple[DomainIndex, DomainIndex]]:\n",
    "        prev: Optional[DomainIndex] = None\n",
    "        for cur in self:\n",
    "            if prev is not None:\n",
    "                yield (prev, self.pred(cur))\n",
    "            prev = cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceInt(SequencePartition[int]):\n",
    "    \n",
    "    def __init__(self, seq: Sequence[int]) -> None:\n",
    "        super().__init__()\n",
    "        self._seq = seq\n",
    "        \n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        return iter(self._seq)\n",
    "    \n",
    "    def pred(self, n: int) -> int:\n",
    "        return n - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mNo slice for a sequence of less than two items\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test No slice for a sequence of less than two items\n",
    "for i, seq in enumerate([[], [0]]):\n",
    "    assert_(eq, actual=len(list(SequenceInt(seq).slices())), expected=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mIterating single slice\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Iterating single slice\n",
    "assert_(eq, expected=[(0, 1)], actual=list(SequenceInt([0, 2]).slices()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mWith sequences not strictly monotonic, we get degenerate slices\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test With sequences not strictly monotonic, we get degenerate slices\n",
    "assert_(eq, expected=[(0, 0), (1, 0)], actual=list(SequenceInt([0, 1, 1]).slices()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mMultiple slices from a sequence of integers\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Multiple slices from a sequence of integers\n",
    "assert_(eq, expected=[(0, 2), (3, 4)], actual=list(SequenceInt([0, 3, 5]).slices()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning sequence of date-times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class SequenceDatetime(SequencePartition[pd.Timestamp]):\n",
    "    \n",
    "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        super().__init__()\n",
    "        self._seq = pd.date_range(*args, **kwargs)\n",
    "        \n",
    "    def __iter__(self) -> Iterator[pd.Timestamp]:\n",
    "        return iter(self._seq)\n",
    "        \n",
    "    def pred(self, ts: pd.Timestamp) -> pd.Timestamp:\n",
    "        return ts - pd.Timedelta(nanoseconds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mMultiple slices from a date-time partitioning sequence\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Multiple slices from a date-time partitioning sequence\n",
    "assert_(\n",
    "    eq,\n",
    "    expected=[\n",
    "        (pd.Timestamp(\"2015-01-01T00:00:00\"), pd.Timestamp(\"2015-01-01T00:59:59.999999999\")),\n",
    "        (pd.Timestamp(\"2015-01-01T01:00:00\"), pd.Timestamp(\"2015-01-01T01:59:59.999999999\")),\n",
    "        (pd.Timestamp(\"2015-01-01T02:00:00\"), pd.Timestamp(\"2015-01-01T02:59:59.999999999\")),\n",
    "        (pd.Timestamp(\"2015-01-01T03:00:00\"), pd.Timestamp(\"2015-01-01T03:59:59.999999999\"))\n",
    "    ],\n",
    "    actual=list(SequenceDatetime(\"2015-01-01T00:00:00\", \"2015-01-01T04:00:00\", freq=pd.Timedelta(hours=1)).slices())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning of the heterogeneous stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(sh: StreamHeterogeneous, seq: SequencePartition) -> Partitioning:\n",
    "    P = {}\n",
    "    for lower, upper in seq.slices():\n",
    "        P[lower] = {name: df.loc[lower:upper] for name, df in sh.items()}\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as ddf\n",
    "from jupytest import Explanation\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def equals(expected: pd.DataFrame, actual: ddf.DataFrame) -> Explanation:\n",
    "    if actual.compute().equals(expected):\n",
    "        return True\n",
    "    return Explanation(\n",
    "        \"These dataframes are not equal as expected\",\n",
    "        [(\"Expected\", expected), (\"Actual\", actual)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mPartitioning an int-indexed heterogeneous stream\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Partitioning an int-indexed heterogeneous stream\n",
    "import dask\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def f1():\n",
    "    return pd.DataFrame({\"a\": [3, 8, 9], \"b\": [10, 102, 89]}, index=pd.Int64Index([1, 4, 6]))\n",
    "\n",
    "@dask.delayed\n",
    "def f2():\n",
    "    return pd.DataFrame({\"a\": [5, 3], \"b\": [90, 32]}, index=pd.Int64Index([8, 11]))\n",
    "\n",
    "@dask.delayed\n",
    "def f3():\n",
    "    return pd.DataFrame({\"a\": [2, 11, 7, 3], \"b\": [56, 98, 99, 67]}, index=pd.Int64Index([13, 14, 15, 19]))\n",
    "\n",
    "@dask.delayed\n",
    "def g1():\n",
    "    return pd.DataFrame(\n",
    "        {\"a\": [6, 3, 8, -1, 0], \"c\": [-3, -8, -11, -7, -10], \"d\": [106, 203, 267, 308, 429]},\n",
    "        index=pd.Int64Index([3, 6, 7, 10, 11])\n",
    "    )\n",
    "\n",
    "@dask.delayed\n",
    "def g2():\n",
    "    return pd.DataFrame(\n",
    "        {\"a\": [2, 8, 9, 5, 5, 11], \"c\": [-13, -6, -7, -6, -8, -2], \"d\": [567, 308, 289, 290, 432, 387]},\n",
    "        index=pd.Int64Index([14, 15, 17, 19, 21, 24])\n",
    "    )\n",
    "\n",
    "\n",
    "sh = {\n",
    "    \"p\": ddf.from_delayed(\n",
    "        [f1(), f2(), f3()],\n",
    "        meta={\"a\": \"int64\", \"b\": \"int64\"},\n",
    "        divisions=[1, 8, 13, 20]\n",
    "    ),\n",
    "    \"q\": ddf.from_delayed(\n",
    "        [g1(), g2()],\n",
    "        meta={\"a\": \"int64\", \"c\": \"int64\", \"d\": \"int64\"}, \n",
    "        divisions=[3, 14, 28]\n",
    "    )\n",
    "}\n",
    "P = partition(sh, SequenceInt([0, 5, 10, 15, 20]))\n",
    "\n",
    "assert_(eq, actual=set(P.keys()), expected={0, 5, 10, 15})\n",
    "assert_(eq, reference={\"p\", \"q\"}, **{str(i): set(p.keys()) for i, p in enumerate(P.values())})\n",
    "\n",
    "assert_(equals, actual=P[0][\"p\"], expected=pd.DataFrame({\"a\": [3, 8], \"b\": [10, 102]}, index=[1, 4]))\n",
    "assert_(equals, actual=P[0][\"q\"], expected=pd.DataFrame({\"a\": [6], \"c\": [-3], \"d\": [106]}, index=[3]))\n",
    "assert_(equals, actual=P[5][\"p\"], expected=pd.DataFrame({\"a\": [9, 5], \"b\": [89, 90]}, index=[6, 8]))\n",
    "assert_(\n",
    "    equals,\n",
    "    actual=P[5][\"q\"],\n",
    "    expected=pd.DataFrame({\"a\": [3, 8], \"c\": [-8, -11], \"d\": [203, 267]}, index=[6, 7])\n",
    ")\n",
    "assert_(\n",
    "    equals,\n",
    "    actual=P[10][\"p\"],\n",
    "    expected=pd.DataFrame({\"a\": [3, 2, 11], \"b\": [32, 56, 98]}, index=[11, 13, 14])\n",
    ")\n",
    "assert_(\n",
    "    equals,\n",
    "    actual=P[10][\"q\"],\n",
    "    expected=pd.DataFrame(\n",
    "        {\"a\": [-1, 0, 2], \"c\": [-7, -10, -13], \"d\": [308, 429, 567]},\n",
    "        index=[10, 11, 14]\n",
    "    )\n",
    ")\n",
    "assert_(\n",
    "    equals,\n",
    "    actual=P[15][\"p\"],\n",
    "    expected=pd.DataFrame({\"a\": [7, 3], \"b\": [99, 67]}, index=[15, 19])\n",
    ")\n",
    "assert_(\n",
    "    equals,\n",
    "    actual=P[15][\"q\"],\n",
    "    expected=pd.DataFrame(\n",
    "        {\"a\": [8, 9, 5], \"c\": [-6, -7, -6], \"d\": [308, 289, 290]},\n",
    "        index=[15, 17, 19]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test result summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 passed, \u001b[37m0 failed\u001b[0m, \u001b[37m0 raised an error\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    _ = summarize_results(suite)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
