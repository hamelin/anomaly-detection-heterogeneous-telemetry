{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection over heterogeneous data streams\n",
    "\n",
    "The recipe, inspired from [this paper](https://arxiv.org/abs/1812.02848) by Palladino and Thissen:\n",
    "\n",
    "1. Split time line into intervals; split the data streams into subsets corresponding to these intervals.\n",
    "1. For each interval:\n",
    "    1. Compose a graph out of the events belonging to the interval.\n",
    "    1. Extract a vector topological features for each vertex.\n",
    "    1. Reduce the dimension of these feature vectors.\n",
    "    1. Compute the anomaly indicator according to the reduced feature vectors for the interval.\n",
    "    \n",
    "These various steps will be detailed as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on --max_line_length 120 --ignore W293,E302\n",
    "%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notebooks_as_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupytest import Suite, Report, Magic, fail, summarize_results, assert_, eq, Explanation, ExplanationOnFailure, join_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = Suite('test')\n",
    "if __name__ == '__main__':\n",
    "    suite |= Report()\n",
    "    suite |= Magic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from growing import growing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time line partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each data stream is taken in as a Dask dataframe, itself indexed by a certain key. A heterogeneous data stream is a set of named data streams, all indexed in the same key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as ddf\n",
    "from typing import Mapping, TypeVar\n",
    "\n",
    "DomainIndex = TypeVar(\"DomainIndex\")\n",
    "Schema = Mapping[str, str]\n",
    "StreamHeterogeneous = Mapping[str, ddf.DataFrame]  # Data frame is indexed by DomainIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a subset of index values (articulated as an ordered, strictly monotonic increasing iterable sequence), a *partition* is thus a heterogeneous stream, mapped by these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Mapping, Tuple\n",
    "\n",
    "Partition = Tuple[DomainIndex, StreamHeterogeneous]\n",
    "Partitioning = Mapping[DomainIndex, StreamHeterogeneous]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning sequence\n",
    "\n",
    "The partitioning sequence is processed into a sequence of slices, with which to interrogate the index of the heterogeneous stream. Namely, a sequence `[i0, i1, i2... iN]` yields mutually exclusive slices `i0:(i1-1)`, `i1:(i2-1)`... `iN-1:((iN)-1)`. Let's embody partition sequences into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Generic, Sequence, Iterator, Tuple\n",
    "\n",
    "\n",
    "class SequencePartition(ABC, Generic[DomainIndex]):\n",
    "\n",
    "    @abstractmethod\n",
    "    def __iter__(self) -> Iterator[DomainIndex]:\n",
    "        raise NotImplementedError()\n",
    "        yield 0\n",
    "        \n",
    "    @abstractmethod\n",
    "    def pred(self, n: DomainIndex) -> DomainIndex:\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def slices(self) -> Iterator[Tuple[DomainIndex, DomainIndex]]:\n",
    "        prev: Optional[DomainIndex] = None\n",
    "        for cur in self:\n",
    "            if prev is not None:\n",
    "                yield (prev, self.pred(cur))\n",
    "            prev = cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceInt(SequencePartition[int]):\n",
    "    \n",
    "    def __init__(self, seq: Sequence[int]) -> None:\n",
    "        super().__init__()\n",
    "        self._seq = seq\n",
    "        \n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        return iter(self._seq)\n",
    "    \n",
    "    def pred(self, n: int) -> int:\n",
    "        return n - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mNo slice for a sequence of less than two items\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test No slice for a sequence of less than two items\n",
    "for i, seq in enumerate([[], [0]]):\n",
    "    assert_(eq, actual=len(list(SequenceInt(seq).slices())), expected=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mIterating single slice\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Iterating single slice\n",
    "assert_(eq, expected=[(0, 1)], actual=list(SequenceInt([0, 2]).slices()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mWith sequences not strictly monotonic, we get degenerate slices\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test With sequences not strictly monotonic, we get degenerate slices\n",
    "assert_(eq, expected=[(0, 0), (1, 0)], actual=list(SequenceInt([0, 1, 1]).slices()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mMultiple slices from a sequence of integers\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Multiple slices from a sequence of integers\n",
    "assert_(eq, expected=[(0, 2), (3, 4)], actual=list(SequenceInt([0, 3, 5]).slices()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning sequence of date-times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class SequenceDatetime(SequencePartition[pd.Timestamp]):\n",
    "    \n",
    "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        super().__init__()\n",
    "        self._seq = pd.date_range(*args, **kwargs)\n",
    "        \n",
    "    def __iter__(self) -> Iterator[pd.Timestamp]:\n",
    "        return iter(self._seq)\n",
    "        \n",
    "    def pred(self, ts: pd.Timestamp) -> pd.Timestamp:\n",
    "        return ts - pd.Timedelta(nanoseconds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mMultiple slices from a date-time partitioning sequence\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Multiple slices from a date-time partitioning sequence\n",
    "assert_(\n",
    "    eq,\n",
    "    expected=[\n",
    "        (pd.Timestamp(\"2015-01-01T00:00:00\"), pd.Timestamp(\"2015-01-01T00:59:59.999999999\")),\n",
    "        (pd.Timestamp(\"2015-01-01T01:00:00\"), pd.Timestamp(\"2015-01-01T01:59:59.999999999\")),\n",
    "        (pd.Timestamp(\"2015-01-01T02:00:00\"), pd.Timestamp(\"2015-01-01T02:59:59.999999999\")),\n",
    "        (pd.Timestamp(\"2015-01-01T03:00:00\"), pd.Timestamp(\"2015-01-01T03:59:59.999999999\"))\n",
    "    ],\n",
    "    actual=list(SequenceDatetime(\"2015-01-01T00:00:00\", \"2015-01-01T04:00:00\", freq=pd.Timedelta(hours=1)).slices())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning of the heterogeneous stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(sh: StreamHeterogeneous, seq: SequencePartition) -> Partitioning:\n",
    "    P = {}\n",
    "    for lower, upper in seq.slices():\n",
    "        P[lower] = {name: df.loc[lower:upper] for name, df in sh.items()}\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as ddf\n",
    "from jupytest import Explanation\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def equals(expected: pd.DataFrame, actual: ddf.DataFrame) -> Explanation:\n",
    "    if actual.compute().equals(expected):\n",
    "        return True\n",
    "    return Explanation(\n",
    "        \"These dataframes are not equal as expected\",\n",
    "        [(\"Expected\", expected), (\"Actual\", actual)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mPartitioning an int-indexed heterogeneous stream\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Partitioning an int-indexed heterogeneous stream\n",
    "import dask\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def f1():\n",
    "    return pd.DataFrame({\"a\": [3, 8, 9], \"b\": [10, 102, 89]}, index=pd.Int64Index([1, 4, 6]))\n",
    "\n",
    "@dask.delayed\n",
    "def f2():\n",
    "    return pd.DataFrame({\"a\": [5, 3], \"b\": [90, 32]}, index=pd.Int64Index([8, 11]))\n",
    "\n",
    "@dask.delayed\n",
    "def f3():\n",
    "    return pd.DataFrame({\"a\": [2, 11, 7, 3], \"b\": [56, 98, 99, 67]}, index=pd.Int64Index([13, 14, 15, 19]))\n",
    "\n",
    "@dask.delayed\n",
    "def g1():\n",
    "    return pd.DataFrame(\n",
    "        {\"a\": [6, 3, 8, -1, 0], \"c\": [-3, -8, -11, -7, -10], \"d\": [106, 203, 267, 308, 429]},\n",
    "        index=pd.Int64Index([3, 6, 7, 10, 11])\n",
    "    )\n",
    "\n",
    "@dask.delayed\n",
    "def g2():\n",
    "    return pd.DataFrame(\n",
    "        {\"a\": [2, 8, 9, 5, 5, 11], \"c\": [-13, -6, -7, -6, -8, -2], \"d\": [567, 308, 289, 290, 432, 387]},\n",
    "        index=pd.Int64Index([14, 15, 17, 19, 21, 24])\n",
    "    )\n",
    "\n",
    "\n",
    "sh = {\n",
    "    \"p\": ddf.from_delayed(\n",
    "        [f1(), f2(), f3()],\n",
    "        meta={\"a\": \"int64\", \"b\": \"int64\"},\n",
    "        divisions=[1, 8, 13, 20]\n",
    "    ),\n",
    "    \"q\": ddf.from_delayed(\n",
    "        [g1(), g2()],\n",
    "        meta={\"a\": \"int64\", \"c\": \"int64\", \"d\": \"int64\"}, \n",
    "        divisions=[3, 14, 28]\n",
    "    )\n",
    "}\n",
    "P = partition(sh, SequenceInt([0, 5, 10, 15, 20]))\n",
    "\n",
    "assert_(eq, actual=set(P.keys()), expected={0, 5, 10, 15})\n",
    "assert_(eq, reference={\"p\", \"q\"}, **{str(i): set(p.keys()) for i, p in enumerate(P.values())})\n",
    "\n",
    "assert_(equals, actual=P[0][\"p\"], expected=pd.DataFrame({\"a\": [3, 8], \"b\": [10, 102]}, index=[1, 4]))\n",
    "assert_(equals, actual=P[0][\"q\"], expected=pd.DataFrame({\"a\": [6], \"c\": [-3], \"d\": [106]}, index=[3]))\n",
    "assert_(equals, actual=P[5][\"p\"], expected=pd.DataFrame({\"a\": [9, 5], \"b\": [89, 90]}, index=[6, 8]))\n",
    "assert_(\n",
    "    equals,\n",
    "    actual=P[5][\"q\"],\n",
    "    expected=pd.DataFrame({\"a\": [3, 8], \"c\": [-8, -11], \"d\": [203, 267]}, index=[6, 7])\n",
    ")\n",
    "assert_(\n",
    "    equals,\n",
    "    actual=P[10][\"p\"],\n",
    "    expected=pd.DataFrame({\"a\": [3, 2, 11], \"b\": [32, 56, 98]}, index=[11, 13, 14])\n",
    ")\n",
    "assert_(\n",
    "    equals,\n",
    "    actual=P[10][\"q\"],\n",
    "    expected=pd.DataFrame(\n",
    "        {\"a\": [-1, 0, 2], \"c\": [-7, -10, -13], \"d\": [308, 429, 567]},\n",
    "        index=[10, 11, 14]\n",
    "    )\n",
    ")\n",
    "assert_(\n",
    "    equals,\n",
    "    actual=P[15][\"p\"],\n",
    "    expected=pd.DataFrame({\"a\": [7, 3], \"b\": [99, 67]}, index=[15, 19])\n",
    ")\n",
    "assert_(\n",
    "    equals,\n",
    "    actual=P[15][\"q\"],\n",
    "    expected=pd.DataFrame(\n",
    "        {\"a\": [8, 9, 5], \"c\": [-6, -7, -6], \"d\": [308, 289, 290]},\n",
    "        index=[15, 17, 19]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crunching event sets into artifact graphs\n",
    "\n",
    "Step 2A above creating an *artifact graph* for each data partition. These graphs involve the decomposition of each event of a partition into a set of *artifacts*, entities associated to typical IT concepts, often acting as signifiers for a host, a computation, a network. The artifact graph is composed as follows:\n",
    "\n",
    "1. Each artifact is associated to a vertex in the graph.\n",
    "1. The artifacts extracted from an event E form a clique within the graph.\n",
    "1. When adding an event clique to the graph, if an edge already exist, we increment its weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph architecture\n",
    "\n",
    "Determining (and engineering) which elements of an event are artifacts is a matter of arbitrary policy. We will encode this policy into a *graph architecture* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Mapping, Sequence, NewType, NamedTuple\n",
    "\n",
    "\n",
    "Artifact = Tuple[str, str]\n",
    "Schema = Mapping[str, str]\n",
    "\n",
    "\n",
    "class ArchGraph(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_artifacts(self, event: NamedTuple, schema: Schema) -> Sequence[Artifact]:\n",
    "        \"\"\"\n",
    "        Extracts the artifacts from an event, itself encoded as a ad hoc namedtuple (hence the absence)\n",
    "        of typing information.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical architecture is to use all event attributes of dtype `object` as artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllObjects(ArchGraph):\n",
    "    \n",
    "    def get_artifacts(self, event: NamedTuple, dtypes: Schema) -> Sequence[Artifact]:\n",
    "        artifacts = []\n",
    "        for attr, dtype in dtypes.items():\n",
    "            if dtype == \"object\":\n",
    "                try:\n",
    "                    artifacts.append((attr, getattr(event, attr)))\n",
    "                except AttributeError:\n",
    "                    # We ignore the absence of any attribute.\n",
    "                    pass\n",
    "        return artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An event factory for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from contextlib import contextmanager\n",
    "from typing import ContextManager\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def test_event_factory(**fields: Any) -> ContextManager[namedtuple]:\n",
    "    type_event = namedtuple(\"TestEvent\", fields.keys())\n",
    "    yield type_event(**fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mNo artifact provided when the dtype catalog is empty\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test No artifact provided when the dtype catalog is empty\n",
    "with test_event_factory(a=\"asdf\", b=\"qwer\", c=38) as event:\n",
    "    assert AllObjects().get_artifacts(event, {}) == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mEvent with no attribute yields no artifact\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Event with no attribute yields no artifact\n",
    "with test_event_factory() as event:\n",
    "    assert AllObjects().get_artifacts(event, {\"a\": \"object\", \"b\": \"object\", \"c\": \"int64\"}) == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mAllObjects yields all object attributes as artifacts\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test AllObjects yields all object attributes as artifacts\n",
    "with test_event_factory(a=\"asdf\", b=\"qwer\", c=38) as event:\n",
    "    assert sorted(AllObjects().get_artifacts(event, {\"a\": \"object\", \"b\": \"object\", \"c\": \"int64\"})) == \\\n",
    "        [(\"a\", \"asdf\"), (\"b\", \"qwer\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mMissing artifact compared to the dtype catalog is silently ignored\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Missing artifact compared to the dtype catalog is silently ignored\n",
    "with test_event_factory(a=\"asdf\", c=38) as event:\n",
    "    assert AllObjects().get_artifacts(event, {\"a\": \"object\", \"b\": \"object\", \"c\": \"int64\"}) == [(\"a\", \"asdf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mExtraneous attributes compared to the dtype catalog are ignored\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Extraneous attributes compared to the dtype catalog are ignored\n",
    "with test_event_factory(a=\"asdf\", b=\"qwer\", d=\"zxcv\") as event:\n",
    "    assert sorted(AllObjects().get_artifacts(event, {\"a\": \"object\", \"b\": \"object\", \"c\": \"int64\"})) == \\\n",
    "        [(\"a\", \"asdf\"), (\"b\", \"qwer\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding artifact cliques to a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def add_clique(g: ig.Graph, clique: Sequence[Artifact]) -> ig.Graph:\n",
    "    dummy = g.add_vertex(\"__DUMMY__\")  # Necessary to query the main vertex sequence.\n",
    "\n",
    "    vertices = []\n",
    "    for type_artifact, value_artifact in clique:\n",
    "        name = f\"{type_artifact}:{value_artifact}\"\n",
    "        vertices += (g.vs(name=name) or [g.add_vertex(name)])\n",
    "\n",
    "    for src, dst in combinations(vertices, 2):\n",
    "        for edge in (g.es(_from=src, _to=dst) or [g.add_edge(src, dst, weight=0)]):\n",
    "            edge[\"weight\"] += 1\n",
    "            \n",
    "    g.delete_vertices(dummy)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "\n",
    "\n",
    "def test_graph() -> ig.Graph:\n",
    "    g = ig.Graph()\n",
    "    v_asdf = g.add_vertex(\"a:asdf\")\n",
    "    v_qwer = g.add_vertex(\"b:qwer\")\n",
    "    g.add_edge(v_asdf, v_qwer, weight=1)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing graphs by reducing them to tuples and sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "ListVertices = List[str]\n",
    "ListEdgesWeighted = List[Tuple[str, str, int]]\n",
    "GraphLists = Tuple[ListVertices, ListEdgesWeighted]\n",
    "\n",
    "\n",
    "def graph2sets(g: ig.Graph) -> GraphLists:\n",
    "    return (\n",
    "        sorted([v[\"name\"] for v in g.vs]),\n",
    "        sorted([tuple(sorted([v[\"name\"] for v in e.vertex_tuple])) + (e[\"weight\"],) for e in g.es])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mGraph sets for simple graph\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Graph sets for simple graph\n",
    "assert_(eq, actual=graph2sets(test_graph()), expected=([\"a:asdf\", \"b:qwer\"], [(\"a:asdf\", \"b:qwer\", 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mGraph sets for bit more complex graph\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Graph sets for bit more complex graph\n",
    "g = ig.Graph()\n",
    "for name in [\"a\", \"b\", \"c\", \"1\", \"2\", \"3\"]:\n",
    "    g.add_vertex(name)\n",
    "g.add_edge(\"a\", \"b\", weight=1)\n",
    "g.add_edge(\"c\", \"b\", weight=3)\n",
    "g.add_edge(\"1\", \"2\", weight=5)\n",
    "g.add_edge(\"3\", \"2\", weight=0)\n",
    "g.add_edge(\"b\", \"2\", weight=10)\n",
    "\n",
    "assert_(\n",
    "    eq,\n",
    "    expected=(\n",
    "        [\"1\", \"2\", \"3\", \"a\", \"b\", \"c\"],\n",
    "        [(\"1\", \"2\", 5), (\"2\", \"3\", 0), (\"2\", \"b\", 10), (\"a\", \"b\", 1), (\"b\", \"c\", 3)]\n",
    "    ),\n",
    "    actual=graph2sets(g)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mGraph sets don't care about vertex addition order\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Graph sets don't care about vertex addition order\n",
    "g1 = ig.Graph()\n",
    "g1.add_vertex(\"a\")\n",
    "g1.add_vertex(\"b\")\n",
    "g1.add_edge(\"a\", \"b\", weight=1)\n",
    "\n",
    "g2 = ig.Graph()\n",
    "g2.add_vertex(\"b\")\n",
    "g2.add_vertex(\"a\")\n",
    "g2.add_edge(\"b\", \"a\", weight=1)\n",
    "\n",
    "assert_(eq, ordered=graph2sets(g1), unordered=graph2sets(g2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mGraph sets when no edge\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Graph sets when no edge\n",
    "g = ig.Graph()\n",
    "for name in [\"a\", \"b\"]:\n",
    "    g.add_vertex(name)\n",
    "    \n",
    "assert_(eq, actual=graph2sets(g), expected=([\"a\", \"b\"], []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit tests for clique adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "\n",
    "def are_graphs_equal(**graphs: ig.Graph) -> ExplanationOnFailure:\n",
    "    sets = {name: graph2sets(g) for name, g in graphs.items()}\n",
    "    if eq(**sets):\n",
    "        return True\n",
    "    return Explanation(\"These graphs are not equal\", join_args([], sets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mAdding a clique without any artifact leaves the graph unchanged\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Adding a clique without any artifact leaves the graph unchanged\n",
    "expected = test_graph()\n",
    "actual = add_clique(test_graph(), [])\n",
    "assert_(are_graphs_equal, expected=expected, actual=actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mAdd a clique to an empty graph\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Add a clique to an empty graph\n",
    "expected = test_graph()\n",
    "actual = ig.Graph()\n",
    "assert_(are_graphs_equal, expected=expected, actual=add_clique(ig.Graph(), [(\"a\", \"asdf\"), (\"b\", \"qwer\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mAdding a clique of all new vertices\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Adding a clique of all new vertices\n",
    "clique: Sequence[Artifact] = [(\"c\", \"uiop\"), (\"d\", \"qwer\"), (\"e\", \"zxcv\"), (\"f\", \"hjkl\")]\n",
    "\n",
    "expected = test_graph()\n",
    "for t, v in clique:\n",
    "    expected.add_vertex(f\"{t}:{v}\")\n",
    "for s, e in [\n",
    "    (\"c:uiop\", \"d:qwer\"), (\"c:uiop\", \"e:zxcv\"), (\"c:uiop\", \"f:hjkl\"),\n",
    "    (\"d:qwer\", \"e:zxcv\"), (\"d:qwer\", \"f:hjkl\"),\n",
    "    (\"e:zxcv\", \"f:hjkl\")\n",
    "]:\n",
    "    expected.add_edge(s, e, weight=1)\n",
    "\n",
    "actual = add_clique(test_graph(), clique)\n",
    "assert_(are_graphs_equal, expected=expected, actual=actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mAdding a clique that intersects another one fully\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Adding a clique that intersects another one fully\n",
    "clique: Sequence[Artifact] = [(\"a\", \"asdf\"), (\"b\", \"qwer\"), (\"c\", \"zxcv\")]\n",
    "\n",
    "expected = test_graph()\n",
    "a = next(iter(expected.vs(name=\"a:asdf\")))\n",
    "b = next(iter(expected.vs(name=\"b:qwer\")))\n",
    "c = expected.add_vertex(\"c:zxcv\")\n",
    "for e in expected.es(_from=a, _to=b):\n",
    "    e[\"weight\"] = 2\n",
    "expected.add_edge(a, c, weight=1)\n",
    "expected.add_edge(b, c, weight=1)\n",
    "\n",
    "actual = add_clique(test_graph(), clique)\n",
    "assert_(are_graphs_equal, expected=expected, actual=actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mAdding a clique that intersects another one by one vertex only\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Adding a clique that intersects another one by one vertex only\n",
    "clique = [(\"b\", \"qwer\"), (\"c\", \"zxcv\")]\n",
    "\n",
    "expected = test_graph()\n",
    "b = next(iter(expected.vs(name=\"b:qwer\")))\n",
    "c = expected.add_vertex(\"c:zxcv\")\n",
    "expected.add_edge(b, c, weight=1)\n",
    "\n",
    "actual = add_clique(test_graph(), clique)\n",
    "assert_(are_graphs_equal, expected=expected, actual=actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mAdding a clique that intersects another partly, but by more than a single vertex\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Adding a clique that intersects another partly, but by more than a single vertex\n",
    "clique = [(\"a\", \"asdf\"), (\"c\", \"zxcv\"), (\"d\", \"hjkl\")]\n",
    "\n",
    "expected = test_graph()\n",
    "a = next(iter(expected.vs(name=\"a:asdf\")))\n",
    "b = next(iter(expected.vs(name=\"b:qwer\")))\n",
    "c = expected.add_vertex(\"c:zxcv\")\n",
    "expected.add_edge(a, c, weight=2)\n",
    "expected.add_edge(b, c, weight=1)\n",
    "d = expected.add_vertex(\"d:hjkl\")\n",
    "expected.add_edge(a, d, weight=1)\n",
    "expected.add_edge(c, d, weight=1)\n",
    "\n",
    "actual = test_graph()\n",
    "actual.add_vertex(\"c:zxcv\")\n",
    "actual.add_edge(a, c, weight=1)\n",
    "actual.add_edge(b, c, weight=1)\n",
    "add_clique(actual, clique)\n",
    "\n",
    "assert_(are_graphs_equal, expected=expected, actual=actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "from memo import memo\n",
    "\n",
    "\n",
    "@memo\n",
    "def build_graph_artifacts(sh: StreamHeterogeneous, arch: ArchGraph) -> ig.Graph:\n",
    "    graph = ig.Graph()\n",
    "    for name_stream, df_stream in sh.items():\n",
    "        schema: Schema = {n: str(t) for n, t in df_stream.dtypes.items()}\n",
    "        for event in df_stream.itertuples():\n",
    "            add_clique(graph, arch.get_artifacts(event, schema))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mBuilding an artifact graph\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Building an artifact graph\n",
    "from memo import suspending_memoization\n",
    "\n",
    "sh = {\n",
    "    \"asdf\": pd.DataFrame(\n",
    "        data={\n",
    "            \"X\": [\"a.b.a.b\", \"a.c.c.b\", \"a.b.a.c\", \"a.a.a.a\", \"b.a.b.c\"],\n",
    "            \"Y\": [\"443\", \"21\", \"22\", \"443\", \"443\"]\n",
    "        },\n",
    "        index=[1, 3, 7, 8, 9]\n",
    "    ),\n",
    "    \"qwer\": pd.DataFrame(\n",
    "        data={\n",
    "            \"W\": [\"10.2.3.1\", \"10.2.128.3\", \"10.2.1.1\", \"10.10.1.1\", \"10.2.128.38\", \"10.10.1.43\"],\n",
    "            \"X\": [\"a.b.a.b\", \"a.e.a.d\", \"a.a.b.b\", \"a.b.a.c\", \"b.c.b.c\", \"a.a.a.a\"],\n",
    "            \"Z\": [\"asdf.exe\", \"qwer.exe\", \"zxcv.exe\", \"qwer.exe\", \"./hkjl\", \"asdf.exe\"]\n",
    "        },\n",
    "        index=[0, 3, 4, 6, 8, 9]\n",
    "    )\n",
    "}\n",
    "\n",
    "expected = ig.Graph()\n",
    "for v in [\n",
    "    \"X:a.b.a.b\", \"X:a.c.c.b\", \"X:a.b.a.c\", \"X:a.a.a.a\", \"X:b.a.b.c\", \"X:a.e.a.d\", \"X:a.a.b.b\", \"X:b.c.b.c\",\n",
    "    \"Y:443\", \"Y:21\", \"Y:22\",\n",
    "    \"W:10.2.3.1\", \"W:10.2.128.3\", \"W:10.2.1.1\", \"W:10.10.1.1\", \"W:10.2.128.38\", \"W:10.10.1.43\",\n",
    "    \"Z:asdf.exe\", \"Z:qwer.exe\", \"Z:zxcv.exe\", \"Z:./hkjl\"\n",
    "]:\n",
    "    expected.add_vertex(v)\n",
    "for s, e in zip(\n",
    "    [\"X:a.b.a.b\", \"X:a.c.c.b\", \"X:a.b.a.c\", \"X:a.a.a.a\", \"X:b.a.b.c\"],\n",
    "    [\"Y:443\", \"Y:21\", \"Y:22\", \"Y:443\", \"Y:443\"]\n",
    "):\n",
    "    expected.add_edge(s, e, weight=1)\n",
    "for a, b, c in zip(\n",
    "    [\"W:10.2.3.1\", \"W:10.2.128.3\", \"W:10.2.1.1\", \"W:10.10.1.1\", \"W:10.2.128.38\", \"W:10.10.1.43\"],\n",
    "    [\"X:a.b.a.b\", \"X:a.e.a.d\", \"X:a.a.b.b\", \"X:a.b.a.c\", \"X:b.c.b.c\", \"X:a.a.a.a\"],\n",
    "    [\"Z:asdf.exe\", \"Z:qwer.exe\", \"Z:zxcv.exe\", \"Z:qwer.exe\", \"Z:./hkjl\", \"Z:asdf.exe\"]\n",
    "):\n",
    "    expected.add_edge(a, b, weight=1)\n",
    "    expected.add_edge(a, c, weight=1)\n",
    "    expected.add_edge(b, c, weight=1)\n",
    "\n",
    "with suspending_memoization():\n",
    "    assert_(are_graphs_equal, expected=expected, actual=build_graph_artifacts(sh, AllObjects()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test result summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 passed, \u001b[37m0 failed\u001b[0m, \u001b[37m0 raised an error\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    _ = summarize_results(suite)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
