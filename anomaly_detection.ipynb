{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection over heterogeneous data streams\n",
    "\n",
    "The recipe, inspired from [this paper](https://arxiv.org/abs/1812.02848) by Palladino and Thissen:\n",
    "\n",
    "1. Split time line into intervals; split the data streams into subsets corresponding to these intervals.\n",
    "1. For each interval:\n",
    "    1. Compose a graph out of the events belonging to the interval.\n",
    "    1. Extract a vector topological features for each vertex.\n",
    "    1. Reduce the dimension of these feature vectors.\n",
    "    1. Compute the anomaly indicator according to the reduced feature vectors for the interval.\n",
    "    \n",
    "These various steps will be detailed as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on --max_line_length 120 --ignore W293,E302\n",
    "%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notebooks_as_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupytest import Suite, Report, Magic, fail, summarize_results, assert_, eq, Explanation, ExplanationOnFailure, join_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = Suite('test')\n",
    "if __name__ == '__main__':\n",
    "    suite |= Report()\n",
    "    suite |= Magic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from growing import growing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Time line partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Each data stream is taken in as a Dask dataframe, itself indexed by a certain key. A heterogeneous data stream is a set of named data streams, all indexed in the same key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as ddf\n",
    "from typing import Mapping, TypeVar\n",
    "\n",
    "DomainIndex = TypeVar(\"DomainIndex\")\n",
    "Schema = Mapping[str, str]\n",
    "StreamHeterogeneous = Mapping[str, ddf.DataFrame]  # Data frame is indexed by DomainIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Given a subset of index values (articulated as an ordered, strictly monotonic increasing iterable sequence), a *partition* is thus a heterogeneous stream, mapped by these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Mapping, Tuple\n",
    "\n",
    "Partition = Tuple[DomainIndex, StreamHeterogeneous]\n",
    "Partitioning = Mapping[DomainIndex, StreamHeterogeneous]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Partitioning sequence\n",
    "\n",
    "The partitioning sequence is processed into a sequence of slices, with which to interrogate the index of the heterogeneous stream. Namely, a sequence `[i0, i1, i2... iN]` yields mutually exclusive slices `i0:(i1-1)`, `i1:(i2-1)`... `iN-1:((iN)-1)`. Let's embody partition sequences into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Generic, Sequence, Iterator, Tuple\n",
    "\n",
    "\n",
    "class SequencePartition(ABC, Generic[DomainIndex]):\n",
    "\n",
    "    @abstractmethod\n",
    "    def __iter__(self) -> Iterator[DomainIndex]:\n",
    "        raise NotImplementedError()\n",
    "        yield 0\n",
    "        \n",
    "    @abstractmethod\n",
    "    def pred(self, n: DomainIndex) -> DomainIndex:\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def slices(self) -> Iterator[Tuple[DomainIndex, DomainIndex]]:\n",
    "        prev: Optional[DomainIndex] = None\n",
    "        for cur in self:\n",
    "            if prev is not None:\n",
    "                yield (prev, self.pred(cur))\n",
    "            prev = cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SequenceInt(SequencePartition[int]):\n",
    "    \n",
    "    def __init__(self, seq: Sequence[int]) -> None:\n",
    "        super().__init__()\n",
    "        self._seq = seq\n",
    "        \n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        return iter(self._seq)\n",
    "    \n",
    "    def pred(self, n: int) -> int:\n",
    "        return n - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mNo slice for a sequence of less than two items\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test No slice for a sequence of less than two items\n",
    "for i, seq in enumerate([[], [0]]):\n",
    "    assert_(eq, actual=len(list(SequenceInt(seq).slices())), expected=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mIterating single slice\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Iterating single slice\n",
    "assert_(eq, expected=[(0, 1)], actual=list(SequenceInt([0, 2]).slices()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mWith sequences not strictly monotonic, we get degenerate slices\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test With sequences not strictly monotonic, we get degenerate slices\n",
    "assert_(eq, expected=[(0, 0), (1, 0)], actual=list(SequenceInt([0, 1, 1]).slices()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mMultiple slices from a sequence of integers\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Multiple slices from a sequence of integers\n",
    "assert_(eq, expected=[(0, 2), (3, 4)], actual=list(SequenceInt([0, 3, 5]).slices()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Partitioning sequence of date-times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class SequenceDatetime(SequencePartition[pd.Timestamp]):\n",
    "    \n",
    "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        super().__init__()\n",
    "        self._seq = pd.date_range(*args, **kwargs)\n",
    "        \n",
    "    def __iter__(self) -> Iterator[pd.Timestamp]:\n",
    "        return iter(self._seq)\n",
    "        \n",
    "    def pred(self, ts: pd.Timestamp) -> pd.Timestamp:\n",
    "        return ts - pd.Timedelta(nanoseconds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mMultiple slices from a date-time partitioning sequence\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Multiple slices from a date-time partitioning sequence\n",
    "assert_(\n",
    "    eq,\n",
    "    expected=[\n",
    "        (pd.Timestamp(\"2015-01-01T00:00:00\"), pd.Timestamp(\"2015-01-01T00:59:59.999999999\")),\n",
    "        (pd.Timestamp(\"2015-01-01T01:00:00\"), pd.Timestamp(\"2015-01-01T01:59:59.999999999\")),\n",
    "        (pd.Timestamp(\"2015-01-01T02:00:00\"), pd.Timestamp(\"2015-01-01T02:59:59.999999999\")),\n",
    "        (pd.Timestamp(\"2015-01-01T03:00:00\"), pd.Timestamp(\"2015-01-01T03:59:59.999999999\"))\n",
    "    ],\n",
    "    actual=list(SequenceDatetime(\"2015-01-01T00:00:00\", \"2015-01-01T04:00:00\", freq=pd.Timedelta(hours=1)).slices())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Partitioning of the heterogeneous stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def partition(sh: StreamHeterogeneous, seq: SequencePartition) -> Partitioning:\n",
    "    P = {}\n",
    "    for lower, upper in seq.slices():\n",
    "        P[lower] = {name: df.loc[lower:upper] for name, df in sh.items()}\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as ddf\n",
    "from jupytest import Explanation\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def equals(expected: pd.DataFrame, actual: ddf.DataFrame) -> Explanation:\n",
    "    if actual.compute().equals(expected):\n",
    "        return True\n",
    "    return Explanation(\n",
    "        \"These dataframes are not equal as expected\",\n",
    "        [(\"Expected\", expected), (\"Actual\", actual)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mPartitioning an int-indexed heterogeneous stream\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Partitioning an int-indexed heterogeneous stream\n",
    "import dask\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def f1():\n",
    "    return pd.DataFrame({\"a\": [3, 8, 9], \"b\": [10, 102, 89]}, index=pd.Int64Index([1, 4, 6]))\n",
    "\n",
    "@dask.delayed\n",
    "def f2():\n",
    "    return pd.DataFrame({\"a\": [5, 3], \"b\": [90, 32]}, index=pd.Int64Index([8, 11]))\n",
    "\n",
    "@dask.delayed\n",
    "def f3():\n",
    "    return pd.DataFrame({\"a\": [2, 11, 7, 3], \"b\": [56, 98, 99, 67]}, index=pd.Int64Index([13, 14, 15, 19]))\n",
    "\n",
    "@dask.delayed\n",
    "def g1():\n",
    "    return pd.DataFrame(\n",
    "        {\"a\": [6, 3, 8, -1, 0], \"c\": [-3, -8, -11, -7, -10], \"d\": [106, 203, 267, 308, 429]},\n",
    "        index=pd.Int64Index([3, 6, 7, 10, 11])\n",
    "    )\n",
    "\n",
    "@dask.delayed\n",
    "def g2():\n",
    "    return pd.DataFrame(\n",
    "        {\"a\": [2, 8, 9, 5, 5, 11], \"c\": [-13, -6, -7, -6, -8, -2], \"d\": [567, 308, 289, 290, 432, 387]},\n",
    "        index=pd.Int64Index([14, 15, 17, 19, 21, 24])\n",
    "    )\n",
    "\n",
    "\n",
    "sh = {\n",
    "    \"p\": ddf.from_delayed(\n",
    "        [f1(), f2(), f3()],\n",
    "        meta={\"a\": \"int64\", \"b\": \"int64\"},\n",
    "        divisions=[1, 8, 13, 20]\n",
    "    ),\n",
    "    \"q\": ddf.from_delayed(\n",
    "        [g1(), g2()],\n",
    "        meta={\"a\": \"int64\", \"c\": \"int64\", \"d\": \"int64\"}, \n",
    "        divisions=[3, 14, 28]\n",
    "    )\n",
    "}\n",
    "P = partition(sh, SequenceInt([0, 5, 10, 15, 20]))\n",
    "\n",
    "assert_(eq, actual=set(P.keys()), expected={0, 5, 10, 15})\n",
    "assert_(eq, reference={\"p\", \"q\"}, **{str(i): set(p.keys()) for i, p in enumerate(P.values())})\n",
    "\n",
    "assert_(equals, actual=P[0][\"p\"], expected=pd.DataFrame({\"a\": [3, 8], \"b\": [10, 102]}, index=[1, 4]))\n",
    "assert_(equals, actual=P[0][\"q\"], expected=pd.DataFrame({\"a\": [6], \"c\": [-3], \"d\": [106]}, index=[3]))\n",
    "assert_(equals, actual=P[5][\"p\"], expected=pd.DataFrame({\"a\": [9, 5], \"b\": [89, 90]}, index=[6, 8]))\n",
    "assert_(\n",
    "    equals,\n",
    "    actual=P[5][\"q\"],\n",
    "    expected=pd.DataFrame({\"a\": [3, 8], \"c\": [-8, -11], \"d\": [203, 267]}, index=[6, 7])\n",
    ")\n",
    "assert_(\n",
    "    equals,\n",
    "    actual=P[10][\"p\"],\n",
    "    expected=pd.DataFrame({\"a\": [3, 2, 11], \"b\": [32, 56, 98]}, index=[11, 13, 14])\n",
    ")\n",
    "assert_(\n",
    "    equals,\n",
    "    actual=P[10][\"q\"],\n",
    "    expected=pd.DataFrame(\n",
    "        {\"a\": [-1, 0, 2], \"c\": [-7, -10, -13], \"d\": [308, 429, 567]},\n",
    "        index=[10, 11, 14]\n",
    "    )\n",
    ")\n",
    "assert_(\n",
    "    equals,\n",
    "    actual=P[15][\"p\"],\n",
    "    expected=pd.DataFrame({\"a\": [7, 3], \"b\": [99, 67]}, index=[15, 19])\n",
    ")\n",
    "assert_(\n",
    "    equals,\n",
    "    actual=P[15][\"q\"],\n",
    "    expected=pd.DataFrame(\n",
    "        {\"a\": [8, 9, 5], \"c\": [-6, -7, -6], \"d\": [308, 289, 290]},\n",
    "        index=[15, 17, 19]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Crunching event sets into artifact graphs\n",
    "\n",
    "Step 2A above creating an *artifact graph* for each data partition. These graphs involve the decomposition of each event of a partition into a set of *artifacts*, entities associated to typical IT concepts, often acting as signifiers for a host, a computation, a network. The artifact graph is composed as follows:\n",
    "\n",
    "1. Each artifact is associated to a vertex in the graph.\n",
    "1. The artifacts extracted from an event E form a clique within the graph.\n",
    "1. When adding an event clique to the graph, if an edge already exist, we increment its weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Graph architecture\n",
    "\n",
    "Determining (and engineering) which elements of an event are artifacts is a matter of arbitrary policy. We will encode this policy into a *graph architecture* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Mapping, Sequence, NewType, NamedTuple\n",
    "\n",
    "\n",
    "Artifact = Tuple[str, str]\n",
    "Schema = Mapping[str, str]\n",
    "\n",
    "\n",
    "class ArchGraph(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_artifacts(self, event: NamedTuple, schema: Schema) -> Sequence[Artifact]:\n",
    "        \"\"\"\n",
    "        Extracts the artifacts from an event, itself encoded as a ad hoc namedtuple (hence the absence)\n",
    "        of typing information.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The typical architecture is to use all event attributes of dtype `object` as artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class AllObjects(ArchGraph):\n",
    "    \n",
    "    def get_artifacts(self, event: NamedTuple, dtypes: Schema) -> Sequence[Artifact]:\n",
    "        artifacts = []\n",
    "        for attr, dtype in dtypes.items():\n",
    "            if dtype == \"object\":\n",
    "                try:\n",
    "                    artifacts.append((attr, getattr(event, attr)))\n",
    "                except AttributeError:\n",
    "                    # We ignore the absence of any attribute.\n",
    "                    pass\n",
    "        return artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An event factory for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from contextlib import contextmanager\n",
    "from typing import ContextManager\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def test_event_factory(**fields: Any) -> ContextManager[namedtuple]:\n",
    "    type_event = namedtuple(\"TestEvent\", fields.keys())\n",
    "    yield type_event(**fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mNo artifact provided when the dtype catalog is empty\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test No artifact provided when the dtype catalog is empty\n",
    "with test_event_factory(a=\"asdf\", b=\"qwer\", c=38) as event:\n",
    "    assert AllObjects().get_artifacts(event, {}) == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mEvent with no attribute yields no artifact\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Event with no attribute yields no artifact\n",
    "with test_event_factory() as event:\n",
    "    assert AllObjects().get_artifacts(event, {\"a\": \"object\", \"b\": \"object\", \"c\": \"int64\"}) == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mAllObjects yields all object attributes as artifacts\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test AllObjects yields all object attributes as artifacts\n",
    "with test_event_factory(a=\"asdf\", b=\"qwer\", c=38) as event:\n",
    "    assert sorted(AllObjects().get_artifacts(event, {\"a\": \"object\", \"b\": \"object\", \"c\": \"int64\"})) == \\\n",
    "        [(\"a\", \"asdf\"), (\"b\", \"qwer\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mMissing artifact compared to the dtype catalog is silently ignored\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Missing artifact compared to the dtype catalog is silently ignored\n",
    "with test_event_factory(a=\"asdf\", c=38) as event:\n",
    "    assert AllObjects().get_artifacts(event, {\"a\": \"object\", \"b\": \"object\", \"c\": \"int64\"}) == [(\"a\", \"asdf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mExtraneous attributes compared to the dtype catalog are ignored\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Extraneous attributes compared to the dtype catalog are ignored\n",
    "with test_event_factory(a=\"asdf\", b=\"qwer\", d=\"zxcv\") as event:\n",
    "    assert sorted(AllObjects().get_artifacts(event, {\"a\": \"object\", \"b\": \"object\", \"c\": \"int64\"})) == \\\n",
    "        [(\"a\", \"asdf\"), (\"b\", \"qwer\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Adding artifact cliques to a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def add_clique(g: ig.Graph, clique: Sequence[Artifact]) -> ig.Graph:\n",
    "    dummy = g.add_vertex(\"__DUMMY__\")  # Necessary to query the main vertex sequence.\n",
    "\n",
    "    vertices = []\n",
    "    for type_artifact, value_artifact in clique:\n",
    "        name = f\"{type_artifact}:{value_artifact}\"\n",
    "        vertices += (g.vs(name=name) or [g.add_vertex(name)])\n",
    "\n",
    "    for src, dst in combinations(vertices, 2):\n",
    "        for edge in (g.es(_from=src, _to=dst) or [g.add_edge(src, dst, weight=0)]):\n",
    "            edge[\"weight\"] += 1\n",
    "            \n",
    "    g.delete_vertices(dummy)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "\n",
    "\n",
    "def test_graph() -> ig.Graph:\n",
    "    g = ig.Graph()\n",
    "    v_asdf = g.add_vertex(\"a:asdf\")\n",
    "    v_qwer = g.add_vertex(\"b:qwer\")\n",
    "    g.add_edge(v_asdf, v_qwer, weight=1)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Comparing graphs by reducing them to tuples and sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "ListVertices = List[str]\n",
    "ListEdgesWeighted = List[Tuple[str, str, int]]\n",
    "GraphLists = Tuple[ListVertices, ListEdgesWeighted]\n",
    "\n",
    "\n",
    "def graph2sets(g: ig.Graph) -> GraphLists:\n",
    "    return (\n",
    "        sorted([v[\"name\"] for v in g.vs]),\n",
    "        sorted([tuple(sorted([v[\"name\"] for v in e.vertex_tuple])) + (e[\"weight\"],) for e in g.es])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mGraph sets for simple graph\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Graph sets for simple graph\n",
    "assert_(eq, actual=graph2sets(test_graph()), expected=([\"a:asdf\", \"b:qwer\"], [(\"a:asdf\", \"b:qwer\", 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mGraph sets for bit more complex graph\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Graph sets for bit more complex graph\n",
    "g = ig.Graph()\n",
    "for name in [\"a\", \"b\", \"c\", \"1\", \"2\", \"3\"]:\n",
    "    g.add_vertex(name)\n",
    "g.add_edge(\"a\", \"b\", weight=1)\n",
    "g.add_edge(\"c\", \"b\", weight=3)\n",
    "g.add_edge(\"1\", \"2\", weight=5)\n",
    "g.add_edge(\"3\", \"2\", weight=0)\n",
    "g.add_edge(\"b\", \"2\", weight=10)\n",
    "\n",
    "assert_(\n",
    "    eq,\n",
    "    expected=(\n",
    "        [\"1\", \"2\", \"3\", \"a\", \"b\", \"c\"],\n",
    "        [(\"1\", \"2\", 5), (\"2\", \"3\", 0), (\"2\", \"b\", 10), (\"a\", \"b\", 1), (\"b\", \"c\", 3)]\n",
    "    ),\n",
    "    actual=graph2sets(g)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mGraph sets don't care about vertex addition order\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Graph sets don't care about vertex addition order\n",
    "g1 = ig.Graph()\n",
    "g1.add_vertex(\"a\")\n",
    "g1.add_vertex(\"b\")\n",
    "g1.add_edge(\"a\", \"b\", weight=1)\n",
    "\n",
    "g2 = ig.Graph()\n",
    "g2.add_vertex(\"b\")\n",
    "g2.add_vertex(\"a\")\n",
    "g2.add_edge(\"b\", \"a\", weight=1)\n",
    "\n",
    "assert_(eq, ordered=graph2sets(g1), unordered=graph2sets(g2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mGraph sets when no edge\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Graph sets when no edge\n",
    "g = ig.Graph()\n",
    "for name in [\"a\", \"b\"]:\n",
    "    g.add_vertex(name)\n",
    "    \n",
    "assert_(eq, actual=graph2sets(g), expected=([\"a\", \"b\"], []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Unit tests for clique adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "\n",
    "def are_graphs_equal(**graphs: ig.Graph) -> ExplanationOnFailure:\n",
    "    sets = {name: graph2sets(g) for name, g in graphs.items()}\n",
    "    if eq(**sets):\n",
    "        return True\n",
    "    return Explanation(\"These graphs are not equal\", join_args([], sets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mAdding a clique without any artifact leaves the graph unchanged\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Adding a clique without any artifact leaves the graph unchanged\n",
    "expected = test_graph()\n",
    "actual = add_clique(test_graph(), [])\n",
    "assert_(are_graphs_equal, expected=expected, actual=actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mAdd a clique to an empty graph\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Add a clique to an empty graph\n",
    "expected = test_graph()\n",
    "actual = ig.Graph()\n",
    "assert_(are_graphs_equal, expected=expected, actual=add_clique(ig.Graph(), [(\"a\", \"asdf\"), (\"b\", \"qwer\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mAdding a clique of all new vertices\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Adding a clique of all new vertices\n",
    "clique: Sequence[Artifact] = [(\"c\", \"uiop\"), (\"d\", \"qwer\"), (\"e\", \"zxcv\"), (\"f\", \"hjkl\")]\n",
    "\n",
    "expected = test_graph()\n",
    "for t, v in clique:\n",
    "    expected.add_vertex(f\"{t}:{v}\")\n",
    "for s, e in [\n",
    "    (\"c:uiop\", \"d:qwer\"), (\"c:uiop\", \"e:zxcv\"), (\"c:uiop\", \"f:hjkl\"),\n",
    "    (\"d:qwer\", \"e:zxcv\"), (\"d:qwer\", \"f:hjkl\"),\n",
    "    (\"e:zxcv\", \"f:hjkl\")\n",
    "]:\n",
    "    expected.add_edge(s, e, weight=1)\n",
    "\n",
    "actual = add_clique(test_graph(), clique)\n",
    "assert_(are_graphs_equal, expected=expected, actual=actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mAdding a clique that intersects another one fully\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Adding a clique that intersects another one fully\n",
    "clique: Sequence[Artifact] = [(\"a\", \"asdf\"), (\"b\", \"qwer\"), (\"c\", \"zxcv\")]\n",
    "\n",
    "expected = test_graph()\n",
    "a = next(iter(expected.vs(name=\"a:asdf\")))\n",
    "b = next(iter(expected.vs(name=\"b:qwer\")))\n",
    "c = expected.add_vertex(\"c:zxcv\")\n",
    "for e in expected.es(_from=a, _to=b):\n",
    "    e[\"weight\"] = 2\n",
    "expected.add_edge(a, c, weight=1)\n",
    "expected.add_edge(b, c, weight=1)\n",
    "\n",
    "actual = add_clique(test_graph(), clique)\n",
    "assert_(are_graphs_equal, expected=expected, actual=actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mAdding a clique that intersects another one by one vertex only\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Adding a clique that intersects another one by one vertex only\n",
    "clique = [(\"b\", \"qwer\"), (\"c\", \"zxcv\")]\n",
    "\n",
    "expected = test_graph()\n",
    "b = next(iter(expected.vs(name=\"b:qwer\")))\n",
    "c = expected.add_vertex(\"c:zxcv\")\n",
    "expected.add_edge(b, c, weight=1)\n",
    "\n",
    "actual = add_clique(test_graph(), clique)\n",
    "assert_(are_graphs_equal, expected=expected, actual=actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mAdding a clique that intersects another partly, but by more than a single vertex\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Adding a clique that intersects another partly, but by more than a single vertex\n",
    "clique = [(\"a\", \"asdf\"), (\"c\", \"zxcv\"), (\"d\", \"hjkl\")]\n",
    "\n",
    "expected = test_graph()\n",
    "a = next(iter(expected.vs(name=\"a:asdf\")))\n",
    "b = next(iter(expected.vs(name=\"b:qwer\")))\n",
    "c = expected.add_vertex(\"c:zxcv\")\n",
    "expected.add_edge(a, c, weight=2)\n",
    "expected.add_edge(b, c, weight=1)\n",
    "d = expected.add_vertex(\"d:hjkl\")\n",
    "expected.add_edge(a, d, weight=1)\n",
    "expected.add_edge(c, d, weight=1)\n",
    "\n",
    "actual = test_graph()\n",
    "actual.add_vertex(\"c:zxcv\")\n",
    "actual.add_edge(a, c, weight=1)\n",
    "actual.add_edge(b, c, weight=1)\n",
    "add_clique(actual, clique)\n",
    "\n",
    "assert_(are_graphs_equal, expected=expected, actual=actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "from memo import memo\n",
    "\n",
    "\n",
    "@memo\n",
    "def build_graph_artifacts(sh: StreamHeterogeneous, arch: ArchGraph) -> ig.Graph:\n",
    "    graph = ig.Graph()\n",
    "    for name_stream, df_stream in sh.items():\n",
    "        schema: Schema = {n: str(t) for n, t in df_stream.dtypes.items()}\n",
    "        for event in df_stream.itertuples():\n",
    "            add_clique(graph, arch.get_artifacts(event, schema))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mBuilding an artifact graph\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Building an artifact graph\n",
    "from memo import suspending_memoization\n",
    "\n",
    "sh = {\n",
    "    \"asdf\": pd.DataFrame(\n",
    "        data={\n",
    "            \"X\": [\"a.b.a.b\", \"a.c.c.b\", \"a.b.a.c\", \"a.a.a.a\", \"b.a.b.c\"],\n",
    "            \"Y\": [\"443\", \"21\", \"22\", \"443\", \"443\"]\n",
    "        },\n",
    "        index=[1, 3, 7, 8, 9]\n",
    "    ),\n",
    "    \"qwer\": pd.DataFrame(\n",
    "        data={\n",
    "            \"W\": [\"10.2.3.1\", \"10.2.128.3\", \"10.2.1.1\", \"10.10.1.1\", \"10.2.128.38\", \"10.10.1.43\"],\n",
    "            \"X\": [\"a.b.a.b\", \"a.e.a.d\", \"a.a.b.b\", \"a.b.a.c\", \"b.c.b.c\", \"a.a.a.a\"],\n",
    "            \"Z\": [\"asdf.exe\", \"qwer.exe\", \"zxcv.exe\", \"qwer.exe\", \"./hkjl\", \"asdf.exe\"]\n",
    "        },\n",
    "        index=[0, 3, 4, 6, 8, 9]\n",
    "    )\n",
    "}\n",
    "\n",
    "expected = ig.Graph()\n",
    "for v in [\n",
    "    \"X:a.b.a.b\", \"X:a.c.c.b\", \"X:a.b.a.c\", \"X:a.a.a.a\", \"X:b.a.b.c\", \"X:a.e.a.d\", \"X:a.a.b.b\", \"X:b.c.b.c\",\n",
    "    \"Y:443\", \"Y:21\", \"Y:22\",\n",
    "    \"W:10.2.3.1\", \"W:10.2.128.3\", \"W:10.2.1.1\", \"W:10.10.1.1\", \"W:10.2.128.38\", \"W:10.10.1.43\",\n",
    "    \"Z:asdf.exe\", \"Z:qwer.exe\", \"Z:zxcv.exe\", \"Z:./hkjl\"\n",
    "]:\n",
    "    expected.add_vertex(v)\n",
    "for s, e in zip(\n",
    "    [\"X:a.b.a.b\", \"X:a.c.c.b\", \"X:a.b.a.c\", \"X:a.a.a.a\", \"X:b.a.b.c\"],\n",
    "    [\"Y:443\", \"Y:21\", \"Y:22\", \"Y:443\", \"Y:443\"]\n",
    "):\n",
    "    expected.add_edge(s, e, weight=1)\n",
    "for a, b, c in zip(\n",
    "    [\"W:10.2.3.1\", \"W:10.2.128.3\", \"W:10.2.1.1\", \"W:10.10.1.1\", \"W:10.2.128.38\", \"W:10.10.1.43\"],\n",
    "    [\"X:a.b.a.b\", \"X:a.e.a.d\", \"X:a.a.b.b\", \"X:a.b.a.c\", \"X:b.c.b.c\", \"X:a.a.a.a\"],\n",
    "    [\"Z:asdf.exe\", \"Z:qwer.exe\", \"Z:zxcv.exe\", \"Z:qwer.exe\", \"Z:./hkjl\", \"Z:asdf.exe\"]\n",
    "):\n",
    "    expected.add_edge(a, b, weight=1)\n",
    "    expected.add_edge(a, c, weight=1)\n",
    "    expected.add_edge(b, c, weight=1)\n",
    "\n",
    "with suspending_memoization():\n",
    "    assert_(are_graphs_equal, expected=expected, actual=build_graph_artifacts(sh, AllObjects()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation learning by vertex feature aggregation\n",
    "\n",
    "The learning process is initiated with seed features, from which we compute a few iterations of ReFEx (**r**ecursive **fe**ature **ex**traction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Seed features\n",
    "\n",
    "Topological seed features for a vertex $v$:\n",
    "\n",
    "1. Vertex degree\n",
    "1. Internal egonet connectivity\n",
    "    - Number of edges of the subgraph $S(v)$ that are connected only between subgraph nodes distinct from $v$.\n",
    "1. External egonet connectivity\n",
    "    - Number of edges connected to a vertex of $S(v)$ distinct from $v$, and to a vertex outside of $S(v)$.\n",
    "1. Transitivity coefficient of $S(v)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "from typing import Mapping, Dict, Tuple, Callable\n",
    "\n",
    "\n",
    "Vertex2Index = Mapping[str, int]\n",
    "ExtractorFeaturesInit = Callable[[ig.Graph], Tuple[np.ndarray, Vertex2Index]]\n",
    "\n",
    "\n",
    "def seed_topological_basic(graph: ig.Graph) -> Tuple[np.ndarray, Vertex2Index]:\n",
    "    features = np.zeros((len(graph.vs), 4))\n",
    "    v2i: Dict[str, int] = {}\n",
    "    for i, vertex in enumerate(graph.vs):\n",
    "        family = [v.index for v in it.chain([vertex], vertex.neighbors())]\n",
    "        num_edges_total = len(graph.es(_incident=family))\n",
    "        num_edges_egonet = len(graph.es(_within=family))\n",
    "        degree = graph.degree(vertex)\n",
    "        connectivity_internal = num_edges_egonet - degree\n",
    "        connectivity_external = num_edges_total - num_edges_egonet\n",
    "        transitivity = graph.transitivity_local_undirected(vertices=vertex)\n",
    "        \n",
    "        v2i[vertex[\"name\"]] = i\n",
    "        features[i, :] = np.array([degree, connectivity_internal, connectivity_external, transitivity])\n",
    "        \n",
    "    return features, v2i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Graph under consideration for test:\n",
    "\n",
    "```\n",
    "(a)----(b)----(f)----(g)\n",
    " | \\  / | \\    |\n",
    " |  \\/  |  \\   |\n",
    " |  /\\  |   \\  |\n",
    " | /  \\ |    \\ |\n",
    "(c)----(d)    (e)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def graph_for_refex():\n",
    "    import igraph as ig\n",
    "    g = ig.Graph()\n",
    "    for v in \"abcdefg\":\n",
    "        g.add_vertex(v)\n",
    "    g.add_edge(\"a\", \"b\")\n",
    "    g.add_edge(\"a\", \"c\")\n",
    "    g.add_edge(\"a\", \"d\")\n",
    "    g.add_edge(\"b\", \"c\")\n",
    "    g.add_edge(\"b\", \"d\")\n",
    "    g.add_edge(\"c\", \"d\")\n",
    "    g.add_edge(\"b\", \"e\")\n",
    "    g.add_edge(\"b\", \"f\")\n",
    "    g.add_edge(\"e\", \"f\")\n",
    "    g.add_edge(\"f\", \"g\")\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def arrays_equal(**kwargs: np.ndarray) -> ExplanationOnFailure:\n",
    "    left, right = kwargs.values()\n",
    "    if left.size != right.size:\n",
    "        return Explanation(\"Arrays of distinct size\", join_args([], kwargs))\n",
    "    if not np.isclose(left, right, equal_nan=True, atol=1e-5).all():\n",
    "        return Explanation(\"Arrays not equal within tolerance\", join_args([], kwargs))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mSeed feature extraction\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Seed feature extraction\n",
    "import numpy as np\n",
    "\n",
    "g = graph_for_refex()\n",
    "features, v2i = seed_topological_basic(g)\n",
    "assert_(eq, expected=dict(zip(\"abcdefg\", range(7))), v2i=v2i)\n",
    "assert_(\n",
    "    arrays_equal,\n",
    "    expected=np.array(\n",
    "        [\n",
    "            [3, 3, 2, 1.0],\n",
    "            [5, 4, 1, 0.4],\n",
    "            [3, 3, 2, 1.0],\n",
    "            [3, 3, 2, 1.0],\n",
    "            [2, 1, 4, 1.0],\n",
    "            [3, 1, 3, 1.0 / 3.0],\n",
    "            [1, 0, 2, np.nan]\n",
    "        ],\n",
    "        dtype=np.float64\n",
    "    ),\n",
    "    features=features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature aggregation across graph topology\n",
    "\n",
    "Necessary elements for a ReFEx-type aggregator:\n",
    "\n",
    "1. Selection of features from the neighbours of a given vertex.\n",
    "1. Configurable aggregator functions.\n",
    "1. Recursive application of aggregation of selected features over all nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Feature subset selection based on vertex neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def list_neighbor_indices(graph: ig.Graph, vertex: ig.Vertex) -> List[int]:\n",
    "    return sorted([v.index for v in vertex.neighbors()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mList neighbor features\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "@suite.test(name=\"List neighbor features\")\n",
    "def test():\n",
    "    graph = graph_for_refex()\n",
    "    _, vindex = seed_topological_basic(graph)\n",
    "    for name, neighbours in [\n",
    "        (\"a\", \"bcd\"),\n",
    "        (\"b\", \"acdfe\"),\n",
    "        (\"c\", \"abd\"),\n",
    "        (\"d\", \"abc\"),\n",
    "        (\"e\", \"bf\"),\n",
    "        (\"f\", \"beg\"),\n",
    "        (\"g\", \"f\")\n",
    "    ]:\n",
    "        nn = sorted([vindex[n] for n in neighbours])\n",
    "        v, *_ = graph.vs(name=name)\n",
    "        assert_(eq, neighbours=list_neighbor_indices(graph, v), expected=nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Aggregator functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Aggregator = Callable[[np.ndarray], np.ndarray]  # Dimensions: m x n -> m x (2n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def agg_numpy(fn: Callable[..., np.ndarray]) -> Aggregator:\n",
    "    def _agg(ar: np.ndarray) -> np.ndarray:\n",
    "        result_masked = fn(np.ma.array(ar, mask=np.isnan(ar)), axis=0)\n",
    "        result = result_masked.data\n",
    "        if result_masked.mask.any():\n",
    "            result[result_masked.mask] = np.nan\n",
    "        return result\n",
    "    return _agg\n",
    "\n",
    "\n",
    "agg_sum = agg_numpy(np.sum)\n",
    "agg_mean = agg_numpy(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test_arrays() -> Sequence[np.ndarray]:\n",
    "    return [\n",
    "        np.empty((0, 1)),\n",
    "        np.empty((0, 11)),\n",
    "        np.array([[3], [0], [-2]]),\n",
    "        np.array([[3, 5, 0], [1, 3, -2], [-6, 8, 9], [-9, -4, 1]]),\n",
    "        np.array([[4, 2], [np.nan, 12], [2, np.nan]])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mSum aggregator\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Sum aggregator\n",
    "ar = test_arrays()\n",
    "assert_(arrays_equal, expected=np.full((1, 1), np.nan), agg_sum=agg_sum(ar[0]))\n",
    "assert_(arrays_equal, expected=np.full((1, 11), np.nan), agg_sum=agg_sum(ar[1]))\n",
    "assert_(arrays_equal, expected=np.array([[-11, 12, 8]]), agg_sum=agg_sum(ar[3]))\n",
    "assert_(arrays_equal, expected=np.array([[6, 14]]), agg_sum=agg_sum(ar[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mMean aggregator\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Mean aggregator\n",
    "ar = test_arrays()\n",
    "assert_(arrays_equal, expected=np.full((1, 1), np.nan), agg_mean1=agg_mean(ar[0]))\n",
    "assert_(arrays_equal, expected=np.full((1, 11), np.nan), agg_mean2=agg_mean(ar[1]))\n",
    "assert_(arrays_equal, expected=np.array([[1 / 3]]), agg_mean3=agg_mean(ar[2]))\n",
    "assert_(arrays_equal, expected=np.array([[-11 / 4, 3, 2]]), agg_mean4=agg_mean(ar[3]))\n",
    "assert_(arrays_equal, expected=np.array([[3, 7]]), agg_mean4=agg_mean(ar[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A single aggregation iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def aggregate_once(graph: ig.Graph, features: np.ndarray, cols_input: int, aggregators: Sequence[Aggregator]) -> int:\n",
    "    for i, agg in enumerate(aggregators, start=1):\n",
    "        index_col = i * cols_input\n",
    "        for v in graph.vs:\n",
    "            i_neighbors = list_neighbor_indices(graph, v)\n",
    "            features[v.index, index_col:index_col + cols_input] = agg(features[i_neighbors, 0:cols_input])\n",
    "    return cols_input * (len(aggregators) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mSingle aggregation iteration, one aggregator\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Single aggregation iteration, one aggregator\n",
    "G = graph_for_refex()\n",
    "features = np.concatenate((\n",
    "    np.array([\n",
    "        [3, 3, 2, 1.0],\n",
    "        [5, 4, 1, 0.4],\n",
    "        [3, 3, 2, 1.0],\n",
    "        [3, 3, 2, 1.0],\n",
    "        [2, 1, 4, 1.0],\n",
    "        [3, 1, 3, 1.0 / 3.0],\n",
    "        [1, 0, 2, np.nan]\n",
    "    ]),\n",
    "    np.zeros((7, 4))\n",
    "), axis=1)\n",
    "assert_(eq, expected=8, actual=aggregate_once(G, features, 4, [agg_sum]))\n",
    "assert_(\n",
    "    arrays_equal,\n",
    "    expected=np.array([\n",
    "        [3, 3, 2, 1.0, 11.0, 10.0, 5.0, 2.4],\n",
    "        [5, 4, 1, 0.4, 14.0, 11.0, 13.0, 4.0 + 1.0 / 3.0],\n",
    "        [3, 3, 2, 1.0, 11.0, 10.0, 5.0, 2.4],\n",
    "        [3, 3, 2, 1.0, 11.0, 10.0, 5.0, 2.4],\n",
    "        [2, 1, 4, 1.0, 8.0, 5.0, 4.0, 0.4 + 1.0 / 3.0],\n",
    "        [3, 1, 3, 1.0 / 3.0, 8.0, 5.0, 7.0, 1.4],\n",
    "        [1, 0, 2, np.nan, 3.0, 1.0, 3.0, 1.0 / 3.0]\n",
    "    ]),\n",
    "    actual=features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mSingle aggregation iteration, two aggregators\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Single aggregation iteration, two aggregators\n",
    "G = graph_for_refex()\n",
    "features = np.concatenate((\n",
    "    np.array([\n",
    "        [3, 3],\n",
    "        [5, 4],\n",
    "        [3, 3],\n",
    "        [3, 3],\n",
    "        [2, 1],\n",
    "        [3, 1],\n",
    "        [1, 0]\n",
    "    ]),\n",
    "    np.zeros((7, 4))\n",
    "), axis=1)\n",
    "assert_(eq, expected=6, actual=aggregate_once(G, features, 2, [agg_sum, agg_mean]))\n",
    "assert_(\n",
    "    arrays_equal,\n",
    "    expected=np.array([\n",
    "        [3, 3, 11.0, 10.0, 11.0 / 3.0, 10.0 / 3.0],\n",
    "        [5, 4, 14.0, 11.0, 14.0 / 5.0, 11.0 / 5.0],\n",
    "        [3, 3, 11.0, 10.0, 11.0 / 3.0, 10.0 / 3.0],\n",
    "        [3, 3, 11.0, 10.0, 11.0 / 3.0, 10.0 / 3.0],\n",
    "        [2, 1, 8.0, 5.0, 4.0, 2.5],\n",
    "        [3, 1, 8.0, 5.0, 8.0 / 3.0, 5.0 / 3.0],\n",
    "        [1, 0, 3.0, 1.0, 3.0, 1.0]\n",
    "    ]),\n",
    "    actual=features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def aggregate_features(\n",
    "    graph: ig.Graph,\n",
    "    seed: np.ndarray,\n",
    "    aggregators: Sequence[Aggregator],\n",
    "    num_iter: int = 3\n",
    ") -> np.ndarray:\n",
    "    num_vertices, num_seeds = seed.shape\n",
    "\n",
    "    def num_features_after_iter(i: int) -> int:\n",
    "        return num_seeds * (len(aggregators) + 1) ** i\n",
    "\n",
    "    features = np.zeros((num_vertices, num_features_after_iter(num_iter)))\n",
    "    features[:, 0:num_seeds] = seed\n",
    "    for i in range(num_iter):\n",
    "        aggregate_once(graph, features, num_features_after_iter(i), aggregators)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue ** Test \u001b[1m\u001b[1mAggregation: two features, one aggregator, one iteration\u001b[0m\u001b[0m ** \u001b[33mFailure\u001b[0m\n",
      "Arrays of distinct size:\n",
      "\n",
      "    actual   => [[ 3.   3.   2.   1.  11.  10.   5.   2. ]\n",
      " [ 5.   4.   1.   nan 14.  11.  13.   4.4]\n",
      " [ 3.   3.   2.   1.  11.  10.   5.   2. ]\n",
      " [ 3.   3.   2.   1.  11.  10.   5.   2. ]\n",
      " [ 2.   1.   4.   1.   8.   5.   4.   0.4]\n",
      " [ 3.   1.   3.   0.4  8.   5.   7.   1. ]\n",
      " [ 1.   0.   2.   nan  3.   1.   3.   0.4]]\n",
      "    expected => [[ 3.  3. 11. 10.]\n",
      " [ 5.  4. 14. 11.]\n",
      " [ 3.  3. 11. 10.]\n",
      " [ 3.  3. 11. 10.]\n",
      " [ 2.  1.  8.  5.]\n",
      " [ 3.  1.  8.  5.]\n",
      " [ 1.  0.  3.  1.]]\n"
     ]
    }
   ],
   "source": [
    "%%test Aggregation: two features, one aggregator, one iteration\n",
    "G = graph_for_refex()\n",
    "seed = np.array([\n",
    "    [3, 3, 2, 1.0],\n",
    "    [5, 4, 1, np.nan],\n",
    "    [3, 3, 2, 1.0],\n",
    "    [3, 3, 2, 1.0],\n",
    "    [2, 1, 4, 1.0],\n",
    "    [3, 1, 3, 0.4],\n",
    "    [1, 0, 2, np.nan]\n",
    "])\n",
    "assert_(\n",
    "    arrays_equal,\n",
    "    expected=np.array([\n",
    "        [3, 3, 11.0, 10.0],\n",
    "        [5, 4, 14.0, 11.0],\n",
    "        [3, 3, 11.0, 10.0],\n",
    "        [3, 3, 11.0, 10.0],\n",
    "        [2, 1, 8.0, 5.0],\n",
    "        [3, 1, 8.0, 5.0],\n",
    "        [1, 0, 3.0, 1.0]\n",
    "    ]),\n",
    "    actual=aggregate_features(G, seed, [agg_sum], 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mAggregation: 3 features, 2 aggregators, 2 iterations\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Aggregation: 3 features, 2 aggregators, 2 iterations\n",
    "G = graph_for_refex()\n",
    "seed = np.array([\n",
    "    [3, 3, 1.0],\n",
    "    [5, 4, 0.4],\n",
    "    [3, 3, 1.0],\n",
    "    [3, np.nan, 1.0],\n",
    "    [2, 1, 1.0],\n",
    "    [3, 1, 1.0 / 3.0],\n",
    "    [1, 0, np.nan]\n",
    "])\n",
    "assert_(\n",
    "    arrays_equal,\n",
    "    expected=np.array([\n",
    "        [3, 3     , 1       , 11,  7, 2.4     , 3.666666, 3.5     , 0.8     , 11,  7, 2.4     , 36, 25, 9.133333, 10.133333,  8.833333, 2.466666, 3.666666, 3.5     , 0.8     , 12       , 8.333333, 3.044444, 3.377777, 2.944444, 0.822222],\n",
    "        [5, 4     , 0.4     , 14,  8, 4.333333, 2.8     , 2       , 0.866666, 14,  8, 4.333333, 49, 34, 9.333333, 17.666666, 14.5     , 3.466666, 2.8     , 2       , 0.866666,  9.8     , 6.8     , 1.866666, 3.533333, 2.9     , 0.693333],\n",
    "        [3, 3     , 1.0     , 11,  7, 2.4     , 3.666666, 3.5     , 0.8     , 11,  7, 2.4     , 36, 25, 9.133333, 10.133333,  8.833333, 2.466666, 3.666666, 3.5     , 0.8     , 12       , 8.333333, 3.044444, 3.377777, 2.944444, 0.822222],\n",
    "        [3, np.nan, 1.0     , 11, 10, 2.4     , 3.666666, 3.333333, 0.8     , 11, 10, 2.4     , 36, 22, 9.133333, 10.133333,  9       , 2.466666, 3.666666, 3.333333, 0.8     , 12       , 7.333333, 3.044444, 3.377777, 3       , 0.822222],\n",
    "        [2, 1     , 1.0     ,  8,  5, 0.733333, 4       , 2.5     , 0.366666,  8,  5, 0.733333, 22, 13, 5.733333,  5.466666,  3.666666, 1.566666, 4       , 2.5     , 0.366666, 11       , 6.5     , 2.866666, 2.733333, 1.833333, 0.783333],\n",
    "        [3, 1     , 0.333333,  8,  5, 1.4     , 2.666666, 1.666666, 0.7     ,  8,  5, 1.4     , 25, 14, 5.4     ,  9.8     ,  5.5     , 1.566666, 2.666666, 1.666666, 0.7     ,  8.333333, 4.666666, 1.8     , 3.266666, 1.833333, 0.522222],\n",
    "        [1, 0     , np.nan  ,  3,  1, 0.333333, 3       , 1       , 0.333333,  3,  1, 0.333333,  8,  5, 1.4     ,  2.666666,  1.666666, 0.7     , 3,        1       , 0.333333,  8       , 5       , 1.4     , 2.666666, 1.666666, 0.7]\n",
    "    ]),\n",
    "    actual=aggregate_features(G, seed, [agg_sum, agg_mean], 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension reduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension reduction approach taken by [Palladino and Thissen](https://arxiv.org/abs/1812.02848) consists of a nonnegative factorization of the feature matrix obtained by aggregation. The reduced features are even normalized so they may be interpreted as *role belonging probabilities*. Since this role belonging property caters to the authors' approach to anomaly detection out of the vector signals, we will compute this normalization once we reach this later stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-negative matrix factorization (classic RolX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.decomposition as skd\n",
    "\n",
    "\n",
    "class MethodFailedToConverge(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def reduce_dim_nmf(\n",
    "    features: np.ndarray,\n",
    "    dim: int,\n",
    "    max_iter: int = 500,\n",
    "    tol: float = 1e-4,\n",
    "    raise_on_convergence_failure: bool = True\n",
    ") -> np.ndarray:\n",
    "    _, num_features = features.shape\n",
    "    if dim >= num_features:\n",
    "        raise ValueError(\n",
    "            f\"Provided dimension {dim} is not a reduction from current vertex embedding dimension {num_features}\"\n",
    "        )\n",
    "    model = skd.NMF(n_components=dim, solver=\"mu\", beta_loss=\"kullback-leibler\", tol=tol, max_iter=max_iter)\n",
    "    features_reduced = model.fit_transform(features)\n",
    "    if raise_on_convergence_failure and model.n_iter_ >= max_iter:\n",
    "        raise MethodFailedToConverge()\n",
    "    return features_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mReducing dimension of a matrix\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Reducing dimension of a matrix\n",
    "R = reduce_dim_nmf(np.random.rand(100, 100), 10, max_iter=500)\n",
    "assert_(eq, Rshape=R.shape, expected=(100, 10))\n",
    "assert (R >= 0.0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mRaising on failure to converge\u001b[0m passed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/miniconda3/envs/crab/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1075: ConvergenceWarning: Maximum number of iteration 10 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iteration %d reached. Increase it to\"\n"
     ]
    }
   ],
   "source": [
    "%%test Raising on failure to converge\n",
    "try:\n",
    "    R = reduce_dim_nmf(np.random.rand(100, 100), 10, max_iter=10)\n",
    "    fail(\"Supposed to raise because the factorization would have failed to converge.\")\n",
    "except MethodFailedToConverge:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test result summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 passed, \u001b[37m0 failed\u001b[0m, \u001b[37m0 raised an error\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    _ = summarize_results(suite)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "307px",
    "width": "378px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "343.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
