{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on --max_line_length 120 --ignore W293,E302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from contextlib import contextmanager\n",
    "from glob import glob\n",
    "import gzip\n",
    "import json\n",
    "from multiprocessing import cpu_count\n",
    "from numbers import Number\n",
    "import os\n",
    "import os.path as op\n",
    "import pickle\n",
    "import sys\n",
    "from typing import Tuple, Sequence, Mapping, Callable, Any, Optional, ContextManager, List\n",
    "\n",
    "import dask\n",
    "from distributed import Client, LocalCluster\n",
    "import igraph as ig\n",
    "import matplotlib.pyplot as pp\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection of heterogeneous telemetry data using graph embeddings\n",
    "\n",
    "This anomaly detector is derived from [this paper](https://arxiv.org/abs/1812.02848), whose author applied the methodology to augment the indicator strengths of a heterogeneous alert feed. We apply it here to raw event data that typically feed the heuristic detectors of such alerting appliances.\n",
    "\n",
    "We demonstrate using the [Los Alamos Cybersecurity](https://csr.lanl.gov/data/cyber1/) dataset, which we use to locate offensive events as anomalous behaviour. The dataset is composed of four streams of telemetry of distinct schema that report various perspective of IT behaviour within the Los Alamos National Laboratory IT infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_DATA = \"/data/lanl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do change `DIR_DATA` so it points to a directory organized as expected by [this notebook](https://github.com/hamelin/lanl-tools/blob/master/Los%20Alamos%20Cybersecurity%20Data%20partitioning.ipynb), whose functions we use to split the dataset into time intervals.\n",
    "\n",
    "As we will go along, we will add experiment configuration to an instance of the following class. Let's make it so we can easily add methods to that class as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "\n",
    "    def __init__(self, name) -> None:\n",
    "        self._name = name\n",
    "        \n",
    "    def method(self, fn) -> Callable:\n",
    "        def _method(*args, **kwargs) -> Callable:\n",
    "            return fn(self, *args, **kwargs)\n",
    "\n",
    "        setattr(self, fn.__name__, _method)\n",
    "        return fn\n",
    "    \n",
    "    def __dask_tokenize__(self, *args, **kwargs) -> str:\n",
    "        return self._name\n",
    "\n",
    "\n",
    "cfg = Config(\"the-experiment\")\n",
    "cfg.path_lanl = DIR_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cfg.method\n",
    "def join_raw(self, *p) -> os.PathLike:\n",
    "    return op.join(self.path_lanl, \"raw\", *p)\n",
    "\n",
    "\n",
    "assert op.isdir(cfg.join_raw())\n",
    "assert op.isfile(cfg.join_raw(\"redteam.txt.gz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelude: dividing the data streams in workable chunks\n",
    "\n",
    "When we download the dataset, it comes as five quasi-CSV files (the column header is absent) respectively compressed in one Gzip block. This file organization makes parallel processing of substreams nearly impossible.\n",
    "\n",
    "Given the documentation on the dataset, we know it translates very easily to dataframes. Let's divide the monolithic compressed streams into compressed chunks, each of which can become a *partition* of a Dask dataframe. Best practices when working with Dask dataframes suggest using partitions of about 100MB in size. We will not use Dask dataframes, but our handling of 100MB-partitioned data with delayed tasks should work along the same constraints and best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_CHUNK = \"100M\"\n",
    "DIR_RAW = op.join(DIR_DATA, \"raw\")\n",
    "DIR_CHUNKED = op.join(DIR_DATA, \"chunked\")\n",
    "os.makedirs(DIR_CHUNKED, exist_ok=True)\n",
    "DIR_RAW, DIR_CHUNKED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STREAMS_TO_CHUNK = \"\\n\".join(\n",
    "    {\"auth\", \"dns\", \"flows\", \"proc\"} - set(os.listdir(DIR_CHUNKED))\n",
    ")\n",
    "STREAMS_TO_CHUNK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `split` UNIX command is a perfect fit for this task. Composeit with `xargs` to chunk as-yet unchunked streams (file `redteam.txt.gz` is not so much a distinct data stream as a labeler for the `auth.txt.gz` records) in parallel. Unfortunately, the `!` magic precludes good visibility of the command; copy-paste it to a raw cell for examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!echo \"$STREAMS_TO_CHUNK\" | time xargs -I^ -P0 -t bash -x -c \"mkdir -p $DIR_CHUNKED/^ && zcat $DIR_RAW/^.txt.gz | split --verbose -d -a 4 -C $SIZE_CHUNK --additional-suffix=.txt.gz --filter='gzip -c >\\$FILE' - $DIR_CHUNKED/^/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the time interval size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at the offensive logons labeling data so that we can determine a data subset and time interval length that would suit the analysis. Naturally, we don't have such information in real life, but the constraints around data capture can initiate a trial-and-error for setting the time resolution at which to perform anomaly detection. It may also be fruitful to run anomaly detection using large time intervals and periods over certain datasets, and small intervals and periods and over other datasets; the practitioner will have to use the methodology and adapt it to their needs.\n",
    "\n",
    "The moment where data was captured is not documented. As the dataset was published sometimes in 2015, let's anchor it to January 1st, 2015 -- just so we don't have to deal with weird timestamps relative to early Epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGIN = pd.Timestamp(\"2015-01-01T00:00:00\")\n",
    "ORIGIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN = Tuple[str, str]\n",
    "SCHEMA = Sequence[COLUMN]\n",
    "\n",
    "\n",
    "def read_lanl_csv(path: os.PathLike, schema: SCHEMA, **kwargs: Any) -> pd.DataFrame:\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        header=None,\n",
    "        names=[\"time\"] + [attr for attr, _ in schema],\n",
    "        dtype=dict(schema),\n",
    "        parse_dates=[\"time\"],\n",
    "        date_parser=lambda n: ORIGIN + pd.Timedelta(seconds=int(n)),\n",
    "        index_col=\"time\",\n",
    "        compression=\"gzip\",\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_labels = read_lanl_csv(\n",
    "    cfg.join_raw(\"redteam.txt.gz\"),\n",
    "    [(\"user_domain_source\", \"object\"), (\"computer_source\", \"object\"), (\"computer_destination\", \"object\")],\n",
    "    squeeze=True,\n",
    "    usecols=[\"time\", \"computer_source\"],\n",
    ")\n",
    "sr_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last day where an attack occurs is January 30th; let's consider the attacks happen over the whole month of January, so from January 1st to 31st."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.time_frame = Config(\"time_frame\")\n",
    "cfg.time_frame.start = ORIGIN\n",
    "cfg.time_frame.end = pd.Timestamp(\"2015-02-01T00:00:00\")\n",
    "tf = cfg.time_frame\n",
    "\n",
    "\n",
    "@tf.method\n",
    "def width(self) -> pd.Timedelta:\n",
    "    return self.end - self.start\n",
    "\n",
    "\n",
    "@tf.method\n",
    "def range_end(self) -> pd.Timestamp:\n",
    "    return self.end - pd.Timedelta(1)\n",
    "\n",
    "\n",
    "@tf.method\n",
    "def empty(self) -> pd.DatetimeIndex:\n",
    "    return pd.date_range(self.start, self.start - pd.Timedelta(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the repartition of attack events over various time intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attacks(sr: pd.Series, cfg: Config, **timedelta_params: Number) -> None:\n",
    "    interval = pd.Timedelta(**timedelta_params)\n",
    "    time_frame = cfg.time_frame.width()\n",
    "    num_intervals = time_frame // interval + min(1, (time_frame % interval).value)\n",
    "    sr.groupby(lambda ts: ((ts - cfg.time_frame.start) // interval) * interval + cfg.time_frame.start)\\\n",
    "        .count()\\\n",
    "        .reindex(pd.date_range(cfg.time_frame.start, cfg.time_frame.end, freq=interval), fill_value=0)\\\n",
    "        .plot(kind=\"bar\", figsize=(15, 4), xticks=np.linspace(0, num_intervals, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attacks(sr_labels, cfg, days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dispersion of attack numbers is very wide here: major incidents range in the hundreds of offensive events, while minor incidents run below 10 offensive events. Likely, such minor incidents, which are crucial to preparing the larger assaults, would be drowned into the noise of normal activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_attacks(sr_labels, cfg, hours=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dispersion of the number of hacking events is less drastic, no more than one order of magnitudes between small and large incidents. In such a case, the many-attacks events between Jan 9 and Jan 14 would likely be detected; small incidents also run a detection chance, if somewhat lesser. Nonetheless, let's run a first analysis at this time resolution. **Decision executed in code cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.interval = pd.Timedelta(hours=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cfg.method\n",
    "def calc_lb_interval(self, ts: pd.Timestamp) -> pd.Timestamp:\n",
    "    return ((ts - self.time_frame.start) // self.interval) * self.interval + self.time_frame.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cfg.method\n",
    "def index_partition(self) -> pd.DatetimeIndex:\n",
    "    return pd.date_range(self.time_frame.start, self.time_frame.range_end(), freq=self.interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning data streams\n",
    "\n",
    "The anomaly detection methodology acts on event subsequences. Let's reorganize these streams according to the chosen time interval. For the sake of partition reuse for various experiments, let's process the full data streams.\n",
    "\n",
    "Through this partitioning process, we will parse the raw data streams. Let's care for the data schema for each data stream as we do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STREAMS: Mapping[str, SCHEMA] = {\n",
    "    \"dns\": [\n",
    "        (\"computer_source\", \"object\"),\n",
    "        (\"computer_destination\", \"object\")\n",
    "    ],\n",
    "    \"flows\": [\n",
    "        (\"duration\", \"int64\"),\n",
    "        (\"computer_source\", \"object\"),\n",
    "        (\"port_source\", \"object\"),\n",
    "        (\"computer_destination\", \"object\"),\n",
    "        (\"port_destination\", \"object\"),\n",
    "        (\"protocol\", \"category\"),\n",
    "        (\"num_packets\", \"int32\"),\n",
    "        (\"num_bytes\", \"int64\")\n",
    "    ],\n",
    "    \"proc\": [\n",
    "        (\"userdomain_source\", \"object\"),\n",
    "        (\"computer_source\", \"object\"),\n",
    "        (\"process\", \"object\"),\n",
    "        (\"action\", \"category\")\n",
    "    ],\n",
    "    \"auth\": [\n",
    "        (\"userdomain_source\", \"object\"),\n",
    "        (\"userdomain_destination\", \"object\"),\n",
    "        (\"computer_source\", \"object\"),\n",
    "        (\"computer_destination\", \"object\"),\n",
    "        (\"auth\", \"category\"),\n",
    "        (\"logon\", \"category\"),\n",
    "        (\"direction\", \"category\"),\n",
    "        (\"result\", \"category\")\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partitioning computations will be run through a Dask compute cluster. If you would rather start your own cluster internally, comment out the next cell and uncomment and run the one after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=cpu_count(), threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = Client(\"localhost:8786\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cfg.method\n",
    "def join_partition(self, *p: os.PathLike) -> os.PathLike:\n",
    "    return op.join(self.path_lanl, \"partitions\", self.interval.isoformat(), *p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_partition(path_partition: os.PathLike, ts: pd.Timestamp) -> os.PathLike:\n",
    "    return op.join(path_partition, ts.isoformat() + \".txt.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_partition(df_partition: Optional[pd.DataFrame], path_partition: os.PathLike, mode: str = \"w\") -> None:\n",
    "    if df_partition is not None:\n",
    "        assert len(df_partition) > 0\n",
    "        ts_interval = df_partition.interval.iloc[0]\n",
    "        \n",
    "        df_partition = df_partition.drop(\"interval\", axis=\"columns\").reset_index()\n",
    "        df_partition.time = df_partition.time.apply(lambda ts: int((ts - ORIGIN).total_seconds()))\n",
    "\n",
    "        os.makedirs(path_partition, exist_ok=True)\n",
    "        df_partition.to_csv(\n",
    "            name_partition(path_partition, ts_interval),\n",
    "            mode=mode,\n",
    "            header=False,\n",
    "            index=False,\n",
    "            compression=\"gzip\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_chunk_metadata(path_chunk: os.PathLike, path_partition: os.PathLike) -> os.PathLike:\n",
    "    name_chunk, _ = op.splitext(op.basename(path_chunk))\n",
    "    return op.join(path_partition, \"meta\", name_chunk + \".json\")\n",
    "\n",
    "\n",
    "def get_chunk_metadata(path_chunk: os.PathLike, path_partition: os.PathLike) -> List[pd.Timestamp]:\n",
    "    path = get_path_chunk_metadata(path_chunk, path_partition)\n",
    "    if op.isfile(path):\n",
    "        with open(path, \"rt\", encoding=\"utf-8\") as file_meta:\n",
    "            return [pd.Timestamp(s) for s in json.load(file_meta)]\n",
    "    return []\n",
    "\n",
    "\n",
    "def persist_chunk_metadata(path_chunk: os.PathLike, path_partition: os.PathLike, meta: Sequence[pd.Timestamp]) -> None:\n",
    "    path = get_path_chunk_metadata(path_chunk, path_partition)\n",
    "    os.makedirs(op.dirname(path), exist_ok=True)\n",
    "    with open(path, \"wt\", encoding=\"utf-8\") as file_meta:\n",
    "        json.dump([ts.isoformat() for ts in meta], file_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extremes:\n",
    "    \n",
    "    def __init__(self, min_interval: pd.Timestamp, max_interval: pd.Timestamp, path_chunk: os.PathLike) -> None:\n",
    "        self.min_interval = min_interval\n",
    "        self.max_interval = max_interval\n",
    "        self.path_chunk = path_chunk\n",
    "        \n",
    "    def __dask_tokenize__(self, *args, **kwargs) -> str:\n",
    "        return \"\".join(str(a) for a in [self.min_interval, self.max_interval, self.path_chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunk(path: os.PathLike, schema: SCHEMA) -> pd.DataFrame:\n",
    "    chunk = read_lanl_csv(path, schema)\n",
    "    chunk[\"interval\"] = chunk.index.to_series().apply(cfg.calc_lb_interval)\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_file_exists(path_partition: os.PathLike, ts: pd.Timestamp) -> bool:\n",
    "    return op.isfile(name_partition(path_partition, ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed(pure=True)\n",
    "def partition_chunk(path_chunk: os.PathLike, stream: str, cfg: Config) -> Extremes:\n",
    "    path_partition = cfg.join_partition(stream)\n",
    "    intervals = get_chunk_metadata(path_chunk, path_partition)\n",
    "    if len(intervals) > 0:\n",
    "        # If a previous run wrote all the non-extreme partitions, we can skip chunk processing here.\n",
    "        if all(partition_file_exists(path_partition, ts) for ts in intervals[1:-1]):\n",
    "            return Extremes(intervals[0], intervals[-1], path_chunk)\n",
    "\n",
    "    chunk = read_chunk(path_chunk, STREAMS[stream])\n",
    "    intervals = [pd.Timestamp(ts) for ts in chunk.interval.unique()]\n",
    "\n",
    "    min_interval, max_interval = chunk.interval.agg(['min', 'max'])\n",
    "    for _, df_partition in chunk[(chunk.interval > min_interval) & (chunk.interval < max_interval)].groupby(\"interval\"):\n",
    "        write_partition(df_partition, path_partition)\n",
    "    \n",
    "    persist_chunk_metadata(path_chunk, path_partition, intervals)\n",
    "    return Extremes(min_interval, max_interval, path_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_partition_increment(\n",
    "    path_chunk: os.PathLike,\n",
    "    schema: SCHEMA,\n",
    "    path_partition: os.PathLike,\n",
    "    interval: pd.Timestamp\n",
    ") -> None:\n",
    "    chunk = read_chunk(path_chunk, schema)\n",
    "    write_partition(chunk[chunk.interval == interval], path_partition, mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed(pure=True)\n",
    "def consolidate_partial_partitions(\n",
    "    path_partition: os.PathLike,\n",
    "    stream: str,\n",
    "    extremes: Sequence[Extremes],\n",
    "    index: int\n",
    ") -> None:\n",
    "    extreme_here = extremes[index]\n",
    "    interval_target = extreme_here.min_interval\n",
    "\n",
    "    extremes_distinct: bool = (interval_target != extreme_here.max_interval)\n",
    "    is_last_consolidator: bool = (index == len(extremes) - 1)\n",
    "    if extremes_distinct or is_last_consolidator:\n",
    "        # This invocation is responsible for combining chunks {i} whose extreme has max_interval[i] == interval_target.\n",
    "        # This also covers the case of the first interval.\n",
    "        if not partition_file_exists(path_partition, interval_target):\n",
    "            for extreme in extremes:\n",
    "                if extreme.max_interval == interval_target:\n",
    "                    transfer_partition_increment(extreme.path_chunk, STREAMS[stream], path_partition, interval_target)\n",
    "            transfer_partition_increment(extreme_here.path_chunk, STREAMS[stream], path_partition, interval_target)\n",
    "            \n",
    "        # The last consolidator is also responsible for transferring the last interval; it has been done already\n",
    "        # if the two extremes were the same.\n",
    "        if all([\n",
    "            extremes_distinct,\n",
    "            is_last_consolidator,\n",
    "            not partition_file_exists(path_partition, extreme_here.max_interval)\n",
    "        ]):\n",
    "            transfer_partition_increment(extreme_here.path_chunk, STREAMS[stream], path_partition, extreme_here.max_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed(pure=True)\n",
    "def complete_partition(c, path_partition):\n",
    "    return path_partition\n",
    "    \n",
    "\n",
    "def partition_stream(stream: str, cfg: Config) -> dask.delayed:\n",
    "    path_partition = cfg.join_partition(stream)\n",
    "    \n",
    "    paths_chunks = sorted(glob(op.join(DIR_CHUNKED, stream, \"*.txt.gz\")))\n",
    "    extremes_with_paths: List[Extremes] = [\n",
    "        partition_chunk(path, stream, cfg)\n",
    "        for path in paths_chunks\n",
    "    ]\n",
    "    consolidated: Sequence[None] = [\n",
    "        consolidate_partial_partitions(path_partition, stream, extremes_with_paths, index)\n",
    "        for index in range(len(extremes_with_paths))\n",
    "    ]\n",
    "    \n",
    "    return complete_partition(consolidated, path_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ld = [partition_stream(stream, cfg) for stream in STREAMS.keys()]\n",
    "futs = client.compute(ld)\n",
    "ld, futs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dynamic artifact graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anomaly detection methodology associates each *artifact* of each event to a vertex in an *artifact graph*, a dynamic graph instantiated at each time interval of our partition; artifacts that appear in a given event are linked by an edge (or the weight of this edge is incremented), so that each event translates to a clique in the artifact graph.\n",
    "\n",
    "For each stream, each event attribute that's tied to a closed set of values can be used as an artifact. What event fields are associated to artifacts is the purpose of the *graph architecture object*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EventArtifacts = Sequence[str]\n",
    "EventAttributes = Mapping[str, Any]\n",
    "\n",
    "\n",
    "class GraphArch:\n",
    "    \n",
    "    NAME = \"dont-use-this\"\n",
    "    \n",
    "    def __init__(self, schema_data: SCHEMA) -> None:\n",
    "        self._schema = dict(schema_data)\n",
    "\n",
    "    @property\n",
    "    def schema(self) -> Mapping[str, str]:\n",
    "        return self._schema\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_event_artifacts(self, event: pd.Series) -> EventArtifacts:\n",
    "        return []\n",
    "\n",
    "    # Not used yet -- use it to determine how to integrate non-artifact attributes into\n",
    "    # embedding scheme.\n",
    "    def get_event_attributes(self, event: pd.Series) -> EventAttributes:\n",
    "        return {}\n",
    "    \n",
    "    def __dask_tokenize__(self, *args, **kwargs) -> str:\n",
    "        return type(self).NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we will use as artifacts every feature of categorical dtype. Even though the set of values for some of these attributes is so small as to question to meaning of including them in the artifact graph (they will be very highly connected vertices in all steps of the dynamic graph, with variations unlikely to be understood as anomalous), we pull them in until we have data to support their exclusion.\n",
    "\n",
    "In addition, the methodology of Palladino and Thissen does not specify how to take into account non-categorical attributes. We thus leave all of them out for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllCategoryNoAttr(GraphArch):\n",
    "    \n",
    "    NAME = \"all-category-no-attr\"\n",
    "\n",
    "    def get_event_artifacts(self, event: pd.Series) -> EventArtifacts:\n",
    "        return [\n",
    "            f\"{attr}::{value}\"\n",
    "            for attr, value in event.iteritems()\n",
    "            if self.schema.get(attr, \"\") in [\"object\", \"category\"]\n",
    "        ]\n",
    "\n",
    "\n",
    "cfg.graph_build = AllCategoryNoAttr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as not to lock ourselves to a certain graph handling library at this stage, let's hide it behind an abstraction. We are here involved with building graphs from events, so let's implement this abstraction according to the [Builder](https://en.wikipedia.org/wiki/Builder_pattern) design pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuilderGraph(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_clique(self, vertices: EventArtifacts, attr: EventAttributes) -> None:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_ext_path(self) -> str:\n",
    "        return \"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def write(self, path: os.PathLike) -> None:\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def absorb(self, path: os.PathLike) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here's a builder now that builds up NetworkX graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuilderNetworkX(BuilderGraph):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self._graph = nx.Graph()\n",
    "        \n",
    "    def add_clique(self, vertices: EventArtifacts, attributes: EventAttributes) -> None:\n",
    "        for i, left in enumerate(vertices):\n",
    "            for right in vertices[i + 1:]:\n",
    "                self._graph.add_edge(\n",
    "                    left,\n",
    "                    right,\n",
    "                    weight=self._graph.adj.get(left, {}).get(right, {\"weight\": 0})[\"weight\"] + 1,\n",
    "                    **attributes\n",
    "                )\n",
    "\n",
    "    def get_ext_path(self) -> str:\n",
    "        return \"nxz\"\n",
    "    \n",
    "    def write(self, path: os.PathLike) -> None:\n",
    "        with gzip.open(path, \"wb\") as file:\n",
    "            pickle.dump(self._graph, file)\n",
    "            \n",
    "    def absorb(self, path: os.PathLike) -> None:\n",
    "        with gzip.open(path, \"rb\") as file:\n",
    "            other = pickle.load(file)\n",
    "        for left, right in other.edges:\n",
    "            self._graph.add_edge(\n",
    "                left,\n",
    "                right,\n",
    "                weight=self._graph.adj.get(left, {}).get(right, {}).get(\"weight\", 0) + other[left][right][\"weight\"]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cfg.method\n",
    "def join_experiment(self, *p: os.PathLike) -> os.PathLike:\n",
    "    return op.join(\n",
    "        self.path_lanl,\n",
    "        \"experiments\",\n",
    "        self.interval.isoformat(),\n",
    "        f\"{self.time_frame.start.isoformat()}__{self.time_frame.end.isoformat()}\",\n",
    "        self.graph_build.NAME,\n",
    "        *p\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cfg.method\n",
    "def join_graphs(self, *p: os.PathLike) -> os.PathLike:\n",
    "    return self.join_experiment(\"graphs\", *p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have Pandas timestamps be tokenizable by Dask, so the graph building is understood as a *pure function* (in the sense of Dask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_tokenize(self, *args, **kwargs) -> str:\n",
    "    return self.isoformat()\n",
    "\n",
    "\n",
    "setattr(pd.Timestamp, \"__dask_tokenize__\", timestamp_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For building graphs, we will distribute computations per partition, and then by stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed(pure=True)\n",
    "def make_graph(\n",
    "    path_partition: os.PathLike,\n",
    "    ts: pd.Timestamp,\n",
    "    klass_builder: type,\n",
    "    cfg: Config\n",
    ") -> os.PathLike:\n",
    "    # Skip if this graph is already on disk.\n",
    "    builder = klass_builder()\n",
    "    stream = op.basename(path_partition)\n",
    "    path_graph = cfg.join_graphs(\"tmp\", f\"{stream}_{ts.isoformat()}.{builder.get_ext_path()}\")\n",
    "    if op.isfile(path_graph):\n",
    "        return path_graph\n",
    "\n",
    "    # Skip building this graph if the graph resulting from the combination of all stream-specific\n",
    "    # graphs is present.\n",
    "    if op.isfile(cfg.join_graphs(ts.isoformat() + \".pkl\")):\n",
    "        return path_graph\n",
    "\n",
    "    path_data = op.join(path_partition, ts.isoformat() + \".txt.gz\")\n",
    "    if op.isfile(path_data):\n",
    "        df_partition = read_lanl_csv(path_data, STREAMS[stream])\n",
    "    else:\n",
    "        df_partition = pd.DataFrame({name: [] for name, _ in STREAMS[stream]}, index=cfg.empty())\n",
    "    \n",
    "    arch = cfg.graph_build(STREAMS[stream])\n",
    "    for _, event in df_partition.iterrows():\n",
    "        builder.add_clique(arch.get_event_artifacts(event), arch.get_event_attributes(event))\n",
    "                \n",
    "    os.makedirs(op.dirname(path_graph), exist_ok=True)\n",
    "    builder.write(path_graph)\n",
    "\n",
    "    return path_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed(pure=True)\n",
    "def combine_graphs(\n",
    "    ts: pd.Timestamp,\n",
    "    klass_builder: type,\n",
    "    paths_tmp: Sequence[os.PathLike]\n",
    ") -> os.PathLike:\n",
    "    builder = klass_builder()\n",
    "    path_graph = cfg.join_graphs(f\"{ts.isoformat()}.{builder.get_ext_path()}\")\n",
    "    if op.isfile(path_graph):\n",
    "        return path_graph\n",
    "\n",
    "    for path in paths_tmp:\n",
    "        builder.absorb(path)\n",
    "\n",
    "    os.makedirs(op.dirname(path_graph), exist_ok=True)\n",
    "    builder.write(path_graph)\n",
    "        \n",
    "    for path in paths_tmp:\n",
    "        os.remove(path)\n",
    "        \n",
    "    return path_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STREAMS_INVOLVED = list(STREAMS.keys())\n",
    "# STREAMS_INVOLVED = [\"auth\"]\n",
    "\n",
    "PARTITIONS: Mapping[str, os.PathLike] = {\n",
    "    stream: partition_stream(stream, cfg)\n",
    "    for stream in STREAMS_INVOLVED\n",
    "}\n",
    "PARTITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPHS_BY_STREAM: Mapping[str, Mapping[str, Sequence[os.PathLike]]] = {\n",
    "    type_graph: {\n",
    "        stream: [\n",
    "            make_graph(path_partition, ts, klass_builder, cfg)\n",
    "            for ts in cfg.index_partition()\n",
    "        ]\n",
    "        for stream, path_partition in PARTITIONS.items()\n",
    "    }\n",
    "    for type_graph, klass_builder in [(\"networkx\", BuilderNetworkX)]\n",
    "}\n",
    "GRAPHS_BY_STREAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPHS_COMBINED: Mapping[str, Sequence[os.PathLike]] = {\n",
    "    type_graph: [\n",
    "        combine_graphs(\n",
    "            ts,\n",
    "            klass_builder,\n",
    "            [GRAPHS_BY_STREAM[type_graph][stream][i] for stream in STREAMS_INVOLVED]\n",
    "        )\n",
    "        for i, ts in enumerate(cfg.index_partition())\n",
    "    ]\n",
    "    for type_graph, klass_builder in [(\"networkx\", BuilderNetworkX)]\n",
    "}\n",
    "\n",
    "futs = client.compute(GRAPHS_COMBINED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ig.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(g.es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.add_vertices([\"asdf\", \"qwer\"])\n",
    "g.add_vertices([\"zxcv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.add_edge(\"asdf\", \"qwer\")\n",
    "list(g.es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.add_edge(\"asdf\", \"qwer\")\n",
    "list(g.es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.add_edge(\"zxcv\", \"asdf\")\n",
    "list(g.es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.incident(\"asdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g[\"qwer\", \"zxcv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[v[\"name\"] for v in g.es[2].vertex_tuple]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\"asdf\", \"qwer\") in g.es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g[\"asdf\", \"qwer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(g.vs.select(name=\"asdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(g.vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
