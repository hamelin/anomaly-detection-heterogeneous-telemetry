{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Los Alamos Cybersecurity dataset tools\n",
    "\n",
    "Includes tools to manipulate dataset storage, as well as load events from said storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset chunking\n",
    "\n",
    "This dataset is large! To facilitate its processing, it is best to cut its bigger files into *chunks*, which can be processed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on --max_line_length 120 --ignore W293,E302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notebooks\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import dask\n",
    "from glob import glob\n",
    "from growing import growing\n",
    "import gzip\n",
    "import io\n",
    "from jupytest import Suite, Report, summarize_results\n",
    "import os\n",
    "import os.path as op\n",
    "import sys\n",
    "from typing import Iterable, ContextManager, TypeVar, Any, Sequence\n",
    "from unittest.mock import patch, Mock, call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = Suite()\n",
    "if __name__ == \"__main__\":\n",
    "    suite |= Report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's embody the intended file hierarchy of the LANL dataset into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@growing\n",
    "class DataStoreLosAlamos:\n",
    "    \"\"\"\n",
    "    Main files making up the Los Alamos Cybersecurity dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path: os.PathLike) -> None:\n",
    "        self._dir_base = path\n",
    "        \n",
    "    @property\n",
    "    def dir_base(self) -> os.PathLike:\n",
    "        return self._dir_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cutting the raw files into compressed chunks is a long-running computation. Let's structure it so it's run into a compute cluster, when we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_CHUNK = (2 << 25) + (2 << 24)  # 96 MB maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@DataStoreLosAlamos.method(wrapped_in=dask.delayed(pure=True))\n",
    "def join_chunked(self, stream: str, *p: os.PathLike) -> os.PathLike:\n",
    "    path_stream_chunked = op.join(self.dir_base, \"chunked\", stream)\n",
    "    os.makedirs(path_stream_chunked, exist_ok=True)\n",
    "    \n",
    "    names_chunk = glob(op.join(path_stream_chunked, \"*.txt.gz\"))\n",
    "    if len(names_chunk) == 0 or any(os.stat(p).st_size == 0 for p in names_chunk):\n",
    "        # Raw files have not been chunked yet, or some chunks are corrupted. It's chunking time.\n",
    "        with gzip.open(op.join(self.dir_base, \"raw\", f\"{stream}.txt.gz\"), \"rb\") as file_raw:\n",
    "            for index in range(sys.maxsize):\n",
    "                with FileChunk(op.join(self.dir_base, \"chunked\", stream, f\"{index:04d}.txt.gz\")) as file_chunk:\n",
    "                    for line in file_raw:\n",
    "                        if not file_chunk.write(line):\n",
    "                            break\n",
    "                    else:\n",
    "                        break  # ...out of outer infinite loop.\n",
    "                        \n",
    "    return op.join(path_stream_chunked, *p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests** for method `join_chunked`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def mocking_global(name: str, value_mock: T) -> ContextManager[T]:\n",
    "    must_restore = False\n",
    "    G = globals()\n",
    "    if name in G:\n",
    "        value_orig = G[name]\n",
    "        must_restore = True\n",
    "    G[name] = value_mock\n",
    "    \n",
    "    try:\n",
    "        yield value_mock\n",
    "    finally:\n",
    "        if must_restore:\n",
    "            G[name] = value_orig\n",
    "        else:\n",
    "            del G[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_file_raw(lines: Iterable[str]) -> Mock:\n",
    "    mock = Mock()\n",
    "    mock.__enter__ = lambda self: self\n",
    "    mock.__exit__ = lambda self, t, v, tb: False\n",
    "    iter_lines = iter(lines)\n",
    "    mock.__iter__ = lambda self: iter_lines\n",
    "    return mock\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def mocking_gzip_open(lines: Iterable[str]) -> ContextManager[Mock]:\n",
    "    with patch(\"gzip.open\", return_value=mock_file_raw(lines)) as mock:\n",
    "        yield mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_file_chunk(**kwargs: Any) -> Mock:\n",
    "    mock = Mock()\n",
    "    mock.__enter__ = lambda self: self\n",
    "    mock.__exit__ = lambda self, t, v, tb: False\n",
    "    mock.write = Mock(**kwargs)\n",
    "    return mock\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def mocking_FileChunk(mocks: Sequence[Mock]) -> ContextManager[Mock]:\n",
    "    with mocking_global(\"FileChunk\", Mock(side_effect=mocks)) as mock:\n",
    "        yield mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mjoin-chunked/Stop\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test join-chunked/Stop\n",
    "mocks_chunk = [\n",
    "    mock_file_chunk(**kwargs)\n",
    "    for kwargs in [dict(side_effect=[True, False]), dict(side_effect=[True, False]), dict(return_value=True)]\n",
    "]\n",
    "with patch(\"os.makedirs\"), patch(\"glob.glob\", return_value=[]),\\\n",
    "        mocking_gzip_open([b\"asdf\\n\", b\"qwer\\n\", b\"zxcv\\n\", b\"qwerty\\n\", b\"uiop\\n\"]) as mock_raw,\\\n",
    "        mocking_FileChunk(mocks_chunk) as mock_class:\n",
    "    ds = DataStoreLosAlamos(\"/path/to/data\")\n",
    "    assert ds.join_chunked(\"dns\", \"asdf\", \"qwer\").compute(scheduler=\"single-threaded\") ==\\\n",
    "        \"/path/to/data/chunked/dns/asdf/qwer\"\n",
    "\n",
    "    mock_class.assert_has_calls(\n",
    "        [call(f\"/path/to/data/chunked/dns/{i:04d}.txt.gz\") for i in range(3)]\n",
    "    )\n",
    "    mocks_chunk[0].write.assert_has_calls([call(s) for s in [b\"asdf\\n\", b\"qwer\\n\"]])\n",
    "    mocks_chunk[1].write.assert_has_calls([call(s) for s in [b\"zxcv\\n\", b\"qwerty\\n\"]])\n",
    "    mocks_chunk[2].write.assert_has_calls([call(s) for s in [b\"uiop\\n\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mjoin-chunked/End of raw file corresponds to end of chunk\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test join-chunked/End of raw file corresponds to end of chunk\n",
    "mocks_chunk = [\n",
    "    mock_file_chunk(**kwargs)\n",
    "    for kwargs in [dict(side_effect=[True, False]), dict(side_effect=[True, False]), dict(return_value=True)]\n",
    "]\n",
    "with patch(\"os.makedirs\"), patch(\"glob.glob\", return_value=[]),\\\n",
    "        mocking_gzip_open([b\"asdf\\n\", b\"qwer\\n\", b\"zxcv\\n\", b\"qwerty\\n\"]) as mock_raw,\\\n",
    "        mocking_FileChunk(mocks_chunk) as mock_class:\n",
    "    ds = DataStoreLosAlamos(\"/path/to/data\")\n",
    "    assert ds.join_chunked(\"dns\", \"asdf\", \"qwer\").compute(scheduler=\"single-threaded\") ==\\\n",
    "        \"/path/to/data/chunked/dns/asdf/qwer\"\n",
    "\n",
    "    mock_class.assert_has_calls(\n",
    "        [call(f\"/path/to/data/chunked/dns/{i:04d}.txt.gz\") for i in range(3)]\n",
    "    )\n",
    "    mocks_chunk[0].write.assert_has_calls([call(s) for s in [b\"asdf\\n\", b\"qwer\\n\"]])\n",
    "    mocks_chunk[1].write.assert_has_calls([call(s) for s in [b\"zxcv\\n\", b\"qwerty\\n\"]])\n",
    "    mocks_chunk[2].write.assert_not_called()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mjoin-chunked/Raw file is empty\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test join-chunked/Raw file is empty\n",
    "mocks_chunk = [\n",
    "    mock_file_chunk(**kwargs)\n",
    "    for kwargs in [dict(side_effect=[True, False]), dict(side_effect=[True, False]), dict(return_value=True)]\n",
    "]\n",
    "with patch(\"os.makedirs\"), patch(\"glob.glob\", return_value=[]),\\\n",
    "        mocking_gzip_open([]) as mock_raw,\\\n",
    "        mocking_FileChunk(mocks_chunk) as mock_class:\n",
    "    ds = DataStoreLosAlamos(\"/path/to/data\")\n",
    "    assert ds.join_chunked(\"dns\", \"asdf\", \"qwer\").compute(scheduler=\"single-threaded\") ==\\\n",
    "        \"/path/to/data/chunked/dns/asdf/qwer\"\n",
    "\n",
    "    mock_class.assert_called_once_with(\"/path/to/data/chunked/dns/0000.txt.gz\")\n",
    "    for mock in mocks_chunk:\n",
    "        mock.write.assert_not_called()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class `FileChunk` then embodies the creation of a chunk and the transfer of its content into the target file. Note that the algorithm of `join_chunked()` made it so the context of the `FileChunk` instance is entered before we have any content for the chunk; the creation of the file should thus be delayed to a call to method `write()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileChunk:\n",
    "    \"\"\"\n",
    "    Delays the creation of a chunk file until the user commits to writing something in it.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path: os.PathLike) -> None:\n",
    "        self._path = path\n",
    "        self._file: Optional[io.RawByteIO] = None\n",
    "        \n",
    "    def __enter__(self) -> \"FileChunk\":\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, type_exc, value_exc, tb_exc) -> bool:\n",
    "        if self._file is not None:\n",
    "            self._file.close()\n",
    "        return False\n",
    "    \n",
    "    def write(self, buf: bytes) -> None:\n",
    "        if self._file is None:\n",
    "            self._file = gzip.open(self._path, \"wb\")\n",
    "        index = 0\n",
    "        while index < len(buf):\n",
    "            index += self._file.write(buf[index:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mFileChunk.write/All written in one single underlying write\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test FileChunk.write/All written in one single underlying write\n",
    "bytes_written = io.BytesIO()\n",
    "with patch(\"gzip.open\", return_value=bytes_written):\n",
    "    with FileChunk(\"asdf\") as file_chunk:\n",
    "        file_chunk.write(b\"qwerty\\n\")\n",
    "        assert bytes_written.getvalue() == b\"qwerty\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mFileChunk.write/Multiple underlying writes needed\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test FileChunk.write/Multiple underlying writes needed\n",
    "with patch(\"gzip.GzipFile\") as mock:\n",
    "    mock.return_value.write = Mock(side_effect=[3, 4])\n",
    "    with FileChunk(\"asdf\") as file_chunk:\n",
    "        file_chunk.write(b\"qwerty\\n\")\n",
    "    mock.return_value.write.assert_has_calls([call(b'qwerty\\n',), call(b'rty\\n',)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mFileChunk/No file created without write\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test FileChunk/No file created without write\n",
    "with patch(\"gzip.open\") as mock:\n",
    "    with FileChunk(\"asdf\") as file_chunk:\n",
    "        pass\n",
    "    mock.assert_not_called()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 passed, \u001b[37m0 failed\u001b[0m, \u001b[37m0 raised an error\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    _ = summarize_results(suite)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
