{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Los Alamos Cybersecurity dataset tools\n",
    "\n",
    "Includes tools to manipulate dataset storage, as well as load events from said storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on --max_line_length 120 --ignore W293,E302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notebooks_as_modules\n",
    "\n",
    "from collections import OrderedDict\n",
    "from contextlib import contextmanager\n",
    "import dask\n",
    "import dask.dataframe as ddf\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from glob import glob\n",
    "from growing import growing\n",
    "import gzip\n",
    "import igraph as ig\n",
    "import io\n",
    "from jupytest import Suite, Report, Magic, summarize_results, assert_, eq, approx, Explanation, ExplanationOnFailure, \\\n",
    "    join_args, fail\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as op\n",
    "import pandas as pd\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from typing import *  # noqa\n",
    "from unittest.mock import patch, Mock, call, MagicMock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = Suite()\n",
    "if __name__ == \"__main__\":\n",
    "    suite |= Report()\n",
    "    suite |= Magic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dataset chunking\n",
    "\n",
    "This dataset is large! To facilitate its processing, it is best to cut its bigger files into *chunks*, which can be processed in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's embody the intended file hierarchy of the LANL dataset into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@growing\n",
    "class DataStoreLosAlamos:\n",
    "    \"\"\"\n",
    "    Main files making up the Los Alamos Cybersecurity dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path: os.PathLike) -> None:\n",
    "        self._dir_base = path\n",
    "        \n",
    "    @property\n",
    "    def dir_base(self) -> os.PathLike:\n",
    "        return self._dir_base\n",
    "    \n",
    "    def __dask_tokenize__(self) -> str:\n",
    "        return self.dir_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Cutting the raw files into compressed chunks is a long-running computation. Let's structure it so it's run into a compute cluster, when we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SIZE_CHUNK = (2 << 25) + (2 << 24)  # 96 MB maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@DataStoreLosAlamos.method(wrapped_in=dask.delayed(pure=True))\n",
    "def join_chunked(self, stream: str, *p: os.PathLike, size_chunk: int = SIZE_CHUNK) -> os.PathLike:\n",
    "    path_stream_chunked = op.join(self.dir_base, \"chunked\", stream)\n",
    "    os.makedirs(path_stream_chunked, exist_ok=True)\n",
    "    \n",
    "    names_chunk = glob(op.join(path_stream_chunked, \"*.txt.gz\"))\n",
    "    if len(names_chunk) == 0 or any(os.stat(p).st_size == 0 for p in names_chunk):\n",
    "        # Raw files have not been chunked yet, or some chunks are corrupted. It's chunking time.\n",
    "        with gzip.open(op.join(self.dir_base, f\"{stream}.txt.gz\"), \"rb\") as file_raw:\n",
    "            for index in range(sys.maxsize):\n",
    "                with FileChunk(\n",
    "                    op.join(self.dir_base, \"chunked\", stream, f\"{index:04d}.txt.gz\"),\n",
    "                    size_chunk\n",
    "                ) as file_chunk:\n",
    "                    for line in file_raw:\n",
    "                        if not file_chunk.write(line):\n",
    "                            break\n",
    "                    else:\n",
    "                        break  # ...out of outer infinite loop.\n",
    "                        \n",
    "    return op.join(path_stream_chunked, *p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Tests** for method `join_chunked`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def mocking_global(name: str, value_mock: T) -> ContextManager[T]:\n",
    "    must_restore = False\n",
    "    G = globals()\n",
    "    if name in G:\n",
    "        value_orig = G[name]\n",
    "        must_restore = True\n",
    "    G[name] = value_mock\n",
    "    \n",
    "    try:\n",
    "        yield value_mock\n",
    "    finally:\n",
    "        if must_restore:\n",
    "            G[name] = value_orig\n",
    "        else:\n",
    "            del G[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def mock_file_raw(lines: Iterable[str]) -> Mock:\n",
    "    mock = Mock()\n",
    "    mock.__enter__ = lambda self: self\n",
    "    mock.__exit__ = lambda self, t, v, tb: False\n",
    "    iter_lines = iter(lines)\n",
    "    mock.__iter__ = lambda self: iter_lines\n",
    "    return mock\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def mocking_gzip_open(lines: Iterable[str]) -> ContextManager[Mock]:\n",
    "    with patch(\"gzip.open\", return_value=mock_file_raw(lines)) as mock:\n",
    "        yield mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def mock_file_chunk(**kwargs: Any) -> Mock:\n",
    "    mock = Mock()\n",
    "    mock.__enter__ = lambda self: self\n",
    "    mock.__exit__ = lambda self, t, v, tb: False\n",
    "    mock.write = Mock(**kwargs)\n",
    "    return mock\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def mocking_FileChunk(mocks: Sequence[Mock]) -> ContextManager[Mock]:\n",
    "    with mocking_global(\"FileChunk\", Mock(side_effect=mocks)) as mock:\n",
    "        yield mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mjoin-chunked/Stop\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test join-chunked/Stop\n",
    "mocks_chunk = [\n",
    "    mock_file_chunk(**kwargs)\n",
    "    for kwargs in [dict(side_effect=[True, False]), dict(side_effect=[True, False]), dict(return_value=True)]\n",
    "]\n",
    "with patch(\"os.makedirs\"), patch(\"glob.glob\", return_value=[]),\\\n",
    "        mocking_gzip_open([b\"asdf\\n\", b\"qwer\\n\", b\"zxcv\\n\", b\"qwerty\\n\", b\"uiop\\n\"]) as mock_raw,\\\n",
    "        mocking_FileChunk(mocks_chunk) as mock_class:\n",
    "    ds = DataStoreLosAlamos(\"/path/to/data\")\n",
    "    assert_(\n",
    "        eq,\n",
    "        actual=ds.join_chunked(\"dns\", \"asdf\", \"qwer\", size_chunk=10).compute(scheduler=\"single-threaded\"),\n",
    "        expected=\"/path/to/data/chunked/dns/asdf/qwer\"\n",
    "    )\n",
    "\n",
    "    mock_class.assert_has_calls(\n",
    "        [call(f\"/path/to/data/chunked/dns/{i:04d}.txt.gz\", 10) for i in range(3)]\n",
    "    )\n",
    "    mocks_chunk[0].write.assert_has_calls([call(s) for s in [b\"asdf\\n\", b\"qwer\\n\"]])\n",
    "    mocks_chunk[1].write.assert_has_calls([call(s) for s in [b\"zxcv\\n\", b\"qwerty\\n\"]])\n",
    "    mocks_chunk[2].write.assert_has_calls([call(s) for s in [b\"uiop\\n\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mjoin-chunked/End of raw file corresponds to end of chunk\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test join-chunked/End of raw file corresponds to end of chunk\n",
    "mocks_chunk = [\n",
    "    mock_file_chunk(**kwargs)\n",
    "    for kwargs in [dict(side_effect=[True, False]), dict(side_effect=[True, False]), dict(return_value=True)]\n",
    "]\n",
    "with patch(\"os.makedirs\"), patch(\"glob.glob\", return_value=[]),\\\n",
    "        mocking_gzip_open([b\"asdf\\n\", b\"qwer\\n\", b\"zxcv\\n\", b\"qwerty\\n\"]) as mock_raw,\\\n",
    "        mocking_FileChunk(mocks_chunk) as mock_class:\n",
    "    ds = DataStoreLosAlamos(\"/path/to/data\")\n",
    "    assert_(\n",
    "        eq,\n",
    "        actual=ds.join_chunked(\"dns\", \"asdf\", \"qwer\", size_chunk=10).compute(scheduler=\"single-threaded\"),\n",
    "        expected=\"/path/to/data/chunked/dns/asdf/qwer\"\n",
    "    )\n",
    "\n",
    "    mock_class.assert_has_calls(\n",
    "        [call(f\"/path/to/data/chunked/dns/{i:04d}.txt.gz\", 10) for i in range(3)]\n",
    "    )\n",
    "    mocks_chunk[0].write.assert_has_calls([call(s) for s in [b\"asdf\\n\", b\"qwer\\n\"]])\n",
    "    mocks_chunk[1].write.assert_has_calls([call(s) for s in [b\"zxcv\\n\", b\"qwerty\\n\"]])\n",
    "    mocks_chunk[2].write.assert_not_called()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mjoin-chunked/Raw file is empty\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test join-chunked/Raw file is empty\n",
    "mocks_chunk = [\n",
    "    mock_file_chunk(**kwargs)\n",
    "    for kwargs in [dict(side_effect=[True, False]), dict(side_effect=[True, False]), dict(return_value=True)]\n",
    "]\n",
    "with patch(\"os.makedirs\"), patch(\"glob.glob\", return_value=[]),\\\n",
    "        mocking_gzip_open([]) as mock_raw,\\\n",
    "        mocking_FileChunk(mocks_chunk) as mock_class:\n",
    "    ds = DataStoreLosAlamos(\"/path/to/data\")\n",
    "    assert_(\n",
    "        eq,\n",
    "        actual=ds.join_chunked(\"dns\", \"asdf\", \"qwer\").compute(scheduler=\"single-threaded\"),\n",
    "        expected=\"/path/to/data/chunked/dns/asdf/qwer\"\n",
    "    )\n",
    "\n",
    "    mock_class.assert_called_once_with(\"/path/to/data/chunked/dns/0000.txt.gz\", SIZE_CHUNK)\n",
    "    for mock in mocks_chunk:\n",
    "        mock.write.assert_not_called()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Class `FileChunk` then embodies the creation of a chunk and the transfer of its content into the target file. Note that the algorithm of `join_chunked()` made it so the context of the `FileChunk` instance is entered before we have any content for the chunk; the creation of the file should thus be delayed to a call to method `write()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class FileChunk:\n",
    "    \"\"\"\n",
    "    Delays the creation of a chunk file until the user commits to writing something in it.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path: os.PathLike, limit: int) -> None:\n",
    "        self._path = path\n",
    "        self._file: Optional[io.RawByteIO] = None\n",
    "        self._limit = limit\n",
    "        self._size = 0\n",
    "        \n",
    "    def __enter__(self) -> \"FileChunk\":\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, type_exc, value_exc, tb_exc) -> bool:\n",
    "        if self._file is not None:\n",
    "            self._file.close()\n",
    "        return False\n",
    "    \n",
    "    def write(self, buf: bytes) -> bool:\n",
    "        if self._file is None:\n",
    "            self._file = gzip.open(self._path, \"wb\")\n",
    "\n",
    "        index = 0\n",
    "        while index < len(buf):\n",
    "            index += self._file.write(buf[index:])\n",
    "            \n",
    "        self._size += len(buf)\n",
    "        return self._size < self._limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Tests**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mFileChunk/No file created without write\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test FileChunk/No file created without write\n",
    "with patch(\"gzip.open\") as mock:\n",
    "    with FileChunk(\"asdf\", 100) as file_chunk:\n",
    "        pass\n",
    "    mock.assert_not_called()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mFileChunk.write/All written in one single underlying write\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test FileChunk.write/All written in one single underlying write\n",
    "bytes_written = io.BytesIO()\n",
    "with patch(\"gzip.open\", return_value=bytes_written):\n",
    "    with FileChunk(\"asdf\", 100) as file_chunk:\n",
    "        assert file_chunk.write(b\"qwerty\\n\")\n",
    "        assert_(eq, actual=bytes_written.getvalue(), expected=b\"qwerty\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mFileChunk.write/Multiple underlying writes needed\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test FileChunk.write/Multiple underlying writes needed\n",
    "with patch(\"gzip.GzipFile\") as mock:\n",
    "    mock.return_value.write = Mock(side_effect=[3, 4])\n",
    "    with FileChunk(\"asdf\", 100) as file_chunk:\n",
    "        assert file_chunk.write(b\"qwerty\\n\")\n",
    "    mock.return_value.write.assert_has_calls([call(b'qwerty\\n',), call(b'rty\\n',)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mFileChunk.write/Return False once chunk once exactly full\u001b[0m passed.\n",
      "Test \u001b[1mFileChunk.write/Return False once chunk once beyond full\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "def test_chunk_filling(last: bytes) -> None:\n",
    "    bytes_written = io.BytesIO()\n",
    "    with patch(\"gzip.open\", return_value=bytes_written):\n",
    "        with FileChunk(\"asdf\", 25) as file_chunk:\n",
    "            assert file_chunk.write(b\"asdf\\nqwer\\n\")\n",
    "            assert file_chunk.write(b\"zxcv\\n\")\n",
    "            assert file_chunk.write(b\"uiop\\n\")\n",
    "            assert not file_chunk.write(last)\n",
    "            assert_(eq, actual=bytes_written.getvalue(), expected=b\"asdf\\nqwer\\nzxcv\\nuiop\\n\" + last)\n",
    "    \n",
    "\n",
    "for adverb, last in [(\"exactly\", b\"1234\\n\"), (\"beyond\", b\"1234567890\\n\")]:\n",
    "    suite.test(\n",
    "        test_chunk_filling,\n",
    "        args=(last,),\n",
    "        name=f\"FileChunk.write/Return False once chunk once {adverb} full\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Experiments repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@DataStoreLosAlamos.method\n",
    "def join_experiments(self, *p: os.PathLike) -> os.PathLike:\n",
    "    return op.join(self.dir_base, \"experiments\", *p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Loading a data stream into a Dask dataframe\n",
    "\n",
    "Dask dataframes easily leverage the chunking of the streams that we have wrought. However, given how the chunks are already sorted, one gets the best benefits from these by supplying knowledge of the *divisions* of the index key (here, time) across the partitions. This is why we implement a custom dataframe loading that quickly extracts the division knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Event timestamps\n",
    "\n",
    "The Los Alamos Cybersecurity dataset has been captured over a period of two months, but the exact dates are unknown; the timestamps provided in the dataset start at 0. While mapping these directly to timestamps would yield funny 1970's dates to events, we rather choose a more modern setting. Given the late-2015 moment the dataset was released, we shall assume the acquisition ran from January 1st, 2015, to February 27th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2015-02-27 23:59:59.999999999')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@growing\n",
    "class Time:\n",
    "    START = pd.Timestamp(\"2015-01-01T00:00:00\")\n",
    "    END = pd.Timestamp(\"2015-02-28T00:00:00\") - pd.Timedelta(nanoseconds=1)\n",
    "\n",
    "\n",
    "Time.END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def seconds2ts(n: str) -> pd.Timestamp:\n",
    "    return Time.START + pd.Timedelta(seconds=int(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mTimestamp mapping\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Timestamp mapping\n",
    "assert_(eq, actual=seconds2ts(\"3600\"), expected=pd.Timestamp(\"2015-01-01T01:00:00\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Data schemas\n",
    "\n",
    "The `SCHEMAS` dictionary describe the columns for each of the four main data streams, in addition to the label array stored in `redteam.txt.gz`. Each stream is sorted (and thus indexable) by its `time` column, which is omitted from the schema descriptions to facilitate the usage of the schema objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SCHEMA = Sequence[Tuple[str, str]]\n",
    "\n",
    "\n",
    "SCHEMAS: Mapping[str, SCHEMA] = {\n",
    "    \"dns\": [\n",
    "        (\"host_focus\", \"object\"),\n",
    "        (\"host_resolved\", \"object\")\n",
    "    ],\n",
    "    \"flows\": [\n",
    "        (\"duration\", \"int64\"),\n",
    "        (\"host_focus\", \"object\"),\n",
    "        (\"port_focus\", \"object\"),\n",
    "        (\"host_server\", \"object\"),\n",
    "        (\"port_server\", \"object\"),\n",
    "        (\"protocol\", \"category\"),\n",
    "        (\"num_packets\", \"int32\"),\n",
    "        (\"num_bytes\", \"int64\")\n",
    "    ],\n",
    "    \"proc\": [\n",
    "        (\"userdomain_focus\", \"object\"),\n",
    "        (\"host_focus\", \"object\"),\n",
    "        (\"process\", \"object\"),\n",
    "        (\"action\", \"category\")\n",
    "    ],\n",
    "    \"auth\": [\n",
    "        (\"userdomain_init\", \"object\"),\n",
    "        (\"userdomain_focus\", \"object\"),\n",
    "        (\"host_init\", \"object\"),\n",
    "        (\"host_focus\", \"object\"),\n",
    "        (\"auth\", \"category\"),\n",
    "        (\"logon\", \"category\"),\n",
    "        (\"direction\", \"category\"),\n",
    "        (\"result\", \"category\")\n",
    "    ],\n",
    "    \"redteam\": [\n",
    "        (\"userdomain_focus\", \"object\"),\n",
    "        (\"host_init\", \"object\"),\n",
    "        (\"host_focus\", \"object\")\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Figuring out which stream a file is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "RXS_NAMES_STREAM = r\"(\" + \"|\".join(SCHEMAS.keys()) + \")\"\n",
    "RX_PATH2STREAM = re.compile(r\"/\" + RXS_NAMES_STREAM + r\"/|\" + RXS_NAMES_STREAM + r\"\\.txt\\.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def path2stream(path: os.PathLike) -> str:\n",
    "    m = re.search(RX_PATH2STREAM, path)\n",
    "    if m is None:\n",
    "        return \"\"\n",
    "    return m.group(1) or m.group(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mStream name for a raw file\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Stream name for a raw file\n",
    "assert_(eq, actual=path2stream(\"/data/lanl/redteam.txt.gz\"), expected=\"redteam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mStream name for a chunk file\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Stream name for a chunk file\n",
    "assert_(eq, actual=path2stream(\"/data/lanl/chunked/auth/0034.txt.gz\"), expected=\"auth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mStream name for a stream-specific processing result (not chunking)\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Stream name for a stream-specific processing result (not chunking)\n",
    "assert_(eq, actual=path2stream(\"/data/lanl/experiments/asdf/qwer/proc/zxcv\"), expected=\"proc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mPath with no stream name\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Path with no stream name\n",
    "assert not path2stream(\"/data/lanl/wtf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Getting the first timestamp of a LANL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_timestamp_lower(path: os.PathLike) -> pd.Timestamp:\n",
    "    with gzip.open(path, \"rb\") as file:\n",
    "        line1 = next(file)  # Assumption: no file empty.\n",
    "        num_seconds, *_ = line1.split(b\",\")\n",
    "        return seconds2ts(num_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mFirst timestamp for line of a DNS stream file\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test First timestamp for line of a DNS stream file\n",
    "with patch(\"gzip.open\", return_value=io.BytesIO(b\"90842,C326,C89\\n\")):\n",
    "    assert_(eq, actual=get_timestamp_lower(\"asdf\"), expected=pd.Timestamp(\"2015-01-02T01:14:02\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mFirst timestamp for line of a flows stream file\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test First timestamp for line of a flows stream file\n",
    "with patch(\"gzip.open\", return_value=io.BytesIO(b\"2957021,2,C347,50234,C812,443,https,12,15723\\nqwerty\\n\")):\n",
    "    assert_(eq, actual=get_timestamp_lower(\"asdf\"), expected=pd.Timestamp(\"2015-02-04T05:23:41\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Loading a LANL CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def read_lanl_csv(path: os.PathLike, **kwargs: Any) -> pd.DataFrame:\n",
    "    stream = path2stream(path)\n",
    "    if not stream:\n",
    "        raise ValueError(f\"Path {path} does not involve a LANL data stream.\")\n",
    "    schema = SCHEMAS[stream]\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        header=None,\n",
    "        names=[\"time\"] + [attr for attr, _ in schema],\n",
    "        dtype=dict(schema),\n",
    "        parse_dates=[\"time\"],\n",
    "        date_parser=seconds2ts,\n",
    "        index_col=\"time\",\n",
    "        compression=\"gzip\",\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def dummy_proc_content() -> ContextManager[os.PathLike]:\n",
    "    content = b\"\"\"\\\n",
    "3,C3@DOM1,C4,P2,Start\n",
    "18,C89@DOM1,C23,P78,Start\n",
    "29,C14@DOM1,C90,P123,Start\n",
    "53,C90@DOM1,C34,P23,End\n",
    "\"\"\"\n",
    "    with patch(\"gzip.builtins.open\", return_value=io.BytesIO(gzip.compress(content))):\n",
    "        yield \"/path/with/proc/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mReading LANL content\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Reading LANL content\n",
    "with dummy_proc_content() as path:\n",
    "    df = read_lanl_csv(path)\n",
    "assert_(eq, actual=len(df), expected=4)\n",
    "assert_(eq, actual=len(df.columns), expected=4)\n",
    "assert_(eq, actual=df.index.dtype, expected=np.dtype(\"datetime64[ns]\"))\n",
    "assert_(\n",
    "    eq,\n",
    "    actual={c: str(dt) for c, dt in df.dtypes.items()},\n",
    "    expected={\n",
    "        \"userdomain_focus\": \"object\",\n",
    "        \"host_focus\": \"object\",\n",
    "        \"process\": \"object\",\n",
    "        \"action\": \"category\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Putting it all together into a Dask dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@DataStoreLosAlamos.method\n",
    "def get_stream(self, name: str) -> ddf.DataFrame:\n",
    "    paths_chunk = sorted(glob(self.join_chunked(name, \"*.txt.gz\").compute()))\n",
    "    divisions = [get_timestamp_lower(path) for path in paths_chunk] + [Time.END]\n",
    "\n",
    "    schema = dict(SCHEMAS[name])\n",
    "    return ddf.from_delayed(\n",
    "        [dask.delayed(read_lanl_csv)(path) for path in paths_chunk],\n",
    "        meta=pd.DataFrame(columns=schema.keys(), index=pd.DatetimeIndex([], name=\"time\")).astype(schema),\n",
    "        divisions=divisions,\n",
    "        prefix=\"load_chunk\",\n",
    "        verify_meta=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mStream dataframe coherence\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Stream dataframe coherence\n",
    "ds = DataStoreLosAlamos(\"/lanl\")\n",
    "indices = [3, 2, 4, 0, 1]\n",
    "with patch(\"__main__.glob\", side_effect=[[f\"/lanl/flows/{n:04d}.txt.gz\" for n in indices]]),\\\n",
    "        patch.object(ds, \"join_chunked\", new=MagicMock()),\\\n",
    "        patch(\n",
    "            \"__main__.get_timestamp_lower\",\n",
    "            side_effect=[pd.Timestamp(s) for s in [\n",
    "                \"2015-01-01T00:00:04\",\n",
    "                \"2015-01-11T12:45:32\",\n",
    "                \"2015-01-27T18:19:19\",\n",
    "                \"2015-02-12T14:10:23\",\n",
    "                \"2015-02-23T18:02:38\"\n",
    "            ]]\n",
    "        ):\n",
    "    df = ds.get_stream(\"flows\")\n",
    "    assert_(eq, actual=df.npartitions, expected=5)\n",
    "    assert_(\n",
    "        eq,\n",
    "        actual=list(df.index.divisions),\n",
    "        expected=[pd.Timestamp(s) for s in [\n",
    "            \"2015-01-01T00:00:04\",\n",
    "            \"2015-01-11T12:45:32\",\n",
    "            \"2015-01-27T18:19:19\",\n",
    "            \"2015-02-12T14:10:23\",\n",
    "            \"2015-02-23T18:02:38\",\n",
    "            \"2015-02-27T23:59:59.999999999\"\n",
    "        ]]\n",
    "    )\n",
    "    assert_(eq, actual=dict(df.dtypes), expected=dict(SCHEMAS[\"flows\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mResolving a complete dataframe\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "@suite.test(name=\"Resolving a complete dataframe\")\n",
    "def resolving_whole_dataframe():\n",
    "    all_events = [\n",
    "        b\"\"\"\\\n",
    "    63,U34@DOM1,U23@DOM1,C98,C98,Kerberos,Network,LogOn,Success\n",
    "    91,U67@DOM1,SYSTEM@C89,C89,C89,Negotiate,Service,LogOn,Success\n",
    "    \"\"\",\n",
    "        b\"\"\"\\\n",
    "    304,U45@DOM1,U45@DOM1,C234,C329,Kerberos,Network,LogOff,Success\n",
    "    897,U93@DOM1,U93@DOM1,C123,C123,Kerberos,Network,LogOn,Success\n",
    "    \"\"\",\n",
    "        b\"\"\"\\\n",
    "    956,U93@DOM1,U93@DOM1,C123,C123,Kerberos,Network,LogOff,Success\n",
    "    3456,U67@DOM1,U45@DOM1,C89,C329,Kerberos,Network,LogOn,Failure\n",
    "    4127,U980@DOM1,U980@DOM1,C23,C32,Kerberos,Service,LogOn,Success\n",
    "    \"\"\"\n",
    "    ]\n",
    "\n",
    "    map_content = {\n",
    "        op.join(\"/lanl\", \"chunked\", \"auth\", f\"{n:02d}.txt.gz\"): content\n",
    "        for n, content in enumerate(all_events)\n",
    "    }\n",
    "\n",
    "    def grab_content(path: os.PathLike, *args, **kwargs) -> io.RawIOBase:\n",
    "        return io.BytesIO(gzip.compress(map_content[path]))\n",
    "\n",
    "    ds = DataStoreLosAlamos(\"asdf\")\n",
    "    with patch.object(ds, \"join_chunked\", new=MagicMock()),\\\n",
    "            patch(\"__main__.glob\", side_effect=[list(map_content.keys())]),\\\n",
    "            patch(\"gzip.builtins.open\", side_effect=grab_content):\n",
    "        df = ds.get_stream(\"auth\")\n",
    "        assert_(eq, actual=df.npartitions, expected=3)\n",
    "        assert_(\n",
    "            eq,\n",
    "            actual=list(df.divisions),\n",
    "            expected=[pd.Timestamp(s) for s in [\n",
    "                \"2015-01-01T00:01:03\",\n",
    "                \"2015-01-01T00:05:04\",\n",
    "                \"2015-01-01T00:15:56\"\n",
    "            ]] + [Time.END]\n",
    "        )\n",
    "        df_realized = df.compute()\n",
    "        assert_(eq, actual=dict(df_realized.dtypes), expected=dict(SCHEMAS[\"auth\"]))\n",
    "\n",
    "    for line, ts_and_row in zip(b\"\".join(all_events).split(b\"\\n\"), df_realized.iterrows()):\n",
    "        num_seconds_expected, *cols_expected = line.split(b\",\")\n",
    "        ts_expected = seconds2ts(num_seconds_expected)\n",
    "        ts_obtained, row_obtained = ts_and_row\n",
    "        assert_(eq, expected=ts_expected, obtained=ts_obtained)\n",
    "        assert_(eq, expected=[str(c, encoding=\"utf-8\") for c in cols_expected], obtained=list(row_obtained))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Provide all streams in one query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@DataStoreLosAlamos.method\n",
    "def streams(self) -> Mapping[str, ddf.DataFrame]:\n",
    "    # First ensure all these streams have been chunked; leverage parallel cluster computation.\n",
    "    streams = [\"auth\", \"dns\", \"flows\", \"proc\"]\n",
    "    persisted = [self.join_chunked(name).persist() for name in streams]\n",
    "    dask.compute(persisted)\n",
    "    del persisted\n",
    "    \n",
    "    return {name: self.get_stream(name) for name in streams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mgathering_all_streams\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "@suite.test\n",
    "def gathering_all_streams():\n",
    "    stream_delay: Mapping[str, float] = {\n",
    "        \"auth\": 4.0,\n",
    "        \"dns\": 0.5,\n",
    "        \"flows\": 1.0,\n",
    "        \"proc\": 2.0\n",
    "    }\n",
    "    longest = max(stream_delay.values())\n",
    "\n",
    "    @dask.delayed\n",
    "    def mock_join_chunked(name: str) -> os.PathLike:\n",
    "        time.sleep(stream_delay[name])\n",
    "        return name  # Unused\n",
    "    \n",
    "    def mock_get_stream(name: str) -> Tuple[str]:\n",
    "        return (name,)\n",
    "    \n",
    "    cluster = LocalCluster(n_workers=4, threads_per_worker=1, dashboard_address=None)\n",
    "    client = Client(cluster)\n",
    "    try:\n",
    "        ds = DataStoreLosAlamos(\"dummy\")\n",
    "        with patch.object(ds, \"join_chunked\", new=Mock(side_effect=mock_join_chunked)),\\\n",
    "                patch.object(ds, \"get_stream\", side_effect=mock_get_stream):\n",
    "            tic = time.time()\n",
    "            assert_(eq, actual=ds.streams(), expected={name: (name,) for name in [\"auth\", \"dns\", \"flows\", \"proc\"]})\n",
    "            toc = time.time()\n",
    "            assert_(approx(longest, 0.1), toc - tic)\n",
    "    finally:\n",
    "        client.close()\n",
    "        cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Combination of all four streams in a single stream\n",
    "\n",
    "This is useful for software that needs to consider the heterogeneous telemetry as a single homogeneous stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def combine_streams(\n",
    "    ds: DataStoreLosAlamos,\n",
    "    streams: Sequence[str] = [],\n",
    "    start: Optional[pd.Timestamp] = None,\n",
    "    end: Optional[pd.Timestamp] = None,\n",
    ") -> ddf.DataFrame:\n",
    "    if not streams:\n",
    "        streams = [\"auth\", \"dns\", \"flows\", \"proc\"]\n",
    "\n",
    "    to_concat = []\n",
    "    for name in streams:\n",
    "        df = ds.get_stream(name)[(start or Time.START):(end or Time.END)]\n",
    "        df[\"stream\"] = name\n",
    "        to_concat.append(df)\n",
    "\n",
    "    return ddf.concat(to_concat, interleave_partitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def dataframes_equal(**dfs: pd.DataFrame) -> ExplanationOnFailure:\n",
    "    left, right = dfs.values()\n",
    "    if not left.equals(right):\n",
    "        return Explanation(\"The two dataframes are not equal\", join_args([], dfs))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class MockLANL(DataStoreLosAlamos):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"dummy\")\n",
    "        \n",
    "    def get_stream(self, name: str) -> ddf.DataFrame:\n",
    "        return ddf.from_pandas(\n",
    "            pd.DataFrame(\n",
    "                data={\n",
    "                    \"auth\": {\n",
    "                        \"index\": [pd.Timestamp(\"2015-01-05T13:00:05\")],\n",
    "                        \"userdomain_init\": [\"U3@DOM1\"],\n",
    "                        \"userdomain_focus\": [\"U3@DOM1\"],\n",
    "                        \"host_init\": [\"C328\"],\n",
    "                        \"host_focus\": [\"C345\"],\n",
    "                        \"auth\": [\"NTLM\"],\n",
    "                        \"logon\": [\"?\"],\n",
    "                        \"direction\": [\"LogOn\"],\n",
    "                        \"result\": [\"Success\"]\n",
    "                    },\n",
    "                    \"proc\": {\n",
    "                        \"index\": [pd.Timestamp(\"2015-01-05T13:00:06\"), pd.Timestamp(\"2015-01-05T13:00:32\")],\n",
    "                        \"userdomain_focus\": [\"U4@DOM1\", \"U3@DOM1\"],\n",
    "                        \"host_focus\": [\"C45\", \"C45\"],\n",
    "                        \"process\": [\"P3254\", \"P129\"],\n",
    "                        \"action\": [\"Start\", \"End\"]\n",
    "                    },\n",
    "                    \"flows\": {\n",
    "                        \"index\": [pd.Timestamp(\"2015-01-05T13:00:03\")],\n",
    "                        \"duration\": [3],\n",
    "                        \"host_focus\": [\"C89\"],\n",
    "                        \"port_focus\": [\"N435\"],\n",
    "                        \"host_server\": [\"C2390\"],\n",
    "                        \"port_server\": [\"443\"],\n",
    "                        \"protocol\": [\"3\"],\n",
    "                        \"num_packets\": [10],\n",
    "                        \"num_bytes\": [1454]\n",
    "                    },\n",
    "                    \"dns\": {\n",
    "                        \"index\": [pd.Timestamp(\"2015-01-05T13:00:03\"), pd.Timestamp(\"2015-01-05T13:00:24\")],\n",
    "                        \"host_focus\": [\"C89\", \"C234\"],\n",
    "                        \"host_resolved\": [\"C2390\", \"C123\"]\n",
    "                    }\n",
    "                }[name]\n",
    "            ).set_index(\"index\"),\n",
    "            npartitions=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test_df(data: Mapping[str, Sequence]) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(data=data)\n",
    "    map_types = {\n",
    "        \"category\": \"object\",\n",
    "        \"int64\": \"float64\",\n",
    "        \"int32\": \"float64\"\n",
    "    }\n",
    "    dict_dtype = {k: map_types.get(v, v) for k, v in set(sum(list(SCHEMAS.values()), [])) if k in df.columns}\n",
    "    return df.astype(dict_dtype).set_index(\"time\").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mCombination of selected streams\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Combination of selected streams\n",
    "assert_(\n",
    "    dataframes_equal,\n",
    "    expected=test_df({\n",
    "        \"time\": [pd.Timestamp(s) for s in [\"2015-01-05T13:00:03\", \"2015-01-05T13:00:03\", \"2015-01-05T13:00:24\"]],\n",
    "        \"host_focus\": [\"C89\", \"C89\", \"C234\"],\n",
    "        \"host_resolved\": [\"C2390\", np.nan, \"C123\"],\n",
    "        \"stream\": [\"dns\", \"flows\", \"dns\"],\n",
    "        \"duration\": [np.nan, 3, np.nan],\n",
    "        \"port_focus\": [np.nan, \"N435\", np.nan],\n",
    "        \"host_server\": [np.nan, \"C2390\", np.nan],\n",
    "        \"port_server\": [np.nan, \"443\", np.nan],\n",
    "        \"protocol\": [np.nan, \"3\", np.nan],\n",
    "        \"num_packets\": [np.nan, 10, np.nan],\n",
    "        \"num_bytes\": [np.nan, 1454, np.nan]\n",
    "    }),\n",
    "    combination=combine_streams(MockLANL(), [\"dns\", \"flows\"]).compute().sort_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mCombination of all streams\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Combination of all streams\n",
    "assert_(\n",
    "    dataframes_equal,\n",
    "    expected=test_df({\n",
    "        \"time\": [pd.Timestamp(s) for s in [\n",
    "            \"2015-01-05T13:00:05\",\n",
    "            \"2015-01-05T13:00:06\",\n",
    "            \"2015-01-05T13:00:32\",\n",
    "            \"2015-01-05T13:00:03\",\n",
    "            \"2015-01-05T13:00:24\",\n",
    "            \"2015-01-05T13:00:03\"\n",
    "        ]],\n",
    "        \"userdomain_init\": [\"U3@DOM1\", np.nan, np.nan, np.nan, np.nan, np.nan],\n",
    "        \"userdomain_focus\": [\"U3@DOM1\", \"U4@DOM1\", \"U3@DOM1\", np.nan, np.nan, np.nan],\n",
    "        \"host_init\": [\"C328\", np.nan, np.nan, np.nan, np.nan, np.nan],\n",
    "        \"host_focus\": [\"C345\", \"C45\", \"C45\", \"C89\", \"C234\", \"C89\"],\n",
    "        \"auth\": [\"NTLM\", np.nan, np.nan, np.nan, np.nan, np.nan],\n",
    "        \"logon\": [\"?\", np.nan, np.nan, np.nan, np.nan, np.nan],\n",
    "        \"direction\": [\"LogOn\", np.nan, np.nan, np.nan, np.nan, np.nan],\n",
    "        \"result\": [\"Success\", np.nan, np.nan, np.nan, np.nan, np.nan],\n",
    "        \"stream\": [\"auth\", \"proc\", \"proc\", \"dns\", \"dns\", \"flows\"],\n",
    "        \"host_resolved\": [np.nan, np.nan, np.nan, \"C2390\", \"C123\", np.nan],\n",
    "        \"duration\": [np.nan, np.nan, np.nan, np.nan, np.nan, 3],\n",
    "        \"port_focus\": [np.nan, np.nan, np.nan, np.nan, np.nan, \"N435\"],\n",
    "        \"host_server\": [np.nan, np.nan, np.nan, np.nan, np.nan, \"C2390\"],\n",
    "        \"port_server\": [np.nan, np.nan, np.nan, np.nan, np.nan, \"443\"],\n",
    "        \"protocol\": [np.nan, np.nan, np.nan, np.nan, np.nan, \"3\"],\n",
    "        \"num_packets\": [np.nan, np.nan, np.nan, np.nan, np.nan, 10],\n",
    "        \"num_bytes\": [np.nan, np.nan, np.nan, np.nan, np.nan, 1454],\n",
    "        \"process\": [np.nan, \"P3254\", \"P129\", np.nan, np.nan, np.nan],\n",
    "        \"action\": [np.nan, \"Start\", \"End\", np.nan, np.nan, np.nan]\n",
    "    }),\n",
    "    combination=combine_streams(MockLANL()).compute().sort_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mCombination of all streams beyond a timestamp\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Combination of all streams beyond a timestamp\n",
    "assert_(\n",
    "    dataframes_equal,\n",
    "    expected=test_df({\n",
    "        \"time\": [pd.Timestamp(s) for s in [\n",
    "            \"2015-01-05T13:00:32\",\n",
    "            \"2015-01-05T13:00:24\"\n",
    "        ]],\n",
    "        \"userdomain_init\": [np.nan, np.nan],\n",
    "        \"userdomain_focus\": [\"U3@DOM1\", np.nan],\n",
    "        \"host_init\": [np.nan, np.nan],\n",
    "        \"host_focus\": [\"C45\", \"C234\"],\n",
    "        \"auth\": [np.nan, np.nan],\n",
    "        \"logon\": [np.nan, np.nan],\n",
    "        \"direction\": [np.nan, np.nan],\n",
    "        \"result\": [np.nan, np.nan],\n",
    "        \"stream\": [\"proc\", \"dns\"],\n",
    "        \"host_resolved\": [np.nan, \"C123\"],\n",
    "        \"duration\": [np.nan, np.nan],\n",
    "        \"port_focus\": [np.nan, np.nan],\n",
    "        \"host_server\": [np.nan, np.nan],\n",
    "        \"port_server\": [np.nan, np.nan],\n",
    "        \"protocol\": [np.nan, np.nan],\n",
    "        \"num_packets\": [np.nan, np.nan],\n",
    "        \"num_bytes\": [np.nan, np.nan],\n",
    "        \"process\": [\"P129\", np.nan],\n",
    "        \"action\": [\"End\", np.nan]\n",
    "    }),\n",
    "    combination=combine_streams(MockLANL(), start=pd.Timestamp(\"2015-01-05T13:00:10\")).compute().sort_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mCombination of all streams prior to a timestamp\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Combination of all streams prior to a timestamp\n",
    "assert_(\n",
    "    dataframes_equal,\n",
    "    expected=test_df({\n",
    "        \"time\": [pd.Timestamp(s) for s in [\n",
    "            \"2015-01-05T13:00:05\",\n",
    "            \"2015-01-05T13:00:03\",\n",
    "            \"2015-01-05T13:00:03\"\n",
    "        ]],\n",
    "        \"userdomain_init\": [\"U3@DOM1\", np.nan, np.nan],\n",
    "        \"userdomain_focus\": [\"U3@DOM1\", np.nan, np.nan],\n",
    "        \"host_init\": [\"C328\", np.nan, np.nan],\n",
    "        \"host_focus\": [\"C345\", \"C89\", \"C89\"],\n",
    "        \"auth\": [\"NTLM\", np.nan, np.nan],\n",
    "        \"logon\": [\"?\", np.nan, np.nan],\n",
    "        \"direction\": [\"LogOn\", np.nan, np.nan],\n",
    "        \"result\": [\"Success\", np.nan, np.nan],\n",
    "        \"stream\": [\"auth\", \"dns\", \"flows\"],\n",
    "        \"host_resolved\": [np.nan, \"C2390\", np.nan],\n",
    "        \"duration\": [np.nan, np.nan, 3],\n",
    "        \"port_focus\": [np.nan, np.nan, \"N435\"],\n",
    "        \"host_server\": [np.nan, np.nan, \"C2390\"],\n",
    "        \"port_server\": [np.nan, np.nan, \"443\"],\n",
    "        \"protocol\": [np.nan, np.nan, \"3\"],\n",
    "        \"num_packets\": [np.nan, np.nan, 10],\n",
    "        \"num_bytes\": [np.nan, np.nan, 1454],\n",
    "        \"process\": [np.nan, np.nan, np.nan],\n",
    "        \"action\": [np.nan, np.nan, np.nan]\n",
    "    }),\n",
    "    combination=combine_streams(MockLANL(), end=pd.Timestamp(\"2015-01-05T13:00:05\")).compute().sort_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mCombination of all streams within a time interval\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Combination of all streams within a time interval\n",
    "assert_(\n",
    "    dataframes_equal,\n",
    "    expected=test_df({\n",
    "        \"time\": [pd.Timestamp(s) for s in [\n",
    "            \"2015-01-05T13:00:05\",\n",
    "            \"2015-01-05T13:00:06\"\n",
    "        ]],\n",
    "        \"userdomain_init\": [\"U3@DOM1\", np.nan],\n",
    "        \"userdomain_focus\": [\"U3@DOM1\", \"U4@DOM1\"],\n",
    "        \"host_init\": [\"C328\", np.nan],\n",
    "        \"host_focus\": [\"C345\", \"C45\"],\n",
    "        \"auth\": [\"NTLM\", np.nan],\n",
    "        \"logon\": [\"?\", np.nan],\n",
    "        \"direction\": [\"LogOn\", np.nan],\n",
    "        \"result\": [\"Success\", np.nan],\n",
    "        \"stream\": [\"auth\", \"proc\"],\n",
    "        \"host_resolved\": [np.nan, np.nan],\n",
    "        \"duration\": [np.nan, np.nan],\n",
    "        \"port_focus\": [np.nan, np.nan],\n",
    "        \"host_server\": [np.nan, np.nan],\n",
    "        \"port_server\": [np.nan, np.nan],\n",
    "        \"protocol\": [np.nan, np.nan],\n",
    "        \"num_packets\": [np.nan, np.nan],\n",
    "        \"num_bytes\": [np.nan, np.nan],\n",
    "        \"process\": [np.nan, \"P3254\"],\n",
    "        \"action\": [np.nan, \"Start\"]\n",
    "    }),\n",
    "    combination=combine_streams(\n",
    "        MockLANL(),\n",
    "        start=pd.Timestamp(\"2015-01-05T13:00:05\"),\n",
    "        end=pd.Timestamp(\"2015-01-05T13:00:10\")\n",
    "    ).compute().sort_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Writing a stream to CSV\n",
    "\n",
    "We restrict writing back CSV to streams that can hold in a reasonable amount of memory, meaning a single cluster node. In such a case, the stream is reduced from a Dask dataframe to a single-node regular Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def write_stream_csv(path: os.PathLike, stream: ddf.DataFrame, **options: Any) -> None:\n",
    "    df = stream.compute().sort_index()\n",
    "    df.insert(0, \"time\", df.index.to_series().apply(lambda ts: int((ts - Time.START).total_seconds())))\n",
    "    df.to_csv(path, index=False, **options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Selecting a feature subset for refining representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When computing artifact-specific numerical representations of data, sometimes one may want to restrict further analysis steps to a subset of artifacts. While embracing the joint analysis of all artifacts can be done (at high runtime and memory usage), it may yield a very generic representation less able to model normal activity *tightly*: such a representation may suffer from reduced sensitivity to anomalous phenomena. So artifact subsets can bring a tighter representation that performs better at anomaly detection. Naturally, only experimental evidence can verify or undermine such hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ArtifactIndex = Mapping[str, int]\n",
    "FeaturesRestricted = Tuple[np.ndarray, ArtifactIndex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Restricting to subset of artifact types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Restricting, for instance, to only host or focused host features could be used to detect as-yet unknown malicious activity over a whole network.\n",
    "\n",
    "In this dataset, not just any combination of vertex types make sense. Rather, one may want to work only with `host_focus` artifact features, or all artifacts whose name starts with `host`. So the type is chosen as a prefix of the actual encoded artifact name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _select_features(features: np.ndarray, vertices: Sequence[ig.Vertex]) -> FeaturesRestricted:\n",
    "    a2i: ArtifactIndex = OrderedDict((v[\"name\"], v.index) for v in vertices)\n",
    "    return features[list(a2i.values()), :], {name: i for i, name in enumerate(a2i.keys())}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def select_by_type(type_artifact: str, features: np.ndarray, graph: ig.Graph) -> FeaturesRestricted:\n",
    "    return _select_features(features, [v for v in graph.vs if v[\"name\"].startswith(type_artifact)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def graph_and_features_for_selection_test():\n",
    "    graph = ig.Graph()\n",
    "    vindex: Mapping[str, ig.Vertex] = {}\n",
    "    for name in [\n",
    "        \"userdomain_init:U34@DOM1\",\n",
    "        \"userdomain_focus:U34@DOM1\",\n",
    "        \"host_init:C23\",\n",
    "        \"host_focus:C87\",\n",
    "        \"host_focus:C27\",\n",
    "        \"process:P8\",\n",
    "        \"userdomain_focus:U12@DOM1\",\n",
    "        \"host_focus:C49\"\n",
    "    ]:\n",
    "        vindex[name] = graph.add_vertex(name)\n",
    "    for name_s, name_e in [\n",
    "        (\"host_init:C23\", \"userdomain_init:U34@DOM1\"),\n",
    "        (\"host_init:C23\", \"userdomain_focus:U34@DOM1\"),\n",
    "        (\"host_init:C23\", \"host_focus:C87\"),\n",
    "        (\"userdomain_init:U34@DOM1\", \"userdomain_focus:U34@DOM1\"),\n",
    "        (\"userdomain_init:U34@DOM1\", \"host_focus:C87\"),\n",
    "        (\"userdomain_focus:U34@DOM1\", \"host_focus:C87\"),\n",
    "        (\"userdomain_focus:U34@DOM1\", \"host_focus:C27\"),\n",
    "        (\"userdomain_focus:U34@DOM1\", \"process:P8\"),\n",
    "        (\"process:P8\", \"host_focus:C27\"),\n",
    "        (\"process:P8\", \"userdomain_focus:U12@DOM1\"),\n",
    "        (\"process:P8\", \"host_focus:C49\"),\n",
    "        (\"host_focus:C49\", \"userdomain_focus:U12@DOM1\")\n",
    "    ]:\n",
    "        graph.add_edge(vindex[name_s], vindex[name_e])\n",
    "        \n",
    "    features = np.array([\n",
    "        [0, 6, 8],\n",
    "        [5, 8, 9],\n",
    "        [1, 3, 8],\n",
    "        [4, 0, 0],\n",
    "        [1, 1, 1],\n",
    "        [10, 11, 3],\n",
    "        [7, 3, 6],\n",
    "        [9, 2, 2]\n",
    "    ])\n",
    "    assert (features[[v.index for v in graph.vs], :] == features).all()\n",
    "    return graph, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def arrays_equal(**kwargs: np.ndarray) -> ExplanationOnFailure:\n",
    "    left, right = kwargs.values()\n",
    "    if left.size != right.size:\n",
    "        return Explanation(\"Arrays of distinct size\", join_args([], kwargs))\n",
    "    if not np.isclose(left, right, equal_nan=True, atol=1e-5).all():\n",
    "        return Explanation(\"Arrays not equal within tolerance\", join_args([], kwargs))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mSelecting features by single vertex type\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Selecting features by single vertex type\n",
    "graph, features = graph_and_features_for_selection_test()\n",
    "selected, a2i = select_by_type(\"host_focus\", features, graph)\n",
    "assert_(\n",
    "    arrays_equal,\n",
    "    expected=np.array([\n",
    "        [4, 0, 0],\n",
    "        [1, 1, 1],\n",
    "        [9, 2, 2]\n",
    "    ]),\n",
    "    selected=selected\n",
    ")\n",
    "assert_(eq, expected={\"host_focus:C87\": 0, \"host_focus:C27\": 1, \"host_focus:C49\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mSelecting features by more general vertex type\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Selecting features by more general vertex type\n",
    "graph, features = graph_and_features_for_selection_test()\n",
    "selected, a2i = select_by_type(\"host\", features, graph)\n",
    "assert_(\n",
    "    arrays_equal,\n",
    "    expected=np.array([\n",
    "        [1, 3, 8],\n",
    "        [4, 0, 0],\n",
    "        [1, 1, 1],\n",
    "        [9, 2, 2]\n",
    "    ]),\n",
    "    selected=selected\n",
    ")\n",
    "assert_(eq, expected={\"host_init:C23\": 0, \"host_focus:C87\": 1, \"host_focus:C27\": 2, \"host_focus:C49\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Restricting to the neighbourhood of a set of artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When investigating a compromise suspected around a certain host, then looking at anomalies in a representation of all artifacts related one step from it in the graph (its *family*) may yield fruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def select_family(artifacts_: Union[str, Container[str]], features: np.ndarray, graph: ig.Graph) -> FeaturesRestricted:\n",
    "    artifacts: Set[str]\n",
    "    if isinstance(artifacts_, str):\n",
    "        artifacts = {artifacts_}\n",
    "    else:\n",
    "        artifacts = set(artifacts_)\n",
    "\n",
    "    vs = set(graph.vs(name_in=artifacts))\n",
    "    return _select_features(features, vs | set(sum([v.neighbors() for v in vs], [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mSelect family of a single vertex\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Select family of a single vertex\n",
    "graph, features = graph_and_features_for_selection_test()\n",
    "selected, a2i = select_family(\"host_focus:C87\", features, graph)\n",
    "assert_(\n",
    "    eq,\n",
    "    expected=[\"host_focus:C87\", \"host_init:C23\", \"userdomain_focus:U34@DOM1\", \"userdomain_init:U34@DOM1\"],\n",
    "    a2i_keys=sorted(list(a2i.keys()))\n",
    ")\n",
    "assert_(eq, expected=list(range(4)), a2i_indexes=sorted(list(a2i.values())))\n",
    "for name, i in a2i.items():\n",
    "    v, *_ = graph.vs(name=name)\n",
    "    assert_(arrays_equal, expected=features[v.index, :], selected=selected[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mSelect family of two neighbouring vertices\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Select family of two neighbouring vertices\n",
    "graph, features = graph_and_features_for_selection_test()\n",
    "selected, a2i = select_family([\"host_focus:C87\", \"host_init:C23\"], features, graph)\n",
    "assert_(\n",
    "    eq,\n",
    "    expected=[\"host_focus:C87\", \"host_init:C23\", \"userdomain_focus:U34@DOM1\", \"userdomain_init:U34@DOM1\"],\n",
    "    a2i_keys=sorted(list(a2i.keys()))\n",
    ")\n",
    "assert_(eq, expected=list(range(4)), a2i_indexes=sorted(list(a2i.values())))\n",
    "for name, i in a2i.items():\n",
    "    v, *_ = graph.vs(name=name)\n",
    "    assert_(arrays_equal, expected=features[v.index, :], selected=selected[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mSelect family of vertices with overlapping but not identical neighbourhoods\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Select family of vertices with overlapping but not identical neighbourhoods\n",
    "graph, features = graph_and_features_for_selection_test()\n",
    "selected, a2i = select_family([\"host_focus:C87\", \"host_focus:C27\"], features, graph)\n",
    "assert_(\n",
    "    eq,\n",
    "    expected=[\n",
    "        \"host_focus:C27\",\n",
    "        \"host_focus:C87\",\n",
    "        \"host_init:C23\",\n",
    "        \"process:P8\",\n",
    "        \"userdomain_focus:U34@DOM1\",\n",
    "        \"userdomain_init:U34@DOM1\"\n",
    "    ],\n",
    "    a2i_keys=sorted(list(a2i.keys()))\n",
    ")\n",
    "assert_(eq, expected=list(range(6)), a2i_indexes=sorted(list(a2i.values())))\n",
    "for name, i in a2i.items():\n",
    "    v, *_ = graph.vs(name=name)\n",
    "    assert_(arrays_equal, expected=features[v.index, :], selected=selected[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mSelect family of vertices without a overlapping neighbourhoods\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Select family of vertices without a overlapping neighbourhoods\n",
    "graph, features = graph_and_features_for_selection_test()\n",
    "selected, a2i = select_family([\"host_focus:C87\", \"host_focus:C49\"], features, graph)\n",
    "assert_(\n",
    "    eq,\n",
    "    expected=[\n",
    "        \"host_focus:C49\",\n",
    "        \"host_focus:C87\",\n",
    "        \"host_init:C23\",\n",
    "        \"process:P8\",\n",
    "        \"userdomain_focus:U12@DOM1\",\n",
    "        \"userdomain_focus:U34@DOM1\",\n",
    "        \"userdomain_init:U34@DOM1\"\n",
    "    ],\n",
    "    a2i_keys=sorted(list(a2i.keys()))\n",
    ")\n",
    "assert_(eq, expected=list(range(7)), a2i_indexes=sorted(list(a2i.values())))\n",
    "for name, i in a2i.items():\n",
    "    v, *_ = graph.vs(name=name)\n",
    "    assert_(arrays_equal, expected=features[v.index, :], selected=selected[i, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Labeling artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `redteam.txt.gz` file carries direct information about authentication (`auth.txt.gz`) records corresponding to logon actions by attackers. The following function provides a labeling oracle for *artifacts* from any record within a time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@DataStoreLosAlamos.method\n",
    "def get_redteam(self) -> pd.DataFrame:\n",
    "    return read_lanl_csv(op.join(self.dir_base, \"redteam.txt.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as ddf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "Artifact = Tuple[str, str]\n",
    "\n",
    "\n",
    "@DataStoreLosAlamos.method\n",
    "def label(\n",
    "    self,\n",
    "    artifacts: Iterator[Artifact],\n",
    "    interval_time: Tuple[pd.Timestamp, pd.Timestamp]\n",
    ") -> Iterator[Tuple[Artifact, float]]:\n",
    "    lower, upper = interval_time\n",
    "    auth = self.get_stream(\"auth\")[lower:upper]\n",
    "    redteam = self.get_redteam()\n",
    "    auth_labeled: ddf.DataFrame = auth.merge(\n",
    "        redteam,\n",
    "        on=[\"time\", \"userdomain_focus\", \"host_init\", \"host_focus\"]\n",
    "    ).compute()\n",
    "    artifacts_malicious = {col: set(auth_labeled[col]) for col in auth_labeled.columns}\n",
    "    set_empty = set()\n",
    "    for column, value in artifacts:\n",
    "        if value in artifacts_malicious.get(column, set_empty):\n",
    "            yield ((column, value), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test_label(\n",
    "    interval_time: Tuple[pd.Timestamp, pd.Timestamp],\n",
    "    expected: List[Tuple[Artifact, float]]\n",
    ") -> None:\n",
    "    DATA = {\n",
    "        \"proc\": b\"\",\n",
    "        \"flows\": b\"\",\n",
    "        \"dns\": b\"\",\n",
    "        \"auth\": b\"\"\"\\\n",
    "63,U34@DOM1,U23@DOM1,C98,C98,Kerberos,Network,LogOn,Success\n",
    "91,U67@DOM1,SYSTEM@C89,C89,C89,Negotiate,Service,LogOn,Success\n",
    "304,U45@DOM1,U45@DOM1,C234,C329,Kerberos,Network,LogOff,Success\n",
    "897,U93@DOM1,U93@DOM1,C123,C123,Kerberos,Network,LogOn,Success\n",
    "956,U93@DOM1,U93@DOM1,C123,C123,Kerberos,Network,LogOff,Success\n",
    "3456,U67@DOM1,U45@DOM1,C89,C329,Kerberos,Network,LogOn,Failure\n",
    "4127,U980@DOM1,U980@DOM1,C23,C32,Kerberos,Service,LogOn,Success\n",
    "\"\"\",\n",
    "        \"redteam\": b\"\"\"\\\n",
    "91,SYSTEM@C89,C89,C89\n",
    "897,U93@DOM1,C123,C123\n",
    "3456,U45@DOM1,C89,C329\n",
    "\"\"\"\n",
    "    }\n",
    "\n",
    "    import gzip\n",
    "    DIR_LANL = (\"./...lanl...\")\n",
    "    try:\n",
    "        os.makedirs(DIR_LANL)\n",
    "        for path, content in DATA.items():\n",
    "            with gzip.open(op.join(DIR_LANL, path + \".txt.gz\"), \"wb\") as file:\n",
    "                file.write(content)\n",
    "\n",
    "        assert_(\n",
    "            eq,\n",
    "            expected=sorted(expected),\n",
    "            labeled=sorted(list(DataStoreLosAlamos(DIR_LANL).label(\n",
    "                [\n",
    "                    (\"userdomain_init\", \"U93@DOM1\"),\n",
    "                    (\"userdomain_focus\", \"SYSTEM@C89\"),\n",
    "                    (\"host_init\", \"C234\"),\n",
    "                    (\"host_focus\", \"C459\"),\n",
    "                    (\"host_focus\", \"C329\"),\n",
    "                    (\"userdomain_focus\", \"U980@DOM1\")\n",
    "                ],\n",
    "                interval_time\n",
    "            )))\n",
    "        )\n",
    "    finally:\n",
    "        if os.path.isdir(DIR_LANL):\n",
    "            shutil.rmtree(DIR_LANL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mArtifact labeling across the whole timeframe\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Artifact labeling across the whole timeframe\n",
    "test_label(\n",
    "    (Time.START, Time.END),\n",
    "    [\n",
    "        ((\"userdomain_init\", \"U93@DOM1\"), 1.0),\n",
    "        ((\"userdomain_focus\", \"SYSTEM@C89\"), 1.0),\n",
    "        ((\"host_focus\", \"C329\"), 1.0)\n",
    "    ]\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mArtifact labeling on a prefix of the timeframe\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Artifact labeling on a prefix of the timeframe\n",
    "test_label(\n",
    "    (Time.START, Time.START + pd.Timedelta(seconds=1000)),\n",
    "    [\n",
    "        ((\"userdomain_init\", \"U93@DOM1\"), 1.0),\n",
    "        ((\"userdomain_focus\", \"SYSTEM@C89\"), 1.0)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 passed, \u001b[37m0 failed\u001b[0m, \u001b[37m0 raised an error\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    _ = summarize_results(suite)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "543px",
    "width": "511px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "269px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
