{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Los Alamos Cybersecurity dataset tools\n",
    "\n",
    "Includes tools to manipulate dataset storage, as well as load events from said storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset chunking\n",
    "\n",
    "This dataset is large! To facilitate its processing, it is best to cut its bigger files into *chunks*, which can be processed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on --max_line_length 120 --ignore W293,E302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notebooks_as_modules\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import dask\n",
    "import dask.dataframe as ddf\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from glob import glob\n",
    "from growing import growing\n",
    "import gzip\n",
    "import io\n",
    "from jupytest import Suite, Report, Magic, summarize_results\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as op\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from typing import Iterable, ContextManager, TypeVar, Any, Sequence, Tuple, Mapping\n",
    "from unittest.mock import patch, Mock, call, MagicMock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = Suite()\n",
    "if __name__ == \"__main__\":\n",
    "    suite |= Report()\n",
    "    suite |= Magic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's embody the intended file hierarchy of the LANL dataset into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@growing\n",
    "class DataStoreLosAlamos:\n",
    "    \"\"\"\n",
    "    Main files making up the Los Alamos Cybersecurity dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path: os.PathLike) -> None:\n",
    "        self._dir_base = path\n",
    "        \n",
    "    @property\n",
    "    def dir_base(self) -> os.PathLike:\n",
    "        return self._dir_base\n",
    "    \n",
    "    def __dask_tokenize__(self) -> str:\n",
    "        return self.dir_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cutting the raw files into compressed chunks is a long-running computation. Let's structure it so it's run into a compute cluster, when we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_CHUNK = (2 << 25) + (2 << 24)  # 96 MB maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@DataStoreLosAlamos.method(wrapped_in=dask.delayed(pure=True))\n",
    "def join_chunked(self, stream: str, *p: os.PathLike, size_chunk: int = SIZE_CHUNK) -> os.PathLike:\n",
    "    path_stream_chunked = op.join(self.dir_base, \"chunked\", stream)\n",
    "    os.makedirs(path_stream_chunked, exist_ok=True)\n",
    "    \n",
    "    names_chunk = glob(op.join(path_stream_chunked, \"*.txt.gz\"))\n",
    "    if len(names_chunk) == 0 or any(os.stat(p).st_size == 0 for p in names_chunk):\n",
    "        # Raw files have not been chunked yet, or some chunks are corrupted. It's chunking time.\n",
    "        with gzip.open(op.join(self.dir_base, f\"{stream}.txt.gz\"), \"rb\") as file_raw:\n",
    "            for index in range(sys.maxsize):\n",
    "                with FileChunk(\n",
    "                    op.join(self.dir_base, \"chunked\", stream, f\"{index:04d}.txt.gz\"),\n",
    "                    size_chunk\n",
    "                ) as file_chunk:\n",
    "                    for line in file_raw:\n",
    "                        if not file_chunk.write(line):\n",
    "                            break\n",
    "                    else:\n",
    "                        break  # ...out of outer infinite loop.\n",
    "                        \n",
    "    return op.join(path_stream_chunked, *p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests** for method `join_chunked`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def mocking_global(name: str, value_mock: T) -> ContextManager[T]:\n",
    "    must_restore = False\n",
    "    G = globals()\n",
    "    if name in G:\n",
    "        value_orig = G[name]\n",
    "        must_restore = True\n",
    "    G[name] = value_mock\n",
    "    \n",
    "    try:\n",
    "        yield value_mock\n",
    "    finally:\n",
    "        if must_restore:\n",
    "            G[name] = value_orig\n",
    "        else:\n",
    "            del G[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_file_raw(lines: Iterable[str]) -> Mock:\n",
    "    mock = Mock()\n",
    "    mock.__enter__ = lambda self: self\n",
    "    mock.__exit__ = lambda self, t, v, tb: False\n",
    "    iter_lines = iter(lines)\n",
    "    mock.__iter__ = lambda self: iter_lines\n",
    "    return mock\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def mocking_gzip_open(lines: Iterable[str]) -> ContextManager[Mock]:\n",
    "    with patch(\"gzip.open\", return_value=mock_file_raw(lines)) as mock:\n",
    "        yield mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_file_chunk(**kwargs: Any) -> Mock:\n",
    "    mock = Mock()\n",
    "    mock.__enter__ = lambda self: self\n",
    "    mock.__exit__ = lambda self, t, v, tb: False\n",
    "    mock.write = Mock(**kwargs)\n",
    "    return mock\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def mocking_FileChunk(mocks: Sequence[Mock]) -> ContextManager[Mock]:\n",
    "    with mocking_global(\"FileChunk\", Mock(side_effect=mocks)) as mock:\n",
    "        yield mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mjoin-chunked/Stop\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test join-chunked/Stop\n",
    "mocks_chunk = [\n",
    "    mock_file_chunk(**kwargs)\n",
    "    for kwargs in [dict(side_effect=[True, False]), dict(side_effect=[True, False]), dict(return_value=True)]\n",
    "]\n",
    "with patch(\"os.makedirs\"), patch(\"glob.glob\", return_value=[]),\\\n",
    "        mocking_gzip_open([b\"asdf\\n\", b\"qwer\\n\", b\"zxcv\\n\", b\"qwerty\\n\", b\"uiop\\n\"]) as mock_raw,\\\n",
    "        mocking_FileChunk(mocks_chunk) as mock_class:\n",
    "    ds = DataStoreLosAlamos(\"/path/to/data\")\n",
    "    assert ds.join_chunked(\"dns\", \"asdf\", \"qwer\", size_chunk=10).compute(scheduler=\"single-threaded\") ==\\\n",
    "        \"/path/to/data/chunked/dns/asdf/qwer\"\n",
    "\n",
    "    mock_class.assert_has_calls(\n",
    "        [call(f\"/path/to/data/chunked/dns/{i:04d}.txt.gz\", 10) for i in range(3)]\n",
    "    )\n",
    "    mocks_chunk[0].write.assert_has_calls([call(s) for s in [b\"asdf\\n\", b\"qwer\\n\"]])\n",
    "    mocks_chunk[1].write.assert_has_calls([call(s) for s in [b\"zxcv\\n\", b\"qwerty\\n\"]])\n",
    "    mocks_chunk[2].write.assert_has_calls([call(s) for s in [b\"uiop\\n\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mjoin-chunked/End of raw file corresponds to end of chunk\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test join-chunked/End of raw file corresponds to end of chunk\n",
    "mocks_chunk = [\n",
    "    mock_file_chunk(**kwargs)\n",
    "    for kwargs in [dict(side_effect=[True, False]), dict(side_effect=[True, False]), dict(return_value=True)]\n",
    "]\n",
    "with patch(\"os.makedirs\"), patch(\"glob.glob\", return_value=[]),\\\n",
    "        mocking_gzip_open([b\"asdf\\n\", b\"qwer\\n\", b\"zxcv\\n\", b\"qwerty\\n\"]) as mock_raw,\\\n",
    "        mocking_FileChunk(mocks_chunk) as mock_class:\n",
    "    ds = DataStoreLosAlamos(\"/path/to/data\")\n",
    "    assert ds.join_chunked(\"dns\", \"asdf\", \"qwer\", size_chunk=10).compute(scheduler=\"single-threaded\") ==\\\n",
    "        \"/path/to/data/chunked/dns/asdf/qwer\"\n",
    "\n",
    "    mock_class.assert_has_calls(\n",
    "        [call(f\"/path/to/data/chunked/dns/{i:04d}.txt.gz\", 10) for i in range(3)]\n",
    "    )\n",
    "    mocks_chunk[0].write.assert_has_calls([call(s) for s in [b\"asdf\\n\", b\"qwer\\n\"]])\n",
    "    mocks_chunk[1].write.assert_has_calls([call(s) for s in [b\"zxcv\\n\", b\"qwerty\\n\"]])\n",
    "    mocks_chunk[2].write.assert_not_called()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mjoin-chunked/Raw file is empty\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test join-chunked/Raw file is empty\n",
    "mocks_chunk = [\n",
    "    mock_file_chunk(**kwargs)\n",
    "    for kwargs in [dict(side_effect=[True, False]), dict(side_effect=[True, False]), dict(return_value=True)]\n",
    "]\n",
    "with patch(\"os.makedirs\"), patch(\"glob.glob\", return_value=[]),\\\n",
    "        mocking_gzip_open([]) as mock_raw,\\\n",
    "        mocking_FileChunk(mocks_chunk) as mock_class:\n",
    "    ds = DataStoreLosAlamos(\"/path/to/data\")\n",
    "    assert ds.join_chunked(\"dns\", \"asdf\", \"qwer\").compute(scheduler=\"single-threaded\") ==\\\n",
    "        \"/path/to/data/chunked/dns/asdf/qwer\"\n",
    "\n",
    "    mock_class.assert_called_once_with(\"/path/to/data/chunked/dns/0000.txt.gz\", SIZE_CHUNK)\n",
    "    for mock in mocks_chunk:\n",
    "        mock.write.assert_not_called()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class `FileChunk` then embodies the creation of a chunk and the transfer of its content into the target file. Note that the algorithm of `join_chunked()` made it so the context of the `FileChunk` instance is entered before we have any content for the chunk; the creation of the file should thus be delayed to a call to method `write()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileChunk:\n",
    "    \"\"\"\n",
    "    Delays the creation of a chunk file until the user commits to writing something in it.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path: os.PathLike, limit: int) -> None:\n",
    "        self._path = path\n",
    "        self._file: Optional[io.RawByteIO] = None\n",
    "        self._limit = limit\n",
    "        self._size = 0\n",
    "        \n",
    "    def __enter__(self) -> \"FileChunk\":\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, type_exc, value_exc, tb_exc) -> bool:\n",
    "        if self._file is not None:\n",
    "            self._file.close()\n",
    "        return False\n",
    "    \n",
    "    def write(self, buf: bytes) -> bool:\n",
    "        if self._file is None:\n",
    "            self._file = gzip.open(self._path, \"wb\")\n",
    "\n",
    "        index = 0\n",
    "        while index < len(buf):\n",
    "            index += self._file.write(buf[index:])\n",
    "            \n",
    "        self._size += len(buf)\n",
    "        return self._size < self._limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mFileChunk/No file created without write\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test FileChunk/No file created without write\n",
    "with patch(\"gzip.open\") as mock:\n",
    "    with FileChunk(\"asdf\", 100) as file_chunk:\n",
    "        pass\n",
    "    mock.assert_not_called()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mFileChunk.write/All written in one single underlying write\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test FileChunk.write/All written in one single underlying write\n",
    "bytes_written = io.BytesIO()\n",
    "with patch(\"gzip.open\", return_value=bytes_written):\n",
    "    with FileChunk(\"asdf\", 100) as file_chunk:\n",
    "        assert file_chunk.write(b\"qwerty\\n\")\n",
    "        assert bytes_written.getvalue() == b\"qwerty\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mFileChunk.write/Multiple underlying writes needed\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test FileChunk.write/Multiple underlying writes needed\n",
    "with patch(\"gzip.GzipFile\") as mock:\n",
    "    mock.return_value.write = Mock(side_effect=[3, 4])\n",
    "    with FileChunk(\"asdf\", 100) as file_chunk:\n",
    "        assert file_chunk.write(b\"qwerty\\n\")\n",
    "    mock.return_value.write.assert_has_calls([call(b'qwerty\\n',), call(b'rty\\n',)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mFileChunk.write/Return False once chunk once exactly full\u001b[0m passed.\n",
      "Test \u001b[1mFileChunk.write/Return False once chunk once beyond full\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "def test_chunk_filling(last: bytes) -> None:\n",
    "    bytes_written = io.BytesIO()\n",
    "    with patch(\"gzip.open\", return_value=bytes_written):\n",
    "        with FileChunk(\"asdf\", 25) as file_chunk:\n",
    "            assert file_chunk.write(b\"asdf\\nqwer\\n\")\n",
    "            assert file_chunk.write(b\"zxcv\\n\")\n",
    "            assert file_chunk.write(b\"uiop\\n\")\n",
    "            assert not file_chunk.write(last)\n",
    "            assert bytes_written.getvalue() == b\"asdf\\nqwer\\nzxcv\\nuiop\\n\" + last\n",
    "    \n",
    "\n",
    "for adverb, last in [(\"exactly\", b\"1234\\n\"), (\"beyond\", b\"1234567890\\n\")]:\n",
    "    suite.test(\n",
    "        test_chunk_filling,\n",
    "        args=(last,),\n",
    "        name=f\"FileChunk.write/Return False once chunk once {adverb} full\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@DataStoreLosAlamos.method\n",
    "def join_experiments(self, *p: os.PathLike) -> os.PathLike:\n",
    "    return op.join(self.dir_base, \"experiments\", *p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a data stream into a Dask dataframe\n",
    "\n",
    "Dask dataframes easily leverage the chunking of the streams that we have wrought. However, given how the chunks are already sorted, one gets the best benefits from these by supplying knowledge of the *divisions* of the index key (here, time) across the partitions. This is why we implement a custom dataframe loading that quickly extracts the division knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event timestamps\n",
    "\n",
    "The Los Alamos Cybersecurity dataset has been captured over a period of two months, but the exact dates are unknown; the timestamps provided in the dataset start at 0. While mapping these directly to timestamps would yield funny 1970's dates to events, we rather choose a more modern setting. Given the late-2015 moment the dataset was released, we shall assume the acquisition ran from January 1st, 2015, to February 27th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2015-02-27 23:59:59.999999999')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@growing\n",
    "class Time:\n",
    "    START = pd.Timestamp(\"2015-01-01T00:00:00\")\n",
    "    END = pd.Timestamp(\"2015-02-28T00:00:00\") - pd.Timedelta(nanoseconds=1)\n",
    "\n",
    "\n",
    "Time.END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds2ts(n: str) -> pd.Timestamp:\n",
    "    return Time.START + pd.Timedelta(seconds=int(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mTimestamp mapping\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Timestamp mapping\n",
    "assert seconds2ts(\"3600\") == pd.Timestamp(\"2015-01-01T01:00:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data schemas\n",
    "\n",
    "The `SCHEMAS` dictionary describe the columns for each of the four main data streams, in addition to the label array stored in `redteam.txt.gz`. Each stream is sorted (and thus indexable) by its `time` column, which is omitted from the schema descriptions to facilitate the usage of the schema objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA = Sequence[Tuple[str, str]]\n",
    "\n",
    "\n",
    "SCHEMAS: Mapping[str, SCHEMA] = {\n",
    "    \"dns\": [\n",
    "        (\"computer_source\", \"object\"),\n",
    "        (\"computer_destination\", \"object\")\n",
    "    ],\n",
    "    \"flows\": [\n",
    "        (\"duration\", \"int64\"),\n",
    "        (\"computer_source\", \"object\"),\n",
    "        (\"port_source\", \"object\"),\n",
    "        (\"computer_destination\", \"object\"),\n",
    "        (\"port_destination\", \"object\"),\n",
    "        (\"protocol\", \"category\"),\n",
    "        (\"num_packets\", \"int32\"),\n",
    "        (\"num_bytes\", \"int64\")\n",
    "    ],\n",
    "    \"proc\": [\n",
    "        (\"userdomain_source\", \"object\"),\n",
    "        (\"computer_source\", \"object\"),\n",
    "        (\"process\", \"object\"),\n",
    "        (\"action\", \"category\")\n",
    "    ],\n",
    "    \"auth\": [\n",
    "        (\"userdomain_source\", \"object\"),\n",
    "        (\"userdomain_destination\", \"object\"),\n",
    "        (\"computer_source\", \"object\"),\n",
    "        (\"computer_destination\", \"object\"),\n",
    "        (\"auth\", \"category\"),\n",
    "        (\"logon\", \"category\"),\n",
    "        (\"direction\", \"category\"),\n",
    "        (\"result\", \"category\")\n",
    "    ],\n",
    "    \"redteam\": [\n",
    "        (\"userdomain_source\", \"object\"),\n",
    "        (\"computer_source\", \"object\"),\n",
    "        (\"computer_destination\", \"object\")\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figuring out which stream a file is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "RXS_NAMES_STREAM = r\"(\" + \"|\".join(SCHEMAS.keys()) + \")\"\n",
    "RX_PATH2STREAM = re.compile(r\"/\" + RXS_NAMES_STREAM + r\"/|\" + RXS_NAMES_STREAM + r\"\\.txt\\.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path2stream(path: os.PathLike) -> str:\n",
    "    m = re.search(RX_PATH2STREAM, path)\n",
    "    if m is None:\n",
    "        return \"\"\n",
    "    return m.group(1) or m.group(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mStream name for a raw file\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Stream name for a raw file\n",
    "assert path2stream(\"/data/lanl/redteam.txt.gz\") == \"redteam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mStream name for a chunk file\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Stream name for a chunk file\n",
    "assert path2stream(\"/data/lanl/chunked/auth/0034.txt.gz\") == \"auth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mStream name for a stream-specific processing result (not chunking)\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Stream name for a stream-specific processing result (not chunking)\n",
    "assert path2stream(\"/data/lanl/experiments/asdf/qwer/proc/zxcv\") == \"proc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mPath with no stream name\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Path with no stream name\n",
    "assert not path2stream(\"/data/lanl/wtf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the first timestamp of a LANL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp_lower(path: os.PathLike) -> pd.Timestamp:\n",
    "    with gzip.open(path, \"rb\") as file:\n",
    "        line1 = next(file)  # Assumption: no file empty.\n",
    "        num_seconds, *_ = line1.split(b\",\")\n",
    "        return seconds2ts(num_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mFirst timestamp for line of a DNS stream file\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test First timestamp for line of a DNS stream file\n",
    "with patch(\"gzip.open\", return_value=io.BytesIO(b\"90842,C326,C89\\n\")):\n",
    "    assert get_timestamp_lower(\"asdf\") == pd.Timestamp(\"2015-01-02T01:14:02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mFirst timestamp for line of a flows stream file\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test First timestamp for line of a flows stream file\n",
    "with patch(\"gzip.open\", return_value=io.BytesIO(b\"2957021,2,C347,50234,C812,443,https,12,15723\\nqwerty\\n\")):\n",
    "    assert get_timestamp_lower(\"asdf\") == pd.Timestamp(\"2015-02-04T05:23:41\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a LANL CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lanl_csv(path: os.PathLike, **kwargs: Any) -> pd.DataFrame:\n",
    "    stream = path2stream(path)\n",
    "    if not stream:\n",
    "        raise ValueError(f\"Path {path} does not involve a LANL data stream.\")\n",
    "    schema = SCHEMAS[stream]\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        header=None,\n",
    "        names=[\"time\"] + [attr for attr, _ in schema],\n",
    "        dtype=dict(schema),\n",
    "        parse_dates=[\"time\"],\n",
    "        date_parser=seconds2ts,\n",
    "        index_col=\"time\",\n",
    "        compression=\"gzip\",\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def dummy_proc_content() -> ContextManager[os.PathLike]:\n",
    "    content = b\"\"\"\\\n",
    "3,C3@DOM1,C4,P2,Start\n",
    "18,C89@DOM1,C23,P78,Start\n",
    "29,C14@DOM1,C90,P123,Start\n",
    "53,C90@DOM1,C34,P23,End\n",
    "\"\"\"\n",
    "    with patch(\"gzip.builtins.open\", return_value=io.BytesIO(gzip.compress(content))):\n",
    "        yield \"/path/with/proc/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mReading LANL content\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Reading LANL content\n",
    "with dummy_proc_content() as path:\n",
    "    df = read_lanl_csv(path)\n",
    "assert len(df) == 4\n",
    "assert len(df.columns) == 4\n",
    "assert df.index.dtype == np.dtype(\"datetime64[ns]\")\n",
    "assert {c: str(dt) for c, dt in df.dtypes.items()} == {\n",
    "    \"userdomain_source\": \"object\",\n",
    "    \"computer_source\": \"object\",\n",
    "    \"process\": \"object\",\n",
    "    \"action\": \"category\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together into a Dask dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@DataStoreLosAlamos.method\n",
    "def get_stream(self, name: str) -> ddf.DataFrame:\n",
    "    schema = SCHEMAS[name]\n",
    "    paths_chunk = sorted(glob(self.join_chunked(name, \"*.txt.gz\").compute()))\n",
    "    divisions = [get_timestamp_lower(path) for path in paths_chunk] + [Time.END]\n",
    "    return ddf.from_delayed(\n",
    "        [dask.delayed(read_lanl_csv)(path) for path in paths_chunk],\n",
    "        meta=dict(schema),\n",
    "        divisions=divisions,\n",
    "        prefix=\"load_chunk\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mStream dataframe coherence\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "%%test Stream dataframe coherence\n",
    "ds = DataStoreLosAlamos(\"/lanl\")\n",
    "indices = [3, 2, 4, 0, 1]\n",
    "with patch(\"__main__.glob\", side_effect=[[f\"/lanl/flows/{n:04d}.txt.gz\" for n in indices]]),\\\n",
    "        patch.object(ds, \"join_chunked\", new=MagicMock()),\\\n",
    "        patch(\n",
    "            \"__main__.get_timestamp_lower\",\n",
    "            side_effect=[pd.Timestamp(s) for s in [\n",
    "                \"2015-01-01T00:00:04\",\n",
    "                \"2015-01-11T12:45:32\",\n",
    "                \"2015-01-27T18:19:19\",\n",
    "                \"2015-02-12T14:10:23\",\n",
    "                \"2015-02-23T18:02:38\"\n",
    "            ]]\n",
    "        ):\n",
    "    df = ds.get_stream(\"flows\")\n",
    "    assert df.npartitions == 5\n",
    "    assert list(df.index.divisions) == [pd.Timestamp(s) for s in [\n",
    "        \"2015-01-01T00:00:04\",\n",
    "        \"2015-01-11T12:45:32\",\n",
    "        \"2015-01-27T18:19:19\",\n",
    "        \"2015-02-12T14:10:23\",\n",
    "        \"2015-02-23T18:02:38\",\n",
    "        \"2015-02-27T23:59:59.999999999\"\n",
    "    ]]\n",
    "    assert dict(df.dtypes) == dict(SCHEMAS[\"flows\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mResolving a complete dataframe\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "@suite.test(name=\"Resolving a complete dataframe\")\n",
    "def resolving_whole_dataframe():\n",
    "    all_events = [\n",
    "        b\"\"\"\\\n",
    "    63,U34@DOM1,U23@DOM1,C98,C98,Kerberos,Network,LogOn,Success\n",
    "    91,U67@DOM1,SYSTEM@C89,C89,C89,Negotiate,Service,LogOn,Success\n",
    "    \"\"\",\n",
    "        b\"\"\"\\\n",
    "    304,U45@DOM1,U45@DOM1,C234,C329,Kerberos,Network,LogOff,Success\n",
    "    897,U93@DOM1,U93@DOM1,C123,C123,Kerberos,Network,LogOn,Success\n",
    "    \"\"\",\n",
    "        b\"\"\"\\\n",
    "    956,U93@DOM1,U93@DOM1,C123,C123,Kerberos,Network,LogOff,Success\n",
    "    3456,U67@DOM1,U45@DOM1,C89,C329,Kerberos,Network,LogOn,Failure\n",
    "    4127,U980@DOM1,U980@DOM1,C23,C32,Kerberos,Service,LogOn,Success\n",
    "    \"\"\"\n",
    "    ]\n",
    "\n",
    "    map_content = {\n",
    "        op.join(\"/lanl\", \"chunked\", \"auth\", f\"{n:02d}.txt.gz\"): content\n",
    "        for n, content in enumerate(all_events)\n",
    "    }\n",
    "\n",
    "    def grab_content(path: os.PathLike, *args, **kwargs) -> io.RawIOBase:\n",
    "        return io.BytesIO(gzip.compress(map_content[path]))\n",
    "\n",
    "    ds = DataStoreLosAlamos(\"asdf\")\n",
    "    with patch.object(ds, \"join_chunked\", new=MagicMock()),\\\n",
    "            patch(\"__main__.glob\", side_effect=[list(map_content.keys())]),\\\n",
    "            patch(\"gzip.builtins.open\", side_effect=grab_content):\n",
    "        df = ds.get_stream(\"auth\")\n",
    "        assert df.npartitions == 3\n",
    "        assert list(df.divisions) == [pd.Timestamp(s) for s in [\n",
    "            \"2015-01-01T00:01:03\",\n",
    "            \"2015-01-01T00:05:04\",\n",
    "            \"2015-01-01T00:15:56\"\n",
    "        ]] + [Time.END]\n",
    "        df_realized = df.compute()\n",
    "        assert dict(df_realized.dtypes) == dict(SCHEMAS[\"auth\"])\n",
    "\n",
    "    for line, ts_and_row in zip(b\"\".join(all_events).split(b\"\\n\"), df_realized.iterrows()):\n",
    "        num_seconds_expected, *cols_expected = line.split(b\",\")\n",
    "        ts_expected = seconds2ts(num_seconds_expected)\n",
    "        ts_obtained, row_obtained = ts_and_row\n",
    "        assert ts_expected == ts_obtained\n",
    "        assert [str(c, encoding=\"utf-8\") for c in cols_expected] == list(row_obtained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide all streams in one query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@DataStoreLosAlamos.method\n",
    "def streams(self) -> Mapping[str, ddf.DataFrame]:\n",
    "    # First ensure all these streams have been chunked; leverage parallel cluster computation.\n",
    "    streams = [\"auth\", \"dns\", \"flows\", \"proc\"]\n",
    "    persisted = [self.join_chunked(name).persist() for name in streams]\n",
    "    dask.compute(persisted)\n",
    "    del persisted\n",
    "    \n",
    "    return {name: self.get_stream(name) for name in streams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test \u001b[1mgathering_all_streams\u001b[0m passed.\n"
     ]
    }
   ],
   "source": [
    "@suite.test\n",
    "def gathering_all_streams():\n",
    "    stream_delay: Mapping[str, float] = {\n",
    "        \"auth\": 4.0,\n",
    "        \"dns\": 0.5,\n",
    "        \"flows\": 1.0,\n",
    "        \"proc\": 2.0\n",
    "    }\n",
    "    longest = max(stream_delay.values())\n",
    "\n",
    "    @dask.delayed\n",
    "    def mock_join_chunked(name: str) -> os.PathLike:\n",
    "        time.sleep(stream_delay[name])\n",
    "        return name  # Unused\n",
    "    \n",
    "    def mock_get_stream(name: str) -> Tuple[str]:\n",
    "        return (name,)\n",
    "    \n",
    "    cluster = LocalCluster(n_workers=4, threads_per_worker=1, dashboard_address=None)\n",
    "    client = Client(cluster)\n",
    "    try:\n",
    "        ds = DataStoreLosAlamos(\"dummy\")\n",
    "        with patch.object(ds, \"join_chunked\", new=Mock(side_effect=mock_join_chunked)),\\\n",
    "                patch.object(ds, \"get_stream\", side_effect=mock_get_stream):\n",
    "            tic = time.time()\n",
    "            assert ds.streams() == {name: (name,) for name in [\"auth\", \"dns\", \"flows\", \"proc\"]}\n",
    "            toc = time.time()\n",
    "            assert(abs(toc - tic - longest) < 0.1)\n",
    "    finally:\n",
    "        client.close()\n",
    "        cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 passed, \u001b[37m0 failed\u001b[0m, \u001b[37m0 raised an error\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    _ = summarize_results(suite)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
